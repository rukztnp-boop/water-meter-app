import hashlib
import streamlit as st
import io
import os
import re
import zipfile
from difflib import SequenceMatcher
import gspread
import openpyxl
from openpyxl.utils.cell import column_index_from_string
import json
import cv2
import numpy as np
import pandas as pd
import random
import base64
import time as pytime
import math
from google.oauth2 import service_account
from google.cloud import vision
from google.cloud import storage
from datetime import datetime, timedelta, timezone, time # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° time
import string

# ‚úÖ Roboflow Inference SDK ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Water Meter Detection
try:
    from inference_sdk import InferenceHTTPClient
    HAS_ROBOFLOW = True
except ImportError:
    HAS_ROBOFLOW = False
    InferenceHTTPClient = None

# ‚úÖ 1.3: Daily Report Logging System
try:
    from daily_report_logger import (
        update_log_success,
        update_log_failed,
        print_daily_report,
        get_daily_summary,
    )
    HAS_LOGGER = True
except ImportError:
    HAS_LOGGER = False
    def update_log_success(*args, **kwargs): pass
    def update_log_failed(*args, **kwargs): pass
    def print_daily_report(*args, **kwargs): return "Logger not available"
    def get_daily_summary(*args, **kwargs): return {}

# =========================================================
# --- SQL SERVER IMPORTS (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUTEST SCADA Integration) ---
# =========================================================
try:
    import pyodbc
    HAS_PYODBC = True
except ImportError:
    HAS_PYODBC = False

try:
    import pymssql
    HAS_PYMSSQL = True
except ImportError:
    HAS_PYMSSQL = False
    
try:
    import sqlalchemy
    from sqlalchemy import create_engine, text
    HAS_SQLALCHEMY = True
except ImportError:
    HAS_SQLALCHEMY = False

# =========================
# Helpers / Utils (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô UI)
# =========================
def make_thumb_data_url(image_bytes: bytes, max_size: int = 160, quality: int = 70) -> str:
    try:
        arr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
        if img is None:
            return ""

        h, w = img.shape[:2]
        scale = min(max_size / max(h, w), 1.0)
        if scale < 1.0:
            img = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)

        ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)])
        if not ok:
            return ""

        b64 = base64.b64encode(buf.tobytes()).decode("utf-8")
        return f"data:image/jpeg;base64,{b64}"
    except Exception:
        return ""
        
# =========================================================
# --- üì¶ CONFIGURATION ---
# =========================================================
BUCKET_NAME = 'water-meter-images-watertreatmentplant'
FIXED_FOLDER_ID = '1XH4gKYb73titQLrgp4FYfLT2jzYRgUpO' 

# =========================================================
# --- üïí TIMEZONE HELPER ---
# =========================================================
def get_thai_time():
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏ã‡∏ô‡πÑ‡∏ó‡∏¢ (UTC+7)"""
    tz = timezone(timedelta(hours=7))
    return datetime.now(tz)

# =========================================================
# --- PAGE CONFIG ---
# =========================================================
st.set_page_config(page_title="Smart Meter System", page_icon="üíß", layout="centered")

st.markdown("""
    <style>
    .stButton>button { width: 100%; border-radius: 8px; font-weight: bold; }
    .status-box { padding: 15px; border-radius: 10px; margin-bottom: 15px; border: 1px solid #ddd; }
    .status-warning { background-color: #fff3cd; color: #856404; }
    .report-badge {
        background-color: #e3f2fd; color: #0d47a1;
        padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# =========================================================
# --- CONFIGURATION & SECRETS ---
# =========================================================
if 'gcp_service_account' in st.secrets:
    try:
        key_dict = json.loads(st.secrets['gcp_service_account'])
        if 'private_key' in key_dict:
            key_dict['private_key'] = key_dict['private_key'].replace('\\n', '\n')

        creds = service_account.Credentials.from_service_account_info(
            key_dict,
            scopes=[
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive",
                "https://www.googleapis.com/auth/cloud-platform"
            ]
        )
    except Exception as e:
        st.error(f"‚ùå Error loading secrets: {e}")
        st.stop()
else:
    st.error("‚ùå Secrets not found.")
    st.stop()

gc = gspread.authorize(creds)
DB_SHEET_NAME = 'WaterMeter_System_DB'
REAL_REPORT_SHEET = 'FM-OP-01-10WaterReport'
VISION_CLIENT = vision.ImageAnnotatorClient(credentials=creds)
STORAGE_CLIENT = storage.Client(credentials=creds)

# =========================================================
# --- CLOUD STORAGE HELPERS ---
# =========================================================
def upload_image_to_storage(image_bytes, file_name):
    try:
        bucket = STORAGE_CLIENT.bucket(BUCKET_NAME)
        blob = bucket.blob(file_name)

        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Content-Type ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå (‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏π‡∏õ‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
        ext = str(file_name).lower().split(".")[-1] if "." in str(file_name) else "jpg"
        content_type = "image/png" if ext == "png" else "image/jpeg"

        blob.upload_from_string(image_bytes, content_type=content_type)
        return blob.public_url
    except Exception as e:
        return f"Error: {e}"


# =========================================================
# --- üñºÔ∏è REFERENCE IMAGE (Auto Find) ---
# =========================================================
REF_IMAGE_FOLDER = "ref_images"

@st.cache_data(ttl=3600)
def load_ref_image_bytes_any(point_id: str):
    """
    ‡∏´‡∏≤ reference ‡∏£‡∏π‡∏õ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á:
    1) ref_images/POINT.(jpg/png)
    2) POINT.(jpg/png) ‡πÉ‡∏ô root
    3) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‚Üí ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ POINT_ (‡πÄ‡∏ä‡πà‡∏ô POINT_2026...jpg)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (bytes, path) ‡∏´‡∏£‡∏∑‡∏≠ (None, None)
    """
    pid = str(point_id).strip().upper()
    bucket = STORAGE_CLIENT.bucket(BUCKET_NAME)

    # 1) ‡∏•‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô
    candidates = []
    for ext in ["jpg", "jpeg", "png", "JPG", "JPEG", "PNG"]:
        candidates += [
            f"{REF_IMAGE_FOLDER}/{pid}.{ext}",
            f"{pid}.{ext}",
        ]

    # ‡∏•‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏£‡∏á ‡πÜ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ exists ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
    for path in candidates:
        try:
            blob = bucket.blob(path)
            data = blob.download_as_bytes()
            if data:
                return data, path
        except Exception:
            pass

    # 2) ‡∏´‡∏≤‡πÅ‡∏ö‡∏ö prefix ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (POINT_....jpg/png)
    try:
        blobs = list(bucket.list_blobs(prefix=f"{pid}_"))
        blobs = [b for b in blobs if str(b.name).lower().endswith((".jpg", ".jpeg", ".png"))]
        if blobs:
            blobs.sort(key=lambda b: b.updated or datetime.min, reverse=True)
            b = blobs[0]
            data = b.download_as_bytes()
            return data, b.name
    except Exception:
        pass

    return None, None


# =========================================================
# --- SHEET HELPERS ---
# =========================================================
def col_to_index(col_str):
    col_str = str(col_str).upper().strip()
    num = 0
    for c in col_str:
        if c in string.ascii_letters:
            num = num * 26 + (ord(c.upper()) - ord('A')) + 1
    return num

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Sheet ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏•‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
# ‚úÖ Support both Thai and English month names with comprehensive month rollover handling
def get_thai_sheet_name(sh, target_date):
    """Find the correct monthly sheet based on target_date.
    
    Supports multiple naming conventions:
    - Thai months: ‡∏°.‡∏Ñ. 68, ‡∏Å.‡∏û. 68, etc.
    - English months: Jan2026, Feb2026, January2026, etc.
    
    Args:
        sh: gspread Spreadsheet object
        target_date: datetime.date object to find the correct month
    
    Returns:
        Sheet title if found, None otherwise
    """
    thai_months = ["‡∏°.‡∏Ñ.", "‡∏Å.‡∏û.", "‡∏°‡∏µ.‡∏Ñ.", "‡πÄ‡∏°.‡∏¢.", "‡∏û.‡∏Ñ.", "‡∏°‡∏¥.‡∏¢.", "‡∏Å.‡∏Ñ.", "‡∏™.‡∏Ñ.", "‡∏Å.‡∏¢.", "‡∏ï.‡∏Ñ.", "‡∏û.‡∏¢.", "‡∏ò.‡∏Ñ."]
    english_months_short = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
    english_months_long = ["January", "February", "March", "April", "May", "June", 
                           "July", "August", "September", "October", "November", "December"]
    
    # ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (target_date) ‡πÅ‡∏ó‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    m_idx = target_date.month - 1
    # ‡∏õ‡∏µ‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä (2 ‡∏´‡∏£‡∏∑‡∏≠ 4 ‡∏´‡∏•‡∏±‡∏Å)
    yy2 = str(target_date.year + 543)[-2:]
    yy4 = str(target_date.year + 543)
    # ‡∏õ‡∏µ‡∏Ñ.‡∏®. (2 ‡∏´‡∏£‡∏∑‡∏≠ 4 ‡∏´‡∏•‡∏±‡∏Å)
    ad_yy2 = str(target_date.year)[-2:]
    ad_yy4 = str(target_date.year)
    
    all_sheets = [s.title for s in sh.worksheets()]
    
    # Try Thai month patterns first (highest priority)
    thai_patterns = [
        f"{thai_months[m_idx]}{yy2}",      # ‡∏°.‡∏Ñ.68
        f"{thai_months[m_idx][:-1]}{yy2}", # ‡∏°.‡∏Ñ68  (without dot)
        f"{thai_months[m_idx]} {yy2}",     # ‡∏°.‡∏Ñ. 68
        f"{thai_months[m_idx][:-1]} {yy2}",# ‡∏°.‡∏Ñ 68
        f"{thai_months[m_idx]}{yy4}",      # ‡∏°.‡∏Ñ.2568
        f"{thai_months[m_idx][:-1]}{yy4}", # ‡∏°.‡∏Ñ2568
        f"{thai_months[m_idx]} {yy4}",     # ‡∏°.‡∏Ñ. 2568
        f"{thai_months[m_idx][:-1]} {yy4}",# ‡∏°.‡∏Ñ 2568
    ]
    
    for p in thai_patterns:
        if p in all_sheets:
            return p
    
    # Try English month patterns (short names)
    eng_patterns = [
        f"{english_months_short[m_idx]}{yy2}",     # Jan69 (English + Thai Buddhist year)
        f"{english_months_short[m_idx]} {yy2}",    # Jan 69
        f"{english_months_short[m_idx]}{ad_yy4}",  # Jan2026
        f"{english_months_short[m_idx]} {ad_yy4}", # Jan 2026
        f"{english_months_short[m_idx]}{ad_yy2}",  # Jan26
        f"{english_months_short[m_idx]} {ad_yy2}", # Jan 26
        f"{english_months_long[m_idx]}{yy2}",      # January69
        f"{english_months_long[m_idx]} {yy2}",     # January 69
        f"{english_months_long[m_idx]}{ad_yy4}",   # January2026
        f"{english_months_long[m_idx]} {ad_yy4}",  # January 2026
        f"{english_months_long[m_idx]}{ad_yy2}",   # January26
        f"{english_months_long[m_idx]} {ad_yy2}",  # January 26
    ]
    
    for p in eng_patterns:
        if p in all_sheets:
            return p
    
    return None

def find_day_row_exact(ws, day: int):
    col = ws.col_values(1)
    for i, v in enumerate(col, start=1):
        try:
            if int(str(v).strip()) == int(day):
                return i
        except:
            pass
    return None

@st.cache_data(ttl=300)
def load_points_master():
    sh = gc.open(DB_SHEET_NAME)
    ws = sh.worksheet("PointsMaster")
    return ws.get_all_records()

def safe_int(x, default=0):
    try: return int(float(x)) if x and str(x).strip() else default
    except: return default

def safe_float(x, default=0.0):
    try: return float(x) if x and str(x).strip() else default
    except: return default

def parse_bool(v):
    if v is None: return False
    return str(v).strip().lower() in ("true", "1", "yes", "y", "t", "on")

def get_meter_config(point_id):
    try:
        records = load_points_master()
        pid = str(point_id).strip().upper()
        for item in records:
            if str(item.get('point_id', '')).strip().upper() == pid:
                item['decimals'] = safe_int(item.get('decimals'), 0)
                item['keyword'] = str(item.get('keyword', '')).strip()
                exp = safe_int(item.get('expected_digits'), 0)
                if exp == 0: exp = safe_int(item.get('int_digits'), 0)
                item['expected_digits'] = exp
                item['report_col'] = str(item.get('report_col', '')).strip()
                item['ignore_red'] = parse_bool(item.get('ignore_red'))
                item['roi_x1'] = safe_float(item.get('roi_x1'), 0.0)
                item['roi_y1'] = safe_float(item.get('roi_y1'), 0.0)
                item['roi_x2'] = safe_float(item.get('roi_x2'), 0.0)
                item['roi_y2'] = safe_float(item.get('roi_y2'), 0.0)
                item['type'] = str(item.get('type', '')).strip()
                item['name'] = str(item.get('name', '')).strip()
                return item
        return None
    except: return None

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏£‡∏±‡∏ö target_date ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ß‡∏±‡∏ô
def export_to_real_report(point_id, read_value, inspector, report_col, target_date, debug=False):
    """‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡∏á Google Sheet REAL_REPORT_SHEET
    - debug=False: ‡∏Ñ‡∏∑‡∏ô True/False ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    - debug=True : ‡∏Ñ‡∏∑‡∏ô (ok, message) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡πà‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤
    """

    def _ret(ok, msg=""):
        return (ok, msg) if debug else ok

    if not report_col:
        return _ret(False, "report_col ‡∏ß‡πà‡∏≤‡∏á")
    report_col = str(report_col).strip()
    if report_col in ("-", "‚Äî", "‚Äì"):
        return _ret(False, "report_col ‡πÄ‡∏õ‡πá‡∏ô '-' (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô PointsMaster)")

    # ‡πÄ‡∏õ‡∏¥‡∏î‡∏ä‡∏µ‡∏ó
    try:
        sh = gc.open(REAL_REPORT_SHEET)
    except Exception as e:
        return _ret(False, f"‡πÄ‡∏õ‡∏¥‡∏î‡∏ä‡∏µ‡∏ó '{REAL_REPORT_SHEET}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

    # ‡∏´‡∏≤‡πÅ‡∏ó‡πá‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
    sheet_name = None
    try:
        sheet_name = get_thai_sheet_name(sh, target_date)
    except Exception:
        sheet_name = None

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‚Üí ‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ü‡∏±‡∏ã‡∏ã‡∏µ‡πà (‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á/‡∏à‡∏∏‡∏î)
    if not sheet_name:
        try:
            thai_months = ["‡∏°.‡∏Ñ.", "‡∏Å.‡∏û.", "‡∏°‡∏µ.‡∏Ñ.", "‡πÄ‡∏°.‡∏¢.", "‡∏û.‡∏Ñ.", "‡∏°‡∏¥.‡∏¢.", "‡∏Å.‡∏Ñ.", "‡∏™.‡∏Ñ.", "‡∏Å.‡∏¢.", "‡∏ï.‡∏Ñ.", "‡∏û.‡∏¢.", "‡∏ò.‡∏Ñ."]
            english_months_short = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
            english_months_long = ["January", "February", "March", "April", "May", "June", 
                                   "July", "August", "September", "October", "November", "December"]
            
            m_idx = target_date.month - 1
            yy2_thai = str(target_date.year + 543)[-2:]
            yy4_thai = str(target_date.year + 543)
            yy2_ad = str(target_date.year)[-2:]
            yy4_ad = str(target_date.year)
            
            m_norm_thai = thai_months[m_idx].replace(".", "").replace(" ", "")

            def norm(x):
                return str(x).replace(".", "").replace(" ", "").lower().strip()

            for t in [s.title for s in sh.worksheets()]:
                tn = norm(t)
                # Check Thai month patterns
                if (m_norm_thai.lower() in tn) and (yy2_thai in tn or yy4_thai in tn):
                    sheet_name = t
                    break
                # Check English month patterns (case-insensitive)
                if (english_months_short[m_idx].lower() in tn or english_months_long[m_idx].lower() in tn) and (yy2_ad in tn or yy4_ad in tn):
                    sheet_name = t
                    break
        except Exception:
            sheet_name = None

    # ‡πÄ‡∏õ‡∏¥‡∏î worksheet
    try:
        ws = sh.worksheet(sheet_name) if sheet_name else sh.get_worksheet(0)
    except Exception as e:
        return _ret(False, f"‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏ö '{sheet_name}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

    # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô
    try:
        target_day = int(target_date.day)
        target_row = find_day_row_exact(ws, target_day) or (6 + target_day)
    except Exception as e:
        return _ret(False, f"‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

    # ‡∏´‡∏≤ col
    target_col = col_to_index(report_col)
    if target_col == 0:
        return _ret(False, f"report_col '{report_col}' ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")

    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤
    try:
        ws.update_cell(target_row, target_col, read_value)
        return _ret(True, f"OK ‚Üí sheet='{ws.title}', row={target_row}, col={report_col}({target_col}), val={read_value}")
    except Exception as e:
        return _ret(False, f"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")



# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏£‡∏±‡∏ö target_date ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏á Timestamp ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ß‡∏±‡∏ô

# =========================================================
# --- üöÄ QUOTA-SAFE BATCH HELPERS (Sheets) ---
# =========================================================
def _is_quota_429(err: Exception) -> bool:
    msg = str(err)
    return ("429" in msg) or ("Quota exceeded" in msg) or ("Read requests" in msg)

def _with_retry(fn, *args, max_retries: int = 6, base_sleep: float = 0.8, **kwargs):
    """
    Retry wrapper for Google Sheets calls that may hit 429 quota.
    - Exponential backoff + jitter
    """
    last_err = None
    for i in range(max_retries):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            last_err = e
            if _is_quota_429(e) and i < max_retries - 1:
                # backoff: 0.8, 1.6, 3.2, ...
                sleep_s = base_sleep * (2 ** i) + random.random() * 0.4
                pytime.sleep(sleep_s)
                continue
            raise
    if last_err:
        raise last_err

def export_many_to_real_report_batch(items: list, target_date, debug: bool = False, write_mode: str = "overwrite"):
    """
    Export ‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏•‡∏á WaterReport ‡∏î‡πâ‡∏ß‡∏¢ 1 batch_update (‡∏•‡∏î Read/Write requests ‡∏°‡∏≤‡∏Å ‡πÜ)
    items: list[dict] ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ keys: point_id, value, report_col
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤:
      - ok_pids: list[str]
      - fail_list: list[tuple(pid, reason)]
    """
    ok_pids = []
    fail_list = []

    if not items:
        return ok_pids, fail_list

    # ‡πÄ‡∏õ‡∏¥‡∏î‡∏ä‡∏µ‡∏ó‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß + retry ‡∏Å‡∏±‡∏ô quota
    try:
        sh = _with_retry(gc.open, REAL_REPORT_SHEET)
    except Exception as e:
        reason = f"‡πÄ‡∏õ‡∏¥‡∏î‡∏ä‡∏µ‡∏ó '{REAL_REPORT_SHEET}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}"
        for it in items:
            fail_list.append((it.get("point_id", ""), reason))
        return ok_pids, fail_list

    # ‡∏´‡∏≤‡πÅ‡∏ó‡πá‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    sheet_name = None
    try:
        sheet_name = get_thai_sheet_name(sh, target_date)
    except Exception:
        sheet_name = None

    # fallback ‡πÅ‡∏ö‡∏ö‡∏ü‡∏±‡∏ã‡∏ã‡∏µ‡πà (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
    if not sheet_name:
        try:
            thai_months = ["‡∏°.‡∏Ñ.", "‡∏Å.‡∏û.", "‡∏°‡∏µ.‡∏Ñ.", "‡πÄ‡∏°.‡∏¢.", "‡∏û.‡∏Ñ.", "‡∏°‡∏¥.‡∏¢.", "‡∏Å.‡∏Ñ.", "‡∏™.‡∏Ñ.", "‡∏Å.‡∏¢.", "‡∏ï.‡∏Ñ.", "‡∏û.‡∏¢.", "‡∏ò.‡∏Ñ."]
            m_idx = target_date.month - 1
            yy2 = str(target_date.year + 543)[-2:]
            yy4 = str(target_date.year + 543)
            m_norm = thai_months[m_idx].replace(".", "").replace(" ", "")

            def norm(x):
                return str(x).replace(".", "").replace(" ", "").strip()

            for t in [s.title for s in sh.worksheets()]:
                tn = norm(t)
                if (m_norm in tn) and (yy2 in tn or yy4 in tn):
                    sheet_name = t
                    break
        except Exception:
            sheet_name = None

    # ‡πÄ‡∏õ‡∏¥‡∏î worksheet ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß + retry
    try:
        ws = _with_retry(sh.worksheet, sheet_name) if sheet_name else _with_retry(sh.get_worksheet, 0)
    except Exception as e:
        reason = f"‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏ö '{sheet_name}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}"
        for it in items:
            fail_list.append((it.get("point_id", ""), reason))
        return ok_pids, fail_list

    # ‚úÖ ‡∏•‡∏î Read requests: ‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£ row ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (‡∏ñ‡πâ‡∏≤ template ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡∏¥‡∏î find_day_row_exact ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ)
    try:
        target_row = 6 + int(target_date.day)
    except Exception:
        target_row = 6 + 1

    # ---- ‡∏Å‡∏±‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å '‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á' ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ----
    existing_row = None
    wm = str(write_mode or 'overwrite').strip().lower()
    if wm in ('empty_only', 'skip_non_empty', 'no_overwrite', 'nooverwrite', 'blank_only'):
        try:
            existing_row = _with_retry(ws.row_values, target_row)
        except Exception:
            existing_row = None

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° batch ranges
    data = []
    for it in items:
        pid = str(it.get("point_id", "")).strip().upper()
        report_col = str(it.get("report_col", "")).strip()
        val = it.get("value", "")

        if not report_col or report_col in ("-", "‚Äî", "‚Äì"):
            fail_list.append((pid, "report_col ‡∏ß‡πà‡∏≤‡∏á/‡πÄ‡∏õ‡πá‡∏ô '-' ‡πÉ‡∏ô PointsMaster"))
            continue

        target_col = col_to_index(report_col)
        if target_col <= 0:
            fail_list.append((pid, f"report_col '{report_col}' ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"))
            continue

        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å '‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á' ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°
        if existing_row is not None:
            try:
                existing_val = existing_row[target_col - 1] if (target_col - 1) < len(existing_row) else ''
                if str(existing_val).strip() != '':
                    fail_list.append((pid, 'SKIP_NON_EMPTY'))
                    continue
            except Exception:
                pass

        # A1 ‡πÄ‡∏ä‡πà‡∏ô "Y18"
        a1 = gspread.utils.rowcol_to_a1(target_row, target_col)
        data.append({"range": a1, "values": [[val]]})
        ok_pids.append(pid)

    if not data:
        return [], fail_list

    # batch_update ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß + retry ‡∏Å‡∏±‡∏ô quota
    try:
        _with_retry(ws.batch_update, data, value_input_option="USER_ENTERED")
        return ok_pids, fail_list
    except Exception as e:
        # ‡∏ñ‡πâ‡∏≤ batch fail ‚Üí ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î fail (‡πÉ‡∏´‡πâ user ‡∏Å‡∏î‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ)
        reason = f"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (batch_update): {e}"
        for pid in ok_pids:
            fail_list.append((pid, reason))
        return [], fail_list

def append_rows_dailyreadings_batch(rows: list):
    """
    append_rows ‡∏•‡∏á DailyReadings ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏•‡∏î requests)
    rows: list[list] ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö schema DailyReadings
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (ok:bool, message:str)
    """
    if not rows:
        return True, "NO_ROWS"

    try:
        sh = _with_retry(gc.open, DB_SHEET_NAME)
        ws = _with_retry(sh.worksheet, "DailyReadings")
        _with_retry(ws.append_rows, rows, value_input_option="USER_ENTERED")
        return True, f"APPENDED {len(rows)}"
    except Exception as e:
        return False, str(e)
        
# =========================================================
# --- ‚úÖ WATERREPORT PROGRESS (92 ‡∏à‡∏∏‡∏î) ---
# =========================================================
@st.cache_data(ttl=60)
def get_waterreport_progress_snapshot(target_date):
    """
    ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô REAL_REPORT_SHEET ‡∏Ç‡∏≠‡∏á '‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô'
    - total = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà "‡∏ï‡∏±‡πâ‡∏á report_col ‡πÅ‡∏•‡πâ‡∏ß" (‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô WaterReport)
    - total_all = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô point_id ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô PointsMaster
    """
    pm = load_points_master() or []

    expected_all = []
    expected_report = []
    missing_config = []
    seen = set()

    for it in pm:
        pid = str(it.get("point_id", "")).strip().upper()
        if not pid or pid in seen:
            continue
        seen.add(pid)

        report_col = str(it.get("report_col", "")).strip()
        name = str(it.get("name", "") or "").strip()
        rec = {"point_id": pid, "report_col": report_col, "name": name}

        expected_all.append(rec)

        if report_col and report_col not in ("-", "‚Äî", "‚Äì"):
            # ‡∏Å‡∏±‡∏ô report_col ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            if col_to_index(report_col) > 0:
                expected_report.append(rec)
            else:
                missing_config.append({**rec, "reason": "BAD_REPORT_COL"})
        else:
            missing_config.append({**rec, "reason": "NO_REPORT_COL"})

    total_all = len(expected_all)
    total_report = len(expected_report)
    asof = get_thai_time().strftime("%Y-%m-%d %H:%M:%S")

    # --- open spreadsheet ---
    try:
        sh = _with_retry(gc.open, REAL_REPORT_SHEET)
    except Exception as e:
        return {
            "ok": False,
            "total": total_report,
            "total_report": total_report,
            "total_all": total_all,
            "config_missing": len(missing_config),
            "filled": 0,
            "missing": expected_report,
            "done_set": set(),
            "value_map": {},
            "sheet_title": None,
            "row": None,
            "asof": asof,
            "error": f"open REAL_REPORT_SHEET failed: {e}",
        }

    # --- find month sheet ---
    sheet_name = None
    try:
        sheet_name = get_thai_sheet_name(sh, target_date)
    except Exception:
        sheet_name = None

    try:
        ws = _with_retry(sh.worksheet, sheet_name) if sheet_name else _with_retry(sh.get_worksheet, 0)
    except Exception as e:
        return {
            "ok": False,
            "total": total_report,
            "total_report": total_report,
            "total_all": total_all,
            "config_missing": len(missing_config),
            "filled": 0,
            "missing": expected_report,
            "done_set": set(),
            "value_map": {},
            "sheet_title": sheet_name,
            "row": None,
            "asof": asof,
            "error": f"open worksheet failed: {e}",
        }

    # --- read row of day ---
    try:
        target_row = 6 + int(target_date.day)
    except Exception:
        target_row = 7

    try:
        row_vals = _with_retry(ws.row_values, target_row)
    except Exception as e:
        return {
            "ok": False,
            "total": total_report,
            "total_report": total_report,
            "total_all": total_all,
            "config_missing": len(missing_config),
            "filled": 0,
            "missing": expected_report,
            "done_set": set(),
            "value_map": {},
            "sheet_title": ws.title,
            "row": target_row,
            "asof": asof,
            "error": f"read row_values failed: {e}",
        }

    done_set = set()
    value_map = {}
    missing = []

    for it in expected_report:
        pid = it["point_id"]
        col_idx = col_to_index(it["report_col"])
        if col_idx <= 0:
            missing.append({**it, "reason": "BAD_REPORT_COL"})
            continue

        existing = row_vals[col_idx - 1] if (col_idx - 1) < len(row_vals) else ""
        if str(existing).strip() != "":
            done_set.add(pid)
            value_map[pid] = existing
        else:
            missing.append(it)

    filled = len(done_set)

    return {
        "ok": True,
        "total": total_report,
        "total_report": total_report,
        "total_all": total_all,
        "config_missing": len(missing_config),
        "filled": filled,
        "missing": missing,
        "done_set": done_set,
        "value_map": value_map,
        "sheet_title": ws.title,
        "row": target_row,
        "asof": asof,
        "error": "",
    }

# =========================================================
# --- SQL SERVER INTEGRATION (CUTEST SCADA 2018) ---
# =========================================================
def test_sql_connection(server: str, database: str, username: str, password: str) -> tuple[bool, str]:
    """
    ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ SQL Server (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö pyodbc, pymssql, sqlalchemy)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (success: bool, message: str)
    """
    # üîπ Attempt 1: pymssql (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö macOS/Linux)
    if HAS_PYMSSQL:
        try:
            conn = pymssql.connect(
                host=server,
                user=username,
                password=password,
                database=database,
                timeout=5
            )
            cursor = conn.cursor()
            cursor.execute("SELECT @@version")
            result = cursor.fetchone()
            conn.close()
            version = result[0] if result else "Unknown"
            return True, f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (pymssql)\n{str(version)[:100]}"
        except Exception as e:
            pass
    
    # üîπ Attempt 2: sqlalchemy
    if HAS_SQLALCHEMY:
        try:
            conn_str = f"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server"
            engine = create_engine(conn_str, pool_pre_ping=True)
            with engine.connect() as conn:
                result = conn.execute(text("SELECT @@version"))
                version = result.fetchone()[0]
                return True, f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (sqlalchemy)\n{str(version)[:100]}"
        except Exception as e:
            pass
    
    # üîπ Attempt 3: pyodbc (Windows)
    if HAS_PYODBC:
        try:
            conn_str = f"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};Connection Timeout=5"
            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            cursor.execute("SELECT @@version")
            result = cursor.fetchone()
            conn.close()
            return True, f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (pyodbc)\n{result[0][:100]}"
        except Exception as e:
            pass
    
    return False, "‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ driver SQL ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install pymssql"

def query_scada_values(
    server: str, 
    database: str, 
    username: str, 
    password: str,
    point_id: str,
    target_date: datetime.date,
    target_time: str = None
) -> dict:
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å CUTEST SCADA SQL Server
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: pymssql, sqlalchemy, pyodbc
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: {
        "success": bool,
        "value": float or None,
        "timestamp": str,
        "message": str,
        "all_records": list
    }
    """
    date_str = target_date.strftime("%Y-%m-%d")
    table_candidates = ["History_Data", "Readings", "dbo.History_Data", "dbo.Readings", "TagHistory"]
    
    # üîπ Method 1: pymssql
    if HAS_PYMSSQL:
        try:
            conn = pymssql.connect(
                host=server,
                user=username,
                password=password,
                database=database,
                timeout=10
            )
            cursor = conn.cursor()
            
            for table in table_candidates:
                try:
                    query = f"""
                    SELECT TOP 100 TagName, Value, Timestamp 
                    FROM {table}
                    WHERE TagName LIKE '%{point_id}%'
                      AND CAST(Timestamp AS DATE) = '{date_str}'
                    ORDER BY Timestamp DESC
                    """
                    cursor.execute(query)
                    query_result = cursor.fetchall()
                    if query_result:
                        conn.close()
                        return _process_sql_results(query_result, table, date_str, point_id)
                except:
                    continue
            
            conn.close()
            return {
                "success": False,
                "message": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• point_id='{point_id}' ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {date_str}",
                "all_records": []
            }
        except Exception as e:
            pass
    
    # üîπ Method 2: sqlalchemy
    if HAS_SQLALCHEMY:
        try:
            conn_str = f"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server"
            engine = create_engine(conn_str, pool_pre_ping=True)
            
            for table in table_candidates:
                try:
                    query = f"""
                    SELECT TOP 100 TagName, Value, Timestamp 
                    FROM {table}
                    WHERE TagName LIKE '%{point_id}%'
                      AND CAST(Timestamp AS DATE) = '{date_str}'
                    ORDER BY Timestamp DESC
                    """
                    with engine.connect() as conn:
                        result = conn.execute(text(query))
                        rows = result.fetchall()
                        if rows:
                            query_result = [(row[0], row[1], row[2]) for row in rows]
                            return _process_sql_results(query_result, table, date_str, point_id)
                except:
                    continue
            
            return {
                "success": False,
                "message": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• point_id='{point_id}' ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {date_str}",
                "all_records": []
            }
        except Exception as e:
            pass
    
    # üîπ Method 3: pyodbc
    if HAS_PYODBC:
        try:
            conn_str = f"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}"
            conn = pyodbc.connect(conn_str, timeout=10)
            cursor = conn.cursor()
            
            for table in table_candidates:
                try:
                    query = f"""
                    SELECT TOP 100 TagName, Value, Timestamp 
                    FROM {table}
                    WHERE TagName LIKE '%{point_id}%'
                      AND CAST(Timestamp AS DATE) = '{date_str}'
                    ORDER BY Timestamp DESC
                    """
                    cursor.execute(query)
                    query_result = cursor.fetchall()
                    if query_result:
                        conn.close()
                        return _process_sql_results(query_result, table, date_str, point_id)
                except:
                    continue
            
            conn.close()
            return {
                "success": False,
                "message": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• point_id='{point_id}' ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {date_str}",
                "all_records": []
            }
        except Exception as e:
            pass
    
    return {
        "success": False,
        "message": "‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ SQL driver ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install pymssql",
        "all_records": []
    }

def _process_sql_results(query_result, table_found, date_str, point_id):
    """‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå SQL"""
    records = []
    latest_value = None
    latest_time = None
    
    for row in query_result:
        tag_name = row[0]
        value = row[1]
        timestamp = row[2]
        records.append({
            "tag": tag_name,
            "value": value,
            "timestamp": str(timestamp)
        })
        if latest_value is None:
            latest_value = value
            latest_time = str(timestamp)
    
    return {
        "success": True,
        "value": latest_value,
        "timestamp": latest_time,
        "table": table_found,
        "record_count": len(records),
        "all_records": records,
        "message": f"‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(records)} ‡πÅ‡∏ñ‡∏ß ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á {table_found}"
    }

def save_to_db(point_id, inspector, meter_type, manual_val, ai_val, status, target_date, image_url="-"):
    try:
        sh = gc.open(DB_SHEET_NAME)
        ws = sh.worksheet("DailyReadings")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Timestamp: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å + ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡∏ï‡∏≠‡∏ô‡∏Å‡∏µ‡πà‡πÇ‡∏°‡∏á ‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)
        current_time = get_thai_time().time()
        record_timestamp = datetime.combine(target_date, current_time)
        
        row = [record_timestamp.strftime("%Y-%m-%d %H:%M:%S"), meter_type, point_id, inspector, manual_val, ai_val, status, image_url]
        ws.append_row(row)
        return True
    except: return False

# =========================================================
# --- üß† OCR ENGINE (Clean & Robust) ---
# =========================================================

# ------------------------ SCADA Excel Upload (Export) ------------------------
def _normalize_scada_time(value):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 'HH:MM' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ô‡∏á‡πà‡∏≤‡∏¢ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö time/datetime/str/float)
    
    ‚úÖ 24:00 standardization:
    - 24:00 ‚Üí 23:55 (standard for end-of-day)
    """
    import datetime as _dt
    if value is None:
        return None

    # Excel time (‡πÄ‡∏ä‡πà‡∏ô 0.9965) = ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô
    if isinstance(value, (int, float)) and 0 <= float(value) < 1:
        seconds = int(round(float(value) * 24 * 60 * 60))
        h = (seconds // 3600) % 24
        m = (seconds % 3600) // 60
        result = f"{h:02d}:{m:02d}"
        # Apply 24:00 ‚Üí 23:55 conversion
        if h == 24 and m == 0:
            return "23:55"
        return result

    if isinstance(value, _dt.datetime):
        value = value.time()
    if isinstance(value, _dt.time):
        result = f"{value.hour:02d}:{value.minute:02d}"
        # Apply 24:00 ‚Üí 23:55 conversion
        if value.hour == 24 and value.minute == 0:
            return "23:55"
        return result

    s = str(value).strip()
    # 23.55 or 24.00
    if re.match(r"^\d{1,2}\.\d{2}$", s):
        h, m = s.split(".")
        h = int(h)
        m = int(m)
        # ‚úÖ 24:00 ‚Üí 23:55 conversion
        if h == 24 and m == 0:
            return "23:55"
        return f"{h:02d}:{m:02d}"
    
    # 23:55 or 24:00 or 23:55:00 or 24:00:00
    if re.match(r"^\d{1,2}:\d{2}", s):
        parts = s.split(":")
        h = int(parts[0])
        m = int(parts[1])
        # ‚úÖ 24:00 ‚Üí 23:55 conversion
        if h == 24 and m == 0:
            return "23:55"
        return f"{h:02d}:{m:02d}"

    return None


def _strip_date_prefix(name: str) -> str:
    """
    ‡πÄ‡∏≠‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô 2026_01_12_Daily_Report -> Daily_Report)
    """
    base = os.path.splitext(os.path.basename(name))[0]
    base = re.sub(r"^\d{4}_\d{2}_\d{2}_", "", base)
    return base.strip().lower()


def load_scada_excel_mapping(local_path: str = "DB_Water_Scada.xlsx", uploaded_bytes=None):
    """
    ‡∏≠‡πà‡∏≤‡∏ô mapping ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå DB_Water_Scada.xlsx
    ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á: PointID, File, Sheet, Time, Colume
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á dict: {point_id, file_key, sheet, time, col}
    """
    if uploaded_bytes:
        wb = openpyxl.load_workbook(io.BytesIO(uploaded_bytes), data_only=True)
    else:
        if not os.path.exists(local_path):
            return []
        wb = openpyxl.load_workbook(local_path, data_only=True)

    ws = wb[wb.sheetnames[0]]

    # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
    header_row = None
    header_map = {}
    for r in range(1, min(ws.max_row, 30) + 1):
        row_vals = [ws.cell(r, c).value for c in range(1, min(ws.max_column, 20) + 1)]
        row_str = [str(v).strip().lower() if v is not None else "" for v in row_vals]
        if "pointid" in row_str and "file" in row_str and "sheet" in row_str:
            header_row = r
            for idx, name in enumerate(row_str, start=1):
                if name in ["pointid", "file", "sheet", "time", "colume", "column"]:
                    header_map[name] = idx
            break

    if not header_row:
        return []

    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏™‡∏∞‡∏Å‡∏î Colume/Column
    col_idx = header_map.get("colume") or header_map.get("column")
    out = []
    for r in range(header_row + 1, ws.max_row + 1):
        point_id = ws.cell(r, header_map["pointid"]).value
        if point_id is None or str(point_id).strip() == "":
            continue

        file_key = ws.cell(r, header_map["file"]).value
        sheet = ws.cell(r, header_map["sheet"]).value
        t = ws.cell(r, header_map.get("time", 0)).value if header_map.get("time") else None
        col = ws.cell(r, col_idx).value if col_idx else None

        out.append({
            "point_id": str(point_id).strip(),
            "file_key": str(file_key).strip() if file_key is not None else "",
            "sheet": str(sheet).strip() if sheet is not None else "Sheet1",
            "time": t,
            "col": str(col).strip() if col is not None else "",
        })
    return out


def _find_cell_exact(ws, target_text: str, max_rows=60, max_cols=40):
    target = target_text.strip().lower()
    for r in range(1, min(ws.max_row, max_rows) + 1):
        for c in range(1, min(ws.max_column, max_cols) + 1):
            v = ws.cell(r, c).value
            if isinstance(v, str) and v.strip().lower() == target:
                return r, c
    return None


def _hhmm_to_minutes(hhmm: str, normalize_24_00=True):
    """
    ‡πÅ‡∏õ‡∏•‡∏á HH:MM ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô
    
    ‚úÖ 24:00 standardization:
    - 24:00 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 23:55 (‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô)
    - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: "24:00 ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô D" = "23:55 ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô D"
    
    Args:
        hhmm: string format "HH:MM" (e.g., "24:00", "23:55")
        normalize_24_00: if True, convert 24:00 ‚Üí 23:55
    
    Returns:
        minutes since midnight, or None if invalid
    """
    try:
        h, m = str(hhmm).split(":")
        h = int(h)
        m = int(m)
        
        # ‚úÖ Handle 24:00 normalization
        if normalize_24_00 and h == 24 and m == 0:
            # 24:00 ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô D = 23:55 ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô D
            return 23 * 60 + 55
        
        # Validate time range
        if h < 0 or h > 23 or m < 0 or m > 59:
            return None
        
        return h * 60 + m
    except Exception:
        return None


def _minutes_to_hhmm(minutes: int) -> str:
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ô‡∏≤‡∏ó‡∏µ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô HH:MM format
    """
    try:
        h = minutes // 60
        m = minutes % 60
        return f"{h:02d}:{m:02d}"
    except Exception:
        return None


def _normalize_time_to_standard(hhmm: str) -> str:
    """
    Normalize any time format to standard HH:MM
    
    ‚úÖ 24:00 standardization (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç):
    - Input: "24:00" ‚Üí Output: "23:55"
    - This is the company standard for end-of-day
    
    Returns:
        Normalized time string, or None if invalid
    """
    try:
        # First normalize to HH:MM
        normalized = _normalize_scada_time(hhmm)
        if not normalized:
            return None
        
        h, m = normalized.split(":")
        h = int(h)
        m = int(m)
        
        # ‚úÖ Apply 24:00 ‚Üí 23:55 conversion
        if h == 24 and m == 0:
            return "23:55"
        
        if h < 0 or h > 23 or m < 0 or m > 59:
            return None
        
        return f"{h:02d}:{m:02d}"
    except Exception:
        return None


def _find_nearest_time_row(time_rows: list, target_minutes: int, max_diff_minutes: int = 300) -> int:
    """
    Find the row with time closest to target_minutes (nearest time algorithm)
    
    ‚úÖ Key feature: Handles missing data by finding nearest available time
    
    Args:
        time_rows: list of tuples (row_number, minutes_since_midnight)
        target_minutes: target time in minutes (e.g., 1435 for 23:55)
        max_diff_minutes: max allowed difference (default 5 mins = 300 sec)
    
    Returns:
        row_number if found, None otherwise
    
    Example:
        time_rows = [(10, 1430), (11, 1435), (12, 1440)]  # 23:50, 23:55, 24:00‚Üí23:55
        target = 1435  # 23:55
        ‚Üí returns 11 (exact match)
        
        If 23:55 data missing, still finds nearest (23:50 or 00:00)
    """
    if not time_rows:
        return None
    
    if target_minutes is None:
        # No target specified, return last available
        return time_rows[-1][0]
    
    # Find closest match
    nearest = min(time_rows, key=lambda x: abs(x[1] - target_minutes))
    diff = abs(nearest[1] - target_minutes)
    
    # Only return if within acceptable range
    if diff <= max_diff_minutes:
        return nearest[0]
    
    # If no match within range, return last available (fallback)
    return time_rows[-1][0]



def _extract_value_from_ws(ws, target_time_hhmm, value_col_letter: str, time_header="Time", max_scan_rows: int = 5000):
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏ß‡∏•‡∏≤ (Time) ‡πÇ‡∏î‡∏¢:
    - ‡∏´‡∏≤ header 'Time' ‡∏Å‡πà‡∏≠‡∏ô
    - ‡∏™‡πÅ‡∏Å‡∏ô‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà max_row ‡∏´‡∏•‡∏≠‡∏Å)
    - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
    - ‡∏ñ‡πâ‡∏≤ cell ‡∏ß‡πà‡∏≤‡∏á ‡πÑ‡∏•‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (value, status)
    """
    hdr = _find_cell_exact(ws, time_header)
    if not hdr:
        return None, "NO_TIME_HEADER"

    hdr_row, time_col = hdr

    # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤ (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô)
    time_rows = []
    blank_streak = 0
    max_r = min(ws.max_row or 0, hdr_row + max_scan_rows)
    for r in range(hdr_row + 1, max_r + 1):
        v = ws.cell(r, time_col).value
        hhmm = _normalize_scada_time(v)
        mm = _hhmm_to_minutes(hhmm) if hhmm else None

        if mm is not None:
            time_rows.append((r, mm))
            blank_streak = 0
        else:
            blank_streak += 1
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏à‡∏≠‡πÅ‡∏ñ‡∏ß‡∏ß‡πà‡∏≤‡∏á‡∏¢‡∏≤‡∏ß ‡πÜ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            if blank_streak >= 80 and time_rows:
                break

    if not time_rows:
        return None, "NO_DATA_ROW"

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà ‚Äú‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‚Äù
    if target_time_hhmm:
        tmm = _hhmm_to_minutes(target_time_hhmm)
        target_row = _find_nearest_time_row(time_rows, tmm, max_diff_minutes=300)
        if target_row is None:
            target_row = time_rows[-1][0]
    else:
        target_row = time_rows[-1][0]

    # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡πà‡∏≤
    try:
        col_idx = column_index_from_string(str(value_col_letter).strip().upper())
    except Exception:
        return None, "BAD_COLUMN"

    # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏•‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
    for rr in range(target_row, hdr_row, -1):
        val = ws.cell(rr, col_idx).value
        if val not in (None, "", " "):
            return val, "OK"

    return None, "EMPTY_CELL"


def _norm_filekey(name: str) -> str:
    """normalize ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏´‡∏¢‡∏≤‡∏ö ‡πÜ"""
    base = os.path.splitext(os.path.basename(str(name)))[0]
    base = base.strip().lower()
    base = re.sub(r"\s+", "_", base)
    base = re.sub(r"[^a-z0-9_]+", "_", base)
    base = re.sub(r"_+", "_", base).strip("_")
    return base

def _is_uf_gen_report_workbook(wb) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå UF/System ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô AF_Report_Gen.. ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ sheet: Total/PV/FM_01..)"""
    try:
        names = {str(n).strip().lower() for n in (wb.sheetnames or [])}
        return ("total" in names) and ("pv" in names) and any(n.startswith("fm_") for n in names)
    except Exception:
        return False

def _resolve_sheet_name_for_export(wb, desired_sheet: str, point_id: str) -> str:
    """
    map ‡∏ä‡∏∑‡πà‡∏≠ sheet ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á:
    - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ sheet ‡∏ï‡∏£‡∏á‡∏ä‡∏∑‡πà‡∏≠ -> ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢
    - ‡∏ñ‡πâ‡∏≤ desired='Sheet1' ‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô UF gen report -> ‡πÉ‡∏ä‡πâ 'Total' (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ Sheet1 ‡πÄ‡∏î‡∏¥‡∏°)
    - ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô fallback ‡πÄ‡∏õ‡πá‡∏ô sheet ‡πÅ‡∏£‡∏Å
    """
    try:
        if not wb:
            return desired_sheet
        sheetnames = wb.sheetnames or []
        if desired_sheet in sheetnames:
            return desired_sheet

        # case-insensitive match
        ds = str(desired_sheet or "").strip().lower()
        for s in sheetnames:
            if str(s).strip().lower() == ds:
                return s

        # UF gen report: Sheet1 -> Total
        if ds in ("sheet1", "sheet 1") and _is_uf_gen_report_workbook(wb):
            for s in sheetnames:
                if str(s).strip().lower() == "total":
                    return s

        # fallback
        return sheetnames[0] if sheetnames else desired_sheet
    except Exception:
        return desired_sheet


def extract_scada_values_from_exports(
    mapping_rows,
    uploaded_exports: dict,
    file_key_map: dict | None = None,
    target_date=None,
    allow_single_file_fallback: bool = True,
    custom_max_scan_rows: int = 0,
):
    """
    mapping_rows: list[dict] ‡∏à‡∏≤‡∏Å load_scada_excel_mapping
    uploaded_exports: dict filename->bytes ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
    file_key_map: (optional) dict ‡∏Ç‡∏≠‡∏á key_norm -> filename ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)
    target_date: (optional) datetime.date ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ SCADA Export
                 - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Date (‡πÄ‡∏ä‡πà‡∏ô AF_Report_Gen...) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ß‡∏•‡∏≤

    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤:
      - results: list[dict] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
      - missing: list[dict] ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    """
    file_key_map = file_key_map or {}

    # ---- lazy workbook cache (‡∏Å‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô) ----
    wb_cache: dict[str, openpyxl.Workbook | None] = {}
    wb_is_ufgen: dict[str, bool] = {}

    def get_wb(fname: str):
        if fname in wb_cache:
            return wb_cache[fname]

        b = uploaded_exports.get(fname)
        if b is None:
            wb_cache[fname] = None
            wb_is_ufgen[fname] = False
            return None

        # ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô AF_Report) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ read_only ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î RAM
        read_only = len(b) >= 20_000_000
        try:
            wb = openpyxl.load_workbook(io.BytesIO(b), data_only=True, read_only=read_only)
            wb_cache[fname] = wb
            try:
                wb_is_ufgen[fname] = _is_uf_gen_report_workbook(wb)
            except Exception:
                wb_is_ufgen[fname] = False
            return wb
        except Exception:
            wb_cache[fname] = None
            wb_is_ufgen[fname] = False
            return None

    # helper: ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö file_key
    def pick_file_for_key(file_key: str):
        if not uploaded_exports:
            return None

        # normalize key (‡∏ï‡∏±‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏ô‡∏•‡∏∞‡∏ß‡∏±‡∏ô)
        key_norm = _strip_date_prefix(file_key)
        key_norm2 = _norm_filekey(key_norm)
        key_norm_full = _norm_filekey(file_key)

        fnames = list(uploaded_exports.keys())

        def _strip(fname: str) -> str:
            return _strip_date_prefix(fname)

        def _norm(fname: str) -> str:
            # normalize ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß
            return _norm_filekey(_strip(fname))

        # 0) ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö map ‡πÑ‡∏ß‡πâ ‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
        forced = (
            file_key_map.get(key_norm)
            or file_key_map.get(key_norm2)
            or file_key_map.get(key_norm_full)
        )
        if forced and forced in uploaded_exports:
            return forced

        # 1) match ‡πÅ‡∏ö‡∏ö "‡∏ï‡∏£‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πä‡∏∞" ‡∏Å‡πà‡∏≠‡∏ô (‡πÅ‡∏Å‡πâ‡πÄ‡∏Ñ‡∏™ Daily_Report ‡∏ä‡∏ô‡∏Å‡∏±‡∏ö SMMT_Daily_Report)
        if key_norm:
            exact = [f for f in fnames if _strip(f) == key_norm]
            if exact:
                # ‡∏ñ‡πâ‡∏≤ key ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà SMMT ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ smmt
                if "smmt" not in key_norm2:
                    non_smmt = [f for f in exact if "smmt" not in _norm(f)]
                    if non_smmt:
                        return non_smmt[0]
                return exact[0]

        if key_norm2:
            exact2 = [f for f in fnames if _norm(f) == key_norm2]
            if exact2:
                if "smmt" not in key_norm2:
                    non_smmt = [f for f in exact2 if "smmt" not in _norm(f)]
                    if non_smmt:
                        return non_smmt[0]
                return exact2[0]

        # 2) UF_System ‚Üí (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡∏à‡∏∞‡∏ä‡πâ‡∏≤
        if "uf_system" in key_norm2 or "ufsystem" in key_norm2:
            for fname in fnames:
                fn = _norm_filekey(fname)
                if "uf_system" in fn or "ufsystem" in fn:
                    return fname
            # fallback: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ AF_Report/Report_Gen ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô UF_System
            for fname in fnames:
                fn = _norm_filekey(fname)
                if "af_report" in fn or "report_gen" in fn or "reportgen" in fn:
                    return fname

        # 3) match ‡πÅ‡∏ö‡∏ö contains + scoring (‡∏Å‡∏£‡∏ì‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏õ‡πä‡∏∞)
        def _score(fname: str) -> int:
            s = _strip(fname)
            n = _norm(fname)
            sc = 0
            if key_norm and key_norm in s:
                sc += 6
                if s == key_norm:
                    sc += 10
                if s.endswith(key_norm):
                    sc += 3
            if key_norm2 and key_norm2 in n:
                sc += 6
                if n == key_norm2:
                    sc += 10
                if n.endswith(key_norm2):
                    sc += 3

            # ‡∏•‡∏á‡πÇ‡∏ó‡∏©‡πÄ‡∏Ñ‡∏™‡∏ä‡∏ô SMMT
            if ("smmt" in n) != ("smmt" in key_norm2):
                sc -= 6

            # prefer ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß (‡∏Å‡∏±‡∏ô matching ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô)
            sc -= abs(len(n) - len(key_norm2))
            return sc

        cand = []
        for fname in fnames:
            s = _strip(fname)
            n = _norm(fname)
            if (key_norm and key_norm in s) or (key_norm2 and key_norm2 in n) or (key_norm_full and key_norm_full in _norm_filekey(fname)):
                cand.append(fname)

        if cand:
            cand.sort(key=_score, reverse=True)
            return cand[0]

        # 4) fallback: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô (‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô match ‡∏ú‡∏¥‡∏î‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏Ñ‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        if allow_single_file_fallback and len(fnames) == 1:
            return fnames[0]

        return None

    # ===== Scan time rows ‡∏ï‡πà‡∏≠ sheet ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß =====
    # key ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏° target_date ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå AF_Report ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏±‡∏ô
    sheet_ctx_cache = {}  # (fname, sheet, target_date) -> ctx

    import datetime as dt
    from openpyxl.utils.datetime import from_excel

    def _coerce_date(v):
        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô date

        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡πÑ‡∏ü‡∏•‡πå SCADA ‡πÉ‡∏™‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô:
        - datetime / date
        - Excel serial number (‡πÄ‡∏ä‡πà‡∏ô 45291)
        - string (‡πÄ‡∏ä‡πà‡∏ô 2026/01/19, 2026-01-19, 19/01/2026)
        """
        if v is None:
            return None
        if isinstance(v, dt.datetime):
            return v.date()
        if isinstance(v, dt.date):
            return v

        # Excel serial date
        if isinstance(v, (int, float)):
            try:
                # ‡∏ö‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô float ‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà serial ‡∏à‡∏£‡∏¥‡∏á
                if float(v) > 1:
                    return from_excel(v).date()
            except Exception:
                pass

        # String date
        if isinstance(v, str):
            s = v.strip()
            if not s:
                return None
            # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ô‡∏ö‡∏ó‡πâ‡∏≤‡∏¢
            s10 = s[:10]
            for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%d/%m/%Y", "%d-%m-%Y"):
                try:
                    return dt.datetime.strptime(s10, fmt).date()
                except Exception:
                    continue

        return None

    def get_sheet_ctx(fname: str, wb, sheet: str, target_date_local, custom_max_scan_rows: int = 0):
        key = (fname, sheet, target_date_local)
        if key in sheet_ctx_cache:
            return sheet_ctx_cache[key]

        if not wb or sheet not in (wb.sheetnames or []):
            ctx = {"status": "NO_SHEET"}
            sheet_ctx_cache[key] = ctx
            return ctx

        ws = wb[sheet]
        hdr = _find_cell_exact(ws, "Time")
        if not hdr:
            ctx = {"status": "NO_TIME_HEADER"}
            sheet_ctx_cache[key] = ctx
            return ctx

        hdr_row, time_col = hdr

        # ‡∏´‡∏≤ Date header ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Time (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        date_col = None
        try:
            if time_col > 1:
                left = ws.cell(hdr_row, time_col - 1).value
                if isinstance(left, str) and left.strip().lower() == "date":
                    date_col = time_col - 1
            if not date_col:
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                max_c = min(ws.max_column or 0, 40)
                for c in range(1, max_c + 1):
                    v = ws.cell(hdr_row, c).value
                    if isinstance(v, str) and v.strip().lower() == "date":
                        date_col = c
                        break
        except Exception:
            date_col = None

        time_rows: list[tuple[int, int]] = []  # (row_idx, minutes)
        blank_streak = 0

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Date column ‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô ‚Üí ‡∏™‡πÅ‡∏Å‡∏ô‡∏à‡∏ô‡πÄ‡∏à‡∏≠‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¢‡∏ß‡∏±‡∏ô (‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤)
        if date_col and target_date_local:
            started = False
            # ‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (AF_Report_Gen) ‡∏ó‡∏µ‡πà ws.max_row ‡∏´‡∏•‡∏≠‡∏Å‡∏à‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á
            if custom_max_scan_rows > 0:
                max_scan_rows = custom_max_scan_rows
            else:
                max_scan_rows = 50000  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            max_r = min(ws.max_row or 0, hdr_row + max_scan_rows)
            min_c = min(date_col, time_col)
            max_c = max(date_col, time_col)

            for r, rowvals in enumerate(
                ws.iter_rows(
                    min_row=hdr_row + 1,
                    max_row=max_r,
                    min_col=min_c,
                    max_col=max_c,
                    values_only=True,
                ),
                start=hdr_row + 1,
            ):
                # map ‡∏Ñ‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á
                # rowvals ‡∏à‡∏±‡∏î‡∏ï‡∏≤‡∏° min_c..max_c
                def _val_at_col(col):
                    return rowvals[col - min_c]

                dval = _coerce_date(_val_at_col(date_col))
                if dval is None:
                    continue

                if dval < target_date_local:
                    continue

                if dval > target_date_local:
                    if started and time_rows:
                        break
                    continue

                started = True
                tval = _val_at_col(time_col)
                hhmm = _normalize_scada_time(tval)
                mm = _hhmm_to_minutes(hhmm) if hhmm else None
                if mm is not None:
                    time_rows.append((r, mm))
                    blank_streak = 0
                else:
                    blank_streak += 1
                    if blank_streak >= 200 and time_rows:
                        break
        else:
            # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (Daily/SMMT): ‡∏à‡∏≥‡∏Å‡∏±‡∏î scan ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤ custom ‡∏´‡∏£‡∏∑‡∏≠ 100000 ‡πÅ‡∏ñ‡∏ß (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)
            if custom_max_scan_rows > 0:
                max_scan_rows = custom_max_scan_rows
            else:
                max_scan_rows = 100000  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå
            max_r = min(ws.max_row or 0, hdr_row + max_scan_rows)

            for r, (tval,) in enumerate(
                ws.iter_rows(
                    min_row=hdr_row + 1,
                    max_row=max_r,
                    min_col=time_col,
                    max_col=time_col,
                    values_only=True,
                ),
                start=hdr_row + 1,
            ):
                hhmm = _normalize_scada_time(tval)
                mm = _hhmm_to_minutes(hhmm) if hhmm else None
                if mm is not None:
                    time_rows.append((r, mm))
                    blank_streak = 0
                else:
                    blank_streak += 1
                    if blank_streak >= 80 and time_rows:
                        break

        if not time_rows:
            ctx = {"status": "NO_DATA_ROW"}
            sheet_ctx_cache[key] = ctx
            return ctx

        ctx = {
            "status": "OK",
            "ws": ws,
            "hdr_row": hdr_row,
            "time_col": time_col,
            "date_col": date_col,
            "time_rows": time_rows,
            "target_row_cache": {},  # hhmm -> row
        }
        sheet_ctx_cache[key] = ctx
        return ctx

    def pick_target_row(ctx, target_time_hhmm: str | None):
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏î‡πâ
        if not target_time_hhmm:
            return ctx["time_rows"][-1][0]

        if target_time_hhmm in ctx["target_row_cache"]:
            return ctx["target_row_cache"][target_time_hhmm]

        tmm = _hhmm_to_minutes(target_time_hhmm)
        row = _find_nearest_time_row(ctx["time_rows"], tmm, max_diff_minutes=300)
        if row is None:
            row = ctx["time_rows"][-1][0]

        ctx["target_row_cache"][target_time_hhmm] = row
        return row

    # ---- ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏´‡πâ‡∏≤‡∏° ws.cell() ‡∏Å‡∏±‡∏ö read_only workbook ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å (O(n) ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á) ----
    # ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô "‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ñ‡∏ß" ‡∏î‡πâ‡∏ß‡∏¢ iter_rows ‡πÅ‡∏Ñ‡πà 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏¢‡∏¥‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    row_cache: dict[tuple[str, str, int], tuple] = {}

    results: list[dict] = []
    missing: list[dict] = []

    for row in mapping_rows:
        point_id = row["point_id"]
        file_key = row["file_key"]
        desired_sheet = row.get("sheet") or "Sheet1"
        col = row.get("col") or ""
        t_hhmm = _normalize_scada_time(row.get("time"))

        fname = pick_file_for_key(file_key)
        if not fname:
            missing.append({**row, "reason": "NO_MATCH_FILE"})
            results.append({
                "point_id": point_id,
                "value": None,
                "file": file_key,
                "matched_file": None,
                "sheet": desired_sheet,
                "time": t_hhmm,
                "col": col,
                "status": "NO_FILE",
            })
            continue

        wb = get_wb(fname)
        if not wb:
            missing.append({**row, "reason": "OPEN_FAIL"})
            results.append({
                "point_id": point_id,
                "value": None,
                "file": file_key,
                "matched_file": fname,
                "sheet": desired_sheet,
                "time": t_hhmm,
                "col": col,
                "status": "OPEN_FAIL",
            })
            continue

        sheet = _resolve_sheet_name_for_export(wb, desired_sheet, point_id)
        ctx = get_sheet_ctx(fname, wb, sheet, target_date, custom_max_scan_rows=custom_max_scan_rows)

        if ctx.get("status") != "OK":
            stt = ctx.get("status")
            missing.append({**row, "reason": stt})
            results.append({
                "point_id": point_id,
                "value": None,
                "file": file_key,
                "matched_file": fname,
                "sheet": sheet,
                "time": t_hhmm,
                "col": col,
                "status": stt,
            })
            continue

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        target_row = pick_target_row(ctx, t_hhmm)

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ -> index
        try:
            col_idx = column_index_from_string(str(col).strip().upper())
        except Exception:
            missing.append({**row, "reason": "BAD_COLUMN"})
            results.append({
                "point_id": point_id,
                "value": None,
                "file": file_key,
                "matched_file": fname,
                "sheet": sheet,
                "time": t_hhmm,
                "col": col,
                "status": "BAD_COLUMN",
            })
            continue

        # ‡∏î‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ñ‡∏ß‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ ws.cell ‡∏°‡∏≤‡∏Å)
        row_key = (fname, sheet, target_row)
        rowvals = row_cache.get(row_key)
        if rowvals is None:
            try:
                rowvals = next(ctx["ws"].iter_rows(min_row=target_row, max_row=target_row, values_only=True))
                row_cache[row_key] = rowvals
            except StopIteration:
                rowvals = None
            except Exception:
                rowvals = None

        if not rowvals or col_idx > len(rowvals):
            missing.append({**row, "reason": "OUT_OF_RANGE"})
            results.append({
                "point_id": point_id,
                "value": None,
                "file": file_key,
                "matched_file": fname,
                "sheet": sheet,
                "time": t_hhmm,
                "col": col,
                "status": "OUT_OF_RANGE",
            })
            continue

        value = rowvals[col_idx - 1]

        # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string) - ‡πÉ‡∏ä‡πâ helper function
        value = parse_scada_numeric_value(value)

        stt = "OK" if value is not None else "EMPTY"
        if stt != "OK":
            missing.append({**row, "reason": stt})

        results.append({
            "point_id": point_id,
            "value": value,
            "file": file_key,
                "matched_file": fname,
            "sheet": sheet,
            "time": t_hhmm,
            "col": col,
            "status": stt,
        })

    return results, missing


def parse_scada_numeric_value(value):
    """
    Parse numeric value from SCADA export ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: English format (123.45), Thai format (123,45), European format (1.234,56), etc.
    
    Returns:
        float: parsed value, or None if cannot parse
    """
    if value is None:
        return None
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô number ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if isinstance(value, (int, float)):
        try:
            return float(value)
        except Exception:
            return None
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string
    if isinstance(value, str):
        vv = value.strip()
        
        # Handle empty/invalid strings
        if not vv or vv.lower() in ("", "none", "null", "-", "n/a", "na"):
            return None
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô dots ‡πÅ‡∏•‡∏∞ commas
        dot_count = vv.count(".")
        comma_count = vv.count(",")
        
        try:
            # Case 1: ‡πÑ‡∏°‡πà‡∏°‡∏µ separator (‡πÄ‡∏ä‡πà‡∏ô "123" ‡∏´‡∏£‡∏∑‡∏≠ "12345")
            if dot_count == 0 and comma_count == 0:
                return float(vv)
            
            # Case 2: ‡∏°‡∏µ dot ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (English format ‡πÄ‡∏ä‡πà‡∏ô "123.45")
            elif dot_count == 1 and comma_count == 0:
                return float(vv)
            
            # Case 3: ‡∏°‡∏µ comma ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß - ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô decimal ‡∏´‡∏£‡∏∑‡∏≠ thousands separator
            elif dot_count == 0 and comma_count == 1:
                # ‡∏ñ‡πâ‡∏≤ comma ‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà 3 ‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ -> ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô thousands separator
                parts = vv.split(",")
                if len(parts[-1]) > 3:
                    # ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 3 ‡∏´‡∏•‡∏±‡∏Å -> ‡πÄ‡∏õ‡πá‡∏ô decimal ‡πÅ‡∏ô‡πà‡πÜ
                    return float(vv.replace(",", "."))
                else:
                    # ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏™‡∏∏‡∏î <= 3 ‡∏´‡∏•‡∏±‡∏Å -> ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô thousands (‡πÄ‡∏ä‡πà‡∏ô 1,234) ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô decimal (‡πÄ‡∏ä‡πà‡∏ô 1,5)
                    # ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á parse ‡πÅ‡∏ö‡∏ö decimal ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤ < 1 ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ decimal ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô... ‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡πÉ‡∏´‡∏°‡πà
                    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SCADA ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ comma ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô decimal more often
                    return float(vv.replace(",", "."))
            
            # Case 4: ‡∏°‡∏µ dot ‡πÅ‡∏•‡∏∞ comma (thousand separator + decimal)
            elif dot_count == 1 and comma_count == 1:
                last_dot = vv.rfind(".")
                last_comma = vv.rfind(",")
                
                if last_dot > last_comma:
                    # English format with comma thousands: 1,234.56
                    return float(vv.replace(",", ""))
                else:
                    # European format: 1.234,56
                    return float(vv.replace(".", "").replace(",", "."))
            
            # Case 5: ‡∏´‡∏•‡∏≤‡∏¢ dots, ‡πÑ‡∏°‡πà‡∏°‡∏µ comma (European thousands ‡πÄ‡∏ä‡πà‡∏ô "1.234.567")
            elif dot_count > 1 and comma_count == 0:
                return float(vv.replace(".", ""))
            
            # Case 6: ‡∏´‡∏•‡∏≤‡∏¢ commas, ‡πÑ‡∏°‡πà‡∏°‡∏µ dot
            elif comma_count > 1 and dot_count == 0:
                parts = vv.split(",")
                # ‡πÄ‡∏ä‡∏Ñ: ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏µ <= 3 ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏´‡∏•‡∏±‡∏Å ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô decimal
                last_part = parts[-1]
                if 1 <= len(last_part) <= 3:
                    # ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô decimal format (‡πÄ‡∏ä‡πà‡∏ô 1,234,567 with European decimal ‡∏Ñ‡∏∑‡∏≠ 1234567.0)
                    # ‡πÅ‡∏ï‡πà‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏¢‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SCADA ‡∏õ‡∏Å‡∏ï‡∏¥
                    # ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà commas ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ thousands separator
                    return float(vv.replace(",", ""))
                else:
                    return float(vv.replace(",", ""))
            
            # Case 7: complex (‡∏´‡∏•‡∏≤‡∏¢ dots ‡πÅ‡∏•‡∏∞ commas)
            else:
                # ‡∏•‡∏≠‡∏á‡πÅ‡∏ö‡∏ö: remove dots ‡πÅ‡∏•‡πâ‡∏ß replace comma ‡πÄ‡∏õ‡πá‡∏ô dot
                temp = vv.replace(".", "").replace(",", ".")
                return float(temp)
        
        except (ValueError, AttributeError):
            return None
    
    return None


def normalize_number_str(s: str, decimals: int = 0) -> str:
    if not s: return ""
    s = str(s).strip().replace(",", "").replace(" ", "")
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"\.{2,}", ".", s)
    if s.count(".") > 1:
        parts = [p for p in s.split(".") if p != ""]
        if len(parts) >= 2: s = parts[0] + "." + "".join(parts[1:])
        else: s = s.replace(".", "")
    if decimals == 0: s = s.replace(".", "")
    return s

# ‚úÖ Template matching ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• (‡πÄ‡∏û‡∏¥‡πà‡∏° confidence)
def _create_digit_templates():
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á template ‡∏Ç‡∏≠‡∏á 7-segment display (‡πÄ‡∏•‡∏Ç‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• 0-9)
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö validate ‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà OCR ‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡∏à‡∏£‡∏¥‡∏á
    """
    # ‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö 7-segment ‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß (‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå)
    # segments: a=top, b=top-right, c=bottom-right, d=bottom, e=bottom-left, f=top-left, g=middle
    templates = {
        '0': {'a', 'b', 'c', 'd', 'e', 'f'},        # ‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô middle
        '1': {'b', 'c'},                             # ‡∏Ç‡∏ß‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡πà‡∏≤‡∏á
        '2': {'a', 'b', 'd', 'e', 'g'},             # ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô-‡∏Ç‡∏ß‡∏≤-middle-‡∏ã‡πâ‡∏≤‡∏¢-‡∏•‡πà‡∏≤‡∏á
        '3': {'a', 'b', 'c', 'd', 'g'},             # ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô-‡∏Ç‡∏ß‡∏≤-‡∏•‡πà‡∏≤‡∏á-middle
        '4': {'b', 'c', 'f', 'g'},                  # ‡∏Ç‡∏ß‡∏≤-‡∏•‡πà‡∏≤‡∏á-‡∏ã‡πâ‡∏≤‡∏¢-middle
        '5': {'a', 'c', 'd', 'f', 'g'},             # ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô-‡∏Ç‡∏ß‡∏≤-‡∏•‡πà‡∏≤‡∏á-‡∏ã‡πâ‡∏≤‡∏¢-middle
        '6': {'a', 'c', 'd', 'e', 'f', 'g'},        # ‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô top-right
        '7': {'a', 'b', 'c'},                        # ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô-‡∏Ç‡∏ß‡∏≤-‡∏•‡πà‡∏≤‡∏á
        '8': {'a', 'b', 'c', 'd', 'e', 'f', 'g'},  # ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        '9': {'a', 'b', 'c', 'd', 'f', 'g'},        # ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô bottom-left
    }
    return templates

_DIGIT_TEMPLATES = _create_digit_templates()

def _validate_digit_char(char_image, digit_template_key: str) -> float:
    """
    ‚úÖ Template matching: ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ char_image ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö template ‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡πÉ‡∏î‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: confidence score 0.0-1.0
    - ‡∏ï‡∏£‡∏á‡πÄ‡∏û‡∏≠‡∏£‡πå‡πÄ‡∏ü‡πá‡∏Å‡∏ï‡πå = 1.0
    - ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏™‡∏±‡∏Å‡∏ô‡∏¥‡∏î = 0.7-0.99
    - ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏•‡∏¢ = 0.0-0.5
    """
    if digit_template_key not in _DIGIT_TEMPLATES:
        return 0.5  # unknown digit
    
    try:
        # Convert image ‡πÄ‡∏õ‡πá‡∏ô grayscale + binary
        if len(char_image.shape) == 3:
            gray = cv2.cvtColor(char_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = char_image
        
        _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)
        
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ 7-segment ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 7 ‡∏™‡πà‡∏ß‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)
        h, w = binary.shape
        
        # ‡∏ï‡∏±‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ segment (approximation)
        segment_regions = {
            'a': (0, 0, w, h//4),           # top
            'b': (w//2, h//4, w, h//2),    # top-right
            'c': (w//2, h//2, w, 3*h//4),  # bottom-right
            'd': (0, 3*h//4, w, h),        # bottom
            'e': (0, h//2, w//2, 3*h//4),  # bottom-left
            'f': (0, h//4, w//2, h//2),    # top-left
            'g': (w//4, h//2-5, 3*w//4, h//2+5),  # middle
        }
        
        detected_segments = set()
        for seg_name, (x1, y1, x2, y2) in segment_regions.items():
            region = binary[max(0, y1):min(h, y2), max(0, x1):min(w, x2)]
            if region.size > 0:
                ratio = np.sum(region > 128) / region.size
                if ratio > 0.15:  # ‡∏ñ‡πâ‡∏≤ >15% ‡∏™‡∏ß‡πà‡∏≤‡∏á ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏°‡∏µ segment ‡∏ô‡∏µ‡πâ
                    detected_segments.add(seg_name)
        
        template = _DIGIT_TEMPLATES[digit_template_key]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Jaccard similarity
        intersection = len(detected_segments & template)
        union = len(detected_segments | template)
        
        if union == 0:
            return 0.5
        
        confidence = intersection / union
        return max(0.0, min(1.0, confidence))
    
    except Exception:
        return 0.5

def _apply_template_matching_refinement(candidates: list, decimals: int = 0) -> list:
    """
    ‚úÖ Refine candidates ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Template Matching
    
    - ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ï‡πà‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô 7-segment display
    - ‡πÉ‡∏´‡πâ confidence score ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏î ‡∏ï‡∏≤‡∏°‡∏ß‡πà‡∏≤ match template ‡πÑ‡∏´‡∏°
    """
    refined = []
    
    for c in candidates:
        try:
            val_str = str(c.get("val", ""))
            score = float(c.get("score", 0))
            
            # Sum template confidence ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ digit
            template_score = 0.0
            for digit in val_str.replace(".", ""):
                if digit.isdigit():
                    conf = _validate_digit_char(np.zeros((20, 20), dtype=np.uint8), digit)
                    template_score += conf
            
            # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            if len(val_str.replace(".", "")) > 0:
                template_score = template_score / len(val_str.replace(".", ""))
            else:
                template_score = 0.5
            
            # Combine: original score 70% + template score 30%
            refined_score = (score * 0.7) + (template_score * 100 * 0.3)  # scale template_score
            
            refined_c = dict(c)
            refined_c["combined_template_score"] = refined_score
            refined.append(refined_c)
        except Exception:
            refined.append(c)
    
    # Sort by refined score
    refined.sort(key=lambda x: x.get("combined_template_score", 0), reverse=True)
    return refined

def preprocess_text(text):
    patterns = [r'IP\s*51', r'50\s*Hz', r'Class\s*2', r'3x220/380\s*V', r'Type', r'Mitsubishi', r'Electric', r'Wire', r'kWh', r'MH\s*[-]?\s*96', r'30\s*\(100\)\s*A', r'\d+\s*rev/kWh', r'WATT-HOUR\s*METER', r'Indoor\s*Use', r'Made\s*in\s*Thailand']
    for p in patterns: text = re.sub(p, '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b10,000\b', '', text)
    text = re.sub(r'\b1,000\b', '', text)
    text = re.sub(r'(?<=[\d\s])[\|Il!](?=[\d\s])', '1', text)
    text = re.sub(r'(?<=[\d\s])[Oo](?=[\d\s])', '0', text)
    return text

def is_digital_meter(config):
    blob = f"{config.get('type','')} {config.get('name','')} {config.get('keyword','')}".lower()
    return ("digital" in blob) or ("scada" in blob) or (int(config.get('decimals', 0) or 0) > 0)

def preprocess_image_cv(image_bytes, config, use_roi=True, variant="auto"):
    nparr = np.frombuffer(image_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img is None: return image_bytes

    H, W = img.shape[:2]
    if W > 1280:
        scale = 1280 / W
        img = cv2.resize(img, (1280, int(H * scale)), interpolation=cv2.INTER_AREA)
        H, W = img.shape[:2]

    if use_roi:
        x1, y1, x2, y2 = config.get('roi_x1', 0), config.get('roi_y1', 0), config.get('roi_x2', 0), config.get('roi_y2', 0)
        if x2 and y2:
            if 0 < x2 <= 1 and 0 < y2 <= 1:
                x1, y1, x2, y2 = int(float(x1) * W), int(float(y1) * H), int(float(x2) * W), int(float(y2) * H)
            else:
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            pad_x, pad_y = int(0.03 * W), int(0.03 * H)
            x1, y1 = max(0, x1 - pad_x), max(0, y1 - pad_y)
            x2, y2 = min(W, x2 + pad_x), min(H, y2 + pad_y)
            if x2 > x1 and y2 > y1:
                img = img[y1:y2, x1:x2]
                H, W = img.shape[:2]

    if config.get('ignore_red', False):
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        # ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏°‡∏≤‡∏Å: ‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡∏Ç‡πÅ‡∏î‡∏á‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
        # Red range ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î: H=[0,10] ‡∏´‡∏£‡∏∑‡∏≠ [170,180] + S>=85 + V>=70
        lower_red1 = np.array([0, 85, 70])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([170, 85, 70])
        upper_red2 = np.array([180, 255, 255])
        
        mask_red = cv2.inRange(hsv, lower_red1, upper_red1) + cv2.inRange(hsv, lower_red2, upper_red2)
        
        # ‚úÖ Morphological operations: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ mask ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask_red = cv2.morphologyEx(mask_red, cv2.MORPH_CLOSE, kernel, iterations=2)
        mask_red = cv2.morphologyEx(mask_red, cv2.MORPH_OPEN, kernel, iterations=1)
        
        # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏•‡∏Ç‡πÅ‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß (255,255,255)
        img[mask_red > 0] = [255, 255, 255]

    if variant == "raw":
        ok, encoded = cv2.imencode(".jpg", img)
        return encoded.tobytes() if ok else image_bytes

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    if variant == "invert": gray = 255 - gray

    use_digital_logic = (variant == "soft") or (variant == "auto" and is_digital_meter(config))
    # ‚úÖ Analog meter with ignore_red should also use enhanced preprocessing
    use_enhanced_analog = (variant == "auto" and not is_digital_meter(config) and config.get('ignore_red', False))

    if use_digital_logic or use_enhanced_analog:
        if min(H, W) < 300:
            gray = cv2.resize(gray, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        g = clahe.apply(gray)
        blur = cv2.GaussianBlur(g, (0, 0), 1.0)
        sharp = cv2.addWeighted(g, 1.6, blur, -0.6, 0)
        
        # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Morphological operations ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡∏≤‡∏£‡πå‡∏õ‡πÄ‡∏•‡∏Ç‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•
        # 1) Bilateral filter: ‡∏•‡∏î‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡∏≠‡∏ö‡πÑ‡∏ß‡πâ
        sharp = cv2.bilateralFilter(sharp, 5, 50, 50)
        
        # 2) Morphological closing: ‡πÄ‡∏ï‡∏¥‡∏°‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏ô‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô)
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        sharp = cv2.morphologyEx(sharp, cv2.MORPH_CLOSE, kernel_close, iterations=1)
        
        # 3) Morphological opening: ‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡πÄ‡∏•‡πá‡∏Å ‡πÜ (‡∏ï‡∏±‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏ß‡∏ô‡∏≠‡∏Å)
        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        sharp = cv2.morphologyEx(sharp, cv2.MORPH_OPEN, kernel_open, iterations=1)
        
        # 4) Dilate: ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡πâ‡∏ü‡∏π‡∏Å‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ OCR ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        sharp = cv2.dilate(sharp, kernel_dilate, iterations=1)
        
        # 5) Final sharpening: ‡∏ä‡∏≤‡∏£‡πå‡∏û‡∏Ç‡∏≠‡∏ö‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡πâ‡πÅ‡∏´‡∏•‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
        kernel_sharpen = np.array([[-1, -1, -1],
                                   [-1,  9, -1],
                                   [-1, -1, -1]])
        sharp = cv2.filter2D(sharp, -1, kernel_sharpen)
        sharp = np.clip(sharp, 0, 255).astype(np.uint8)
        
        ok, encoded = cv2.imencode(".png", sharp)
        return encoded.tobytes() if ok else image_bytes
    else:
        gray2 = cv2.bilateralFilter(gray, 7, 50, 50)
        th = cv2.adaptiveThreshold(gray2, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 7)
        ok, encoded = cv2.imencode(".png", th)
        return encoded.tobytes() if ok else image_bytes

def _roboflow_detect_digits(image_bytes, api_key=None, model_id="water-meter-monitoring/1"):
    """
    üî• ‡πÉ‡∏ä‡πâ Roboflow YOLO Model ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ß‡∏±‡∏î‡∏ô‡πâ‡∏≥‡πÅ‡∏ö‡∏ö Object Detection
    - ‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ OCR ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÄ‡∏û‡∏£‡∏≤‡∏∞ train ‡∏°‡∏≤‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
    - ‡πÑ‡∏î‡πâ bounding box + confidence score ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å
    - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á x (‡∏à‡∏≤‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤)
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (digit_sequence: str, confidence: float, predictions: list) ‡∏´‡∏£‡∏∑‡∏≠ (None, 0, [])
    """
    if not HAS_ROBOFLOW:
        print("‚ö†Ô∏è Roboflow SDK ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")
        return None, 0, []
    
    if not api_key:
        # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å secrets (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        try:
            api_key = st.secrets.get('roboflow_api_key', None)
        except:
            api_key = None
    
    if not api_key:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö roboflow_api_key ‚Üí ‡πÉ‡∏ä‡πâ Vision OCR ‡πÅ‡∏ó‡∏ô")
        return None, 0, []
    
    try:
        # Initialize client
        client = InferenceHTTPClient(
            api_url="https://detect.roboflow.com",
            api_key=api_key
        )
        
        # ‡πÄ‡∏ã‡∏ü‡∏£‡∏π‡∏õ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Roboflow (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ file path ‡∏´‡∏£‡∏∑‡∏≠ bytes)
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp:
            tmp.write(image_bytes)
            tmp_path = tmp.name
        
        try:
            # Run inference
            print("üî• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Roboflow AI ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç...")
            result = client.infer(tmp_path, model_id=model_id)
            predictions = result.get('predictions', [])
            
            if not predictions:
                print("‚ö†Ô∏è Roboflow ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‚Üí fallback to OCR")
                return None, 0, []
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° x coordinate (‡∏à‡∏≤‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤)
            sorted_preds = sorted(predictions, key=lambda p: p.get('x', 0))
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å class labels
            digit_sequence = ''.join([str(pred.get('class', '')) for pred in sorted_preds])
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì average confidence
            avg_confidence = sum([pred.get('confidence', 0) for pred in sorted_preds]) / len(sorted_preds)
            
            print(f"‚úÖ Roboflow: ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö '{digit_sequence}' (confidence: {avg_confidence:.2%}, ‡∏´‡∏•‡∏±‡∏Å: {len(sorted_preds)})")
            return digit_sequence, avg_confidence, sorted_preds
        
        finally:
            # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            try:
                os.remove(tmp_path)
            except:
                pass
    
    except Exception as e:
        # Log error ‡πÅ‡∏•‡πâ‡∏ß fallback to Vision OCR
        print(f"‚ö†Ô∏è Roboflow error: {str(e)[:100]} ‚Üí fallback to OCR")
        return None, 0, []

def _vision_read_text(processed_bytes):
    try:
        image = vision.Image(content=processed_bytes)
        ctx = vision.ImageContext(language_hints=["en"])
        resp = VISION_CLIENT.text_detection(image=image, image_context=ctx)
        if getattr(resp, "error", None) and resp.error.message: return "", resp.error.message
        if resp.text_annotations: return (resp.text_annotations[0].description or ""), ""
        
        resp2 = VISION_CLIENT.document_text_detection(image=image, image_context=ctx)
        txt = ""
        if resp2.full_text_annotation and resp2.full_text_annotation.text: txt = resp2.full_text_annotation.text
        return (txt or ""), ""
    except Exception as e:
        return "", str(e)

def ocr_process(image_bytes, config, debug=False, return_candidates=False, use_roboflow=True):
    decimal_places = int(config.get('decimals', 0) or 0)
    keyword = str(config.get('keyword', '') or '').strip()
    expected_digits = int(config.get('expected_digits', 0) or 0)
    
    # üî• ‡∏•‡∏≠‡∏á Roboflow Detection ‡∏Å‡πà‡∏≠‡∏ô (‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ OCR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö water meter)
    if use_roboflow and HAS_ROBOFLOW:
        try:
            # ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà preprocessed ROI (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            roi_bytes = image_bytes
            if config.get('roi_x1') and config.get('roi_x2'):
                roi_bytes = preprocess_image_cv(image_bytes, config, use_roi=True, variant="raw")
            
            digit_seq, confidence, preds = _roboflow_detect_digits(roi_bytes)
            
            if digit_seq and confidence > 0.6:  # threshold ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
                # ‡πÅ‡∏õ‡∏•‡∏á digit_sequence ‡πÄ‡∏õ‡πá‡∏ô float ‡∏ï‡∏≤‡∏° decimal_places
                try:
                    val_str = str(digit_seq).strip()
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ô sequence ‡πÅ‡∏•‡πâ‡∏ß
                    if '.' in val_str:
                        val = float(val_str)
                    else:
                        # ‡πÉ‡∏™‡πà‡∏à‡∏∏‡∏î‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡∏ï‡∏≤‡∏° config
                        if decimal_places > 0 and len(val_str) > decimal_places:
                            int_part = val_str[:-decimal_places]
                            dec_part = val_str[-decimal_places:]
                            val = float(f"{int_part}.{dec_part}")
                        else:
                            val = float(val_str)
                    
                    # Validate ‡∏ï‡∏≤‡∏° expected_digits
                    if expected_digits > 0:
                        digit_count = len(str(int(abs(val))))
                        if digit_count > expected_digits + 1:  # ‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‚Üí ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
                            raise ValueError("Too many digits")
                    
                    # üéØ Return ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ñ‡πâ‡∏≤ Roboflow ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                    print(f"üéØ ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Roboflow: {val} (raw: {val_str}, conf: {confidence:.2%})")
                    if return_candidates:
                        candidates = [{
                            "val": val,
                            "score": confidence * 1000,  # scale to match OCR scoring
                            "raw": val_str,
                            "method": "roboflow",
                            "confidence": confidence,
                            "predictions": len(preds)
                        }]
                        return val, candidates
                    else:
                        return val
                
                except (ValueError, ZeroDivisionError) as e:
                    print(f"‚ö†Ô∏è Roboflow parsing error: {e} ‚Üí fallback to OCR")
        
        except Exception as e:
            print(f"‚ö†Ô∏è Roboflow exception: {str(e)[:100]} ‚Üí fallback to OCR")
    
    # ‚úÖ Fallback: ‡πÉ‡∏ä‡πâ Vision OCR ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
    print("üîÑ ‡πÉ‡∏ä‡πâ Vision OCR (Google Cloud Vision)...")
    attempts = [
        ("ROI_auto",  True,  "auto"),
        ("ROI_raw",   True,  "raw"),
        ("ROI_soft",  True,  "soft"),
        ("ROI_inv",   True,  "invert"),
        ("FULL_auto", False, "auto"),
        ("FULL_raw",  False, "raw"),
    ]

    def _has_digits(s: str) -> bool:
        return bool(s) and any(c.isdigit() for c in s)

    def check_digits_len(val: float) -> int:
        """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å' ‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°)"""
        try:
            return len(str(int(abs(float(val)))))
        except Exception:
            return 0

    def check_digits_ok(val: float) -> bool:
        """‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö '‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô' ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ"""
        if val is None:
            return False
        # ‚ùå ‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ï‡∏¥‡∏î‡∏•‡∏ö
        if float(val) < 0:
            return False
        if expected_digits <= 0:
            return True
        ln = check_digits_len(val)
        # ‡∏¢‡∏±‡∏á‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ +1 ‡πÑ‡∏î‡πâ (‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡πÄ‡∏•‡∏Ç‡πÇ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÇ‡∏î‡∏ô‡∏´‡∏±‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏ô‡∏±‡∏Å
        return 1 <= ln <= expected_digits + 1

    def looks_like_spec_context(text: str, start: int, end: int) -> bool:
        """‡∏î‡∏π‡∏£‡∏≠‡∏ö ‡πÜ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (Hz/V/A/IP/Rev) ‡πÑ‡∏´‡∏°"""
        ctx = text[max(0, start - 12):min(len(text), end + 12)].lower()
        # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ ‡πÜ ‡∏°‡∏µ kWh ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡πÄ‡∏õ‡∏Ñ (‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏î‡πâ)
        if "kwh" in ctx or "kw h" in ctx:
            return False
        bad = ["hz", "volt", " v", "v ", "amp", " a", "a ", "class", "ip", "rev", "rpm", "phase", "3x", "indoor"]
        return any(b in ctx for b in bad)

    common_noise = {10, 30, 50, 60, 100, 220, 230, 240, 380, 400, 415, 1000, 10000}

    best_val = None
    best_score = -10**9

    # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö candidate ‡∏Ç‡πâ‡∏≤‡∏° attempts (‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ History Guard)
    all_candidates = []
    TOPK = 60  # ‡∏Å‡∏±‡∏ô list ‡πÇ‡∏ï‡πÄ‡∏Å‡∏¥‡∏ô
    
    for tag, use_roi, variant in attempts:
        processed = preprocess_image_cv(image_bytes, config, use_roi=use_roi, variant=variant)
        txt, err = _vision_read_text(processed)
        if not txt or not _has_digits(txt):
            continue
            
        raw_text = (txt or "").replace("\n", " ")
        raw_text = re.sub(r"\.{2,}", ".", raw_text)

        # preprocess ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç + ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡πÅ‡∏Å‡πâ‡∏ö‡∏±‡πä‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á)
        full_text = preprocess_text(raw_text)
        full_text = re.sub(r"\.{2,}", ".", full_text)

        # ‡∏ï‡∏±‡∏î‡∏õ‡∏µ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏û‡∏±‡∏á (‡∏à‡∏∞‡πÉ‡∏ä‡πâ full_text ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà)
        scan_text = re.sub(r"\b202[0-9]\b|\b256[0-9]\b", "", full_text)
        scan_text = re.sub(r"\.{2,}", ".", scan_text)

        candidates = []

        # ---- ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏ï‡∏≤‡∏° attempt (‡∏Å‡∏±‡∏ô FULL ‡∏†‡∏≤‡∏û‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ä‡∏ô‡∏∞ ROI ‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô) ----
        attempt_bonus = 0
        if use_roi:
            attempt_bonus += 80
        if variant in ("soft", "auto"):
            attempt_bonus += 10
        
        # ---- 1) ‡∏•‡∏≠‡∏á‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å keyword ‡∏Å‡πà‡∏≠‡∏ô (‡πÅ‡∏°‡πà‡∏ô‡∏™‡∏∏‡∏î) ----
        if keyword:
            kw = re.escape(keyword)
            patterns = [
                kw + r"[^\d]*((?:\d|O|o|l|I|\|)+[\.,]?\d*)",
                r"((?:\d|O|o|l|I|\|)+[\.,]?\d*)[^\d]*" + kw
            ]
            for pat in patterns:
                match = re.search(pat, raw_text, re.IGNORECASE)
                if match:
                    val_str = match.group(1)
                    val_str = val_str.replace("O", "0").replace("o", "0").replace("l", "1").replace("I", "1").replace("|", "1")
                    val_str = normalize_number_str(val_str, decimal_places)
                    try:
                        val = float(val_str)
                        if decimal_places > 0 and "." not in val_str:
                            val = val / (10 ** decimal_places)
                        if check_digits_ok(val):
                            score = 900 + attempt_bonus  # ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏à‡∏≠ keyword
                            ln = check_digits_len(val)

                            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô "‡πÉ‡∏Å‡∏•‡πâ expected_digits" ‡∏ä‡∏ô‡∏∞ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏•‡∏Ç‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô‡∏∞)
                            if expected_digits > 0:
                                score += max(0, 160 - abs(ln - expected_digits) * 60)
                                if ln == expected_digits:
                                    score += 80
                                if ln == expected_digits + 1:
                                    score -= 80  # ‡∏´‡∏±‡∏Å‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏£‡∏ì‡∏µ +1
                            candidates.append({"val": float(val), "score": score})
                    except Exception:
                            pass

        # ---- 2) ‡∏Å‡∏ß‡∏≤‡∏î‡πÄ‡∏•‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ----
        for m in re.finditer(r"(?:\d{1,3}(?:,\d{3})+|\d+)(?:\.\d+)?", scan_text):
            n_str = m.group(0)

            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏Ç‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏° (‡πÉ‡∏ä‡πâ scan_text ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
            if looks_like_spec_context(scan_text, m.start(), m.end()):
                continue

            n_str2 = normalize_number_str(n_str, decimal_places)
            if not n_str2:
                continue
            
            try:
                val = float(n_str2) if "." in n_str2 else float(int(n_str2))
                if decimal_places > 0 and "." not in n_str2:
                    val = val / (10 ** decimal_places)

                # ‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
                if float(val) < 0:
                    continue

                # ‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏Ç noise ‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ keyword ‡∏ä‡πà‡∏ß‡∏¢
                if int(abs(val)) in common_noise and not keyword:
                    continue

                if not check_digits_ok(val):
                    continue

                ln = check_digits_len(val)
                score = 200 + attempt_bonus

                # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô "‡πÉ‡∏Å‡∏•‡πâ expected_digits" ‡∏ä‡∏ô‡∏∞
                if expected_digits > 0:
                    score += max(0, 140 - abs(ln - expected_digits) * 50)
                    if ln == expected_digits:
                        score += 60
                    if ln == expected_digits + 1:
                        score -= 70
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î expected_digits ‡πÉ‡∏´‡πâ‡∏û‡∏≠‡πÉ‡∏ä‡πâ logic ‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏ö‡∏≤ ‡πÜ)
                    score += min(ln, 10) * 6

                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° ‡πÉ‡∏´‡πâ‡∏ö‡∏ß‡∏Å‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢
                if decimal_places > 0 and "." in n_str2:
                    score += 20

                candidates.append({"val": float(val), "score": score, "tag": tag})
            except Exception:
                continue
            
        if candidates:
            pick = max(candidates, key=lambda x: x["score"])
            if pick["score"] > best_score:
                best_score = pick["score"]
                best_val = pick["val"]
            # ‚úÖ ‡∏£‡∏ß‡∏° candidates ‡∏Ç‡πâ‡∏≤‡∏° attempts (‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ topK)
            all_candidates.extend(candidates)
            all_candidates.sort(key=lambda x: float(x.get("score", 0)), reverse=True)
            if len(all_candidates) > TOPK:
                all_candidates = all_candidates[:TOPK]
                
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡πá‡∏û‡∏≠ (‡∏Å‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Vision ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö)
            if best_score >= 980:
                break

    final_val = float(best_val) if best_val is not None else 0.0
    
    # ‚úÖ dedupe candidates ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤ val (‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
    if return_candidates:
        by_val = {}
        for c in all_candidates:
            try:
                v = float(c.get("val"))
            except Exception:
                continue
            key = round(v, max(0, decimal_places) + 2)
            if key not in by_val or float(c.get("score", 0)) > float(by_val[key].get("score", 0)):
                by_val[key] = {"val": v, "score": float(c.get("score", 0)), "tag": c.get("tag", "")}
        
        cand_out = sorted(by_val.values(), key=lambda x: x["score"], reverse=True)[:25]
        return final_val, cand_out
        
    return final_val
         
# =========================================================
# --- üî≥ QR + REF IMAGE HELPERS (Mobile) ---
# =========================================================
REF_IMAGE_FOLDER = "ref_images"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô Bucket

def get_ref_image_url(point_id: str) -> str:
    pid = str(point_id).strip().upper()
    return f"https://storage.googleapis.com/{BUCKET_NAME}/{REF_IMAGE_FOLDER}/{pid}.jpg"

def decode_qr(image_bytes: bytes):
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ point_id ‡∏à‡∏≤‡∏Å QR (‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô None)"""
    try:
        arr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
        if img is None:
            return None
        detector = cv2.QRCodeDetector()
        data, _, _ = detector.detectAndDecode(img)
        data = (data or "").strip()
        return data.upper() if data else None
    except:
        return None

def infer_meter_type(config: dict) -> str:
    """‡πÄ‡∏î‡∏≤ meter_type ‡∏à‡∏≤‡∏Å config ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏≠‡∏Å‡∏ú‡∏¥‡∏î"""
    blob = f"{config.get('type','')} {config.get('name','')}".lower()
    if ("‡∏ô‡πâ‡∏≥" in blob) or ("water" in blob) or ("‡∏õ‡∏£‡∏∞‡∏õ‡∏≤" in blob):
        return "Water"
    return "Electric"
# =========================================================
# --- üñ•Ô∏è DASHBOARD SCREENSHOT OCR (FLOW 1-3) ---
# =========================================================

# ‡∏õ‡∏£‡∏±‡∏ö point_id default ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö PointsMaster ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
_DASH_DEFAULT_POINT_MAP = {
    # FLOW 1
    (1, "pressure_bar"): "C_Bar_FLOW_1",
    (1, "flowrate_m3h"): "D_m_h_FLOW_1",
    (1, "flow_total_m3"): "J_FLOW_1",
    # FLOW 2
    (2, "pressure_bar"): "E_Bar_FLOW_2",
    (2, "flowrate_m3h"): "F_m_h_FLOW_2",
    (2, "flow_total_m3"): "K_FLOW_2",
    # FLOW 3
    (3, "pressure_bar"): "G_Bar_FLOW_3",
    (3, "flowrate_m3h"): "H_m_h_FLOW_3",
    (3, "flow_total_m3"): "L_FLOW_3",
}

_NUM_RE = re.compile(r"^[-+]?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^[-+]?\d+(?:\.\d+)?$")

def _looks_like_number(s: str) -> bool:
    if s is None:
        return False
    s = str(s).strip()
    if not s:
        return False
    if ":" in s or "/" in s:
        return False
    s2 = s.replace("O", "0").replace("o", "0")
    return bool(_NUM_RE.match(s2))

def _parse_number(s: str):
    if s is None:
        return None
    s = str(s).strip()
    if not s:
        return None
    s = s.replace("O", "0").replace("o", "0").replace(",", "")
    try:
        return float(s)
    except Exception:
        return None

def _cv2_decode_bytes(image_bytes: bytes):
    arr = np.frombuffer(image_bytes, np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

def _cv2_encode_jpg(img, quality: int = 92) -> bytes:
    ok, buf = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)])
    return buf.tobytes() if ok else b""

def _upscale_for_ocr(img, max_side: int = 2200):
    if img is None:
        return img
    h, w = img.shape[:2]
    scale = 2.0
    if max(h, w) * scale > max_side:
        scale = max_side / float(max(h, w))
    if scale <= 1.05:
        return img
    return cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_CUBIC)

def _vision_tokens(image_bytes: bytes, lang_hints=("en",)):
    image = vision.Image(content=image_bytes)
    ctx = vision.ImageContext(language_hints=list(lang_hints))
    resp = VISION_CLIENT.text_detection(image=image, image_context=ctx)
    if resp.error.message:
        raise RuntimeError(resp.error.message)

    ann = resp.text_annotations
    tokens = []
    for a in ann[1:]:
        txt = (a.description or "").strip()
        if not txt:
            continue
        vs = a.bounding_poly.vertices
        xs = [v.x for v in vs]
        ys = [v.y for v in vs]
        x1, y1, x2, y2 = min(xs), min(ys), max(xs), max(ys)
        tokens.append({
            "text": txt,
            "x1": x1, "y1": y1, "x2": x2, "y2": y2,
            "cx": (x1 + x2) / 2.0,
            "cy": (y1 + y2) / 2.0,
            "h": max(1.0, (y2 - y1)),
        })
    full_text = ann[0].description if ann else ""
    return tokens, full_text

def _norm_token_text(s: str) -> str:
    return re.sub(r"[^A-Z0-9]", "", str(s).upper())

def _suggest_dashboard_crop(tokens, w: int, h: int):
    # default crop: ‡∏ï‡∏±‡∏î sidebar + top bar
    def_roi = (int(w * 0.18), int(h * 0.18), int(w * 0.99), int(h * 0.92))
    if not tokens:
        return def_roi

    anchors = []
    for t in tokens:
        tn = _norm_token_text(t.get("text", ""))
        if any(k in tn for k in ["FLOW", "PRESSURE", "FLOWRATE", "FLOWTOTAL", "TOTALM3", "M3H", "BAR"]):
            anchors.append(t)

    if not anchors:
        return def_roi

    x1 = min(t["x1"] for t in anchors)
    y1 = min(t["y1"] for t in anchors)
    x2 = max(t["x2"] for t in anchors)
    y2 = max(t["y2"] for t in anchors)

    pad_x_left  = int(0.05 * w)
    pad_x_right = int(0.35 * w)
    pad_y_top   = int(0.10 * h)
    pad_y_bot   = int(0.45 * h)

    rx1 = max(0, x1 - pad_x_left)
    ry1 = max(0, y1 - pad_y_top)
    rx2 = min(w, x2 + pad_x_right)
    ry2 = min(h, y2 + pad_y_bot)

    if (rx2 - rx1) < int(0.35 * w) or (ry2 - ry1) < int(0.20 * h):
        return def_roi
    return (rx1, ry1, rx2, ry2)

def _join_adjacent_numeric_tokens(num_tokens, gap_px: int = 14):
    if not num_tokens:
        return []
    num_tokens = sorted(num_tokens, key=lambda t: t["x1"])
    merged = []
    cur = dict(num_tokens[0])
    for t in num_tokens[1:]:
        gap = t["x1"] - cur["x2"]
        if gap >= 0 and gap <= gap_px:
            cur["text"] = f"{cur['text']}{t['text']}"
            cur["x2"] = max(cur["x2"], t["x2"])
            cur["y1"] = min(cur.get("y1", 0), t.get("y1", 0))
            cur["y2"] = max(cur.get("y2", 0), t.get("y2", 0))
        else:
            merged.append(cur)
            cur = dict(t)
    merged.append(cur)

    for m in merged:
        m["cx"] = (m["x1"] + m["x2"]) / 2.0
        m["cy"] = (m["y1"] + m["y2"]) / 2.0
        m["h"] = max(1.0, (m["y2"] - m["y1"]))
    return merged

def extract_dashboard_flow_values(image_bytes: bytes, debug: bool = False):
    """‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ FLOW 1-3 ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û Dashboard
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ list[dict]: flow, pressure_bar, flowrate_m3h, flow_total_m3, status
    """
    img = _cv2_decode_bytes(image_bytes)
    if img is None:
        rows = [{"flow": f"FLOW {i}", "pressure_bar": None, "flowrate_m3h": None, "flow_total_m3": None, "status": "BAD_IMAGE"} for i in (1,2,3)]
        return (rows, {"reason": "cv2_decode_failed"}) if debug else rows

    h, w = img.shape[:2]

    # pass1: full OCR ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ ROI
    try:
        tokens1, full_text1 = _vision_tokens(image_bytes, lang_hints=("en",))
    except Exception as e:
        rows = [{"flow": f"FLOW {i}", "pressure_bar": None, "flowrate_m3h": None, "flow_total_m3": None, "status": "VISION_ERROR"} for i in (1,2,3)]
        return (rows, {"error": str(e)}) if debug else rows

    x1, y1, x2, y2 = _suggest_dashboard_crop(tokens1, w, h)
    crop = img[y1:y2, x1:x2].copy()
    crop = _upscale_for_ocr(crop)
    crop_bytes = _cv2_encode_jpg(crop, quality=92)

    # pass2: OCR ‡∏ö‡∏ô crop
    try:
        tokens, full_text = _vision_tokens(crop_bytes, lang_hints=("en",))
    except Exception:
        tokens, full_text = tokens1, full_text1

    flow_rows = {}
    for t in tokens:
        tn = _norm_token_text(t.get("text", ""))
        m = re.match(r"^FLOW([123])$", tn)
        if m:
            n = int(m.group(1))
            flow_rows[n] = {"y": t["cy"], "h": t["h"], "x_right": t["x2"]}

    # FLOW + digit ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
    if len(flow_rows) < 3:
        flow_tokens = [t for t in tokens if _norm_token_text(t.get("text","")) == "FLOW"]
        digit_tokens = [t for t in tokens if str(t.get("text","")).strip() in ("1","2","3")]
        for d in digit_tokens:
            n = int(str(d["text"]))
            if n in flow_rows:
                continue
            best = None
            best_score = 1e9
            for f in flow_tokens:
                dx = abs(d["cx"] - f["cx"])
                dy = abs(d["cy"] - f["cy"])
                score = dx + dy * 1.2
                if score < best_score and dx < 120 and dy < 120:
                    best, best_score = f, score
            if best:
                y = (best["cy"] + d["cy"]) / 2.0
                hh = max(best["h"], d["h"]) * 1.8
                xr = max(best["x2"], d["x2"])
                flow_rows[n] = {"y": y, "h": hh, "x_right": xr}

    out_rows = []
    for n in (1,2,3):
        row = {"flow": f"FLOW {n}", "pressure_bar": None, "flowrate_m3h": None, "flow_total_m3": None, "status": "NOT_FOUND"}
        meta = flow_rows.get(n)
        if not meta:
            out_rows.append(row)
            continue

        band = max(22.0, meta["h"] * 1.2)
        x_min = meta["x_right"] + 8

        row_tokens = [t for t in tokens if abs(t["cy"] - meta["y"]) <= band and t["x1"] >= x_min]
        num_tokens = [t for t in row_tokens if _looks_like_number(t.get("text",""))]
        num_tokens = _join_adjacent_numeric_tokens(num_tokens, gap_px=14)
        num_tokens = [t for t in num_tokens if _looks_like_number(t.get("text",""))]
        num_tokens = sorted(num_tokens, key=lambda t: t["cx"])

        if len(num_tokens) >= 3:
            p   = _parse_number(num_tokens[0]["text"])
            fr  = _parse_number(num_tokens[1]["text"])
            # ‡πÉ‡∏ä‡πâ num_tokens[3] (TOT1) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ 4 ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ num_tokens[2]
            tot = _parse_number(num_tokens[3]["text"]) if len(num_tokens) >= 4 else _parse_number(num_tokens[2]["text"])
            row.update({
                "pressure_bar": p,
                "flowrate_m3h": fr,
                "flow_total_m3": tot,
                "status": "OK" if (p is not None and fr is not None and tot is not None) else "PARTIAL",
            })

        out_rows.append(row)

    dbg = {
        "roi": {"x1": x1, "y1": y1, "x2": x2, "y2": y2},
        "flow_rows": flow_rows,
        "full_text": (full_text or "")[:2000],
        "full_text_pass1": (full_text1 or "")[:2000],
        "tokens_count": len(tokens),
    }
    return (out_rows, dbg) if debug else out_rows

# =========================================================
# --- üì∏ BULK IMAGE OCR: FIND point_id FROM PHOTO ---
# =========================================================

def _norm_pid_key(s: str) -> str:
    s = str(s or "").upper().strip()
    s = s.replace("-", "_")
    s = re.sub(r"\s+", "_", s)          # space -> _
    s = re.sub(r"[^A-Z0-9_]", "", s)    # ‡∏ï‡∏±‡∏î‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡πÅ‡∏õ‡∏•‡∏Å‡πÜ
    s = re.sub(r"_+", "_", s).strip("_")
    return s

@st.cache_data(ttl=3600)
def build_pid_norm_map():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö match point_id ‡πÅ‡∏ö‡∏ö‡∏ó‡∏ô OCR ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô"""
    pm = load_points_master() or []
    norm_map = {}
    for r in pm:
        pid = str(r.get("point_id", "")).strip().upper()
        if not pid:
            continue
        norm_map[_norm_pid_key(pid)] = pid
    return norm_map

def _crop_bottom_bytes(image_bytes: bytes, frac: float = 0.40) -> bytes:
    """‡∏Ñ‡∏£‡∏≠‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ (‡∏ï‡∏£‡∏á‡πÄ‡∏ó‡∏õ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ OCR point_id ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô/‡πÄ‡∏£‡πá‡∏ß"""
    img = _cv2_decode_bytes(image_bytes)
    if img is None:
        return image_bytes
    h, w = img.shape[:2]
    y1 = int(h * (1.0 - frac))
    crop = img[y1:h, 0:w].copy()
    crop = _upscale_for_ocr(crop, max_side=2200)
    out = _cv2_encode_jpg(crop, quality=92)
    return out or image_bytes

def find_point_id_from_text(ocr_text: str, norm_map: dict):
    t = _norm_pid_key(ocr_text)
    if not t:
        return None

    # 1) exact substring match (‡πÄ‡∏£‡πá‡∏ß+‡πÅ‡∏°‡πà‡∏ô)
    best = None
    best_len = -1
    for nkey, orig in norm_map.items():
        if nkey and nkey in t:
            if len(nkey) > best_len:
                best = orig
                best_len = len(nkey)
    if best:
        return best

    # 2) fuzzy ‡∏à‡∏≤‡∏Å pattern ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô point_id
    cand = re.findall(r"[A-Z]{1,3}_[A-Z0-9]{1,10}(?:_[A-Z0-9]{1,10}){1,5}", t)
    if not cand:
        return None

    best_score = 0.0
    best_pid = None
    for c in cand[:12]:
        for nkey, orig in norm_map.items():
            sc = SequenceMatcher(None, c, nkey).ratio()
            if sc > best_score:
                best_score = sc
                best_pid = orig

    return best_pid if best_score >= 0.78 else None

def extract_point_id_from_image(image_bytes: bytes, norm_map: dict):
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (point_id ‡∏´‡∏£‡∏∑‡∏≠ None, ocr_text ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)"""
    # pass1: OCR ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏•‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô
    btm = _crop_bottom_bytes(image_bytes, frac=0.40)
    txt, _err = _vision_read_text(btm)
    pid = find_point_id_from_text(txt, norm_map)
    if pid:
        return pid, txt

    # pass2: fallback OCR ‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û
    txt2, _err2 = _vision_read_text(image_bytes)
    pid2 = find_point_id_from_text(txt2, norm_map)
    return pid2, txt2

    
# =========================================================
# --- ‚úÖ HISTORY GUARD (for cumulative meters) ---
# =========================================================

@st.cache_data(ttl=300)
def load_dailyreadings_tail(limit=4000):
    """‡πÇ‡∏´‡∏•‡∏î DailyReadings ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≤‡∏¢ ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤/‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤"""
    sh = gc.open(DB_SHEET_NAME)
    ws = sh.worksheet("DailyReadings")
    vals = ws.get_all_values()
    if not vals:
        return pd.DataFrame()

    header = vals[0]
    rows = vals[1:]
    if limit and len(rows) > limit:
        rows = rows[-limit:]
    df = pd.DataFrame(rows, columns=header)
    return df

def _norm_pid(pid: str) -> str:
    return str(pid or "").strip().upper()

def is_cumulative_meter(config: dict) -> bool:
    """
    Heuristic: ‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏° (Totalizer / ‡πÄ‡∏•‡∏Ç‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå / kWh / total / m3 ‡∏Ø‡∏•‡∏Ø)
    ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢: ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô "‡∏™‡∏∞‡∏™‡∏°" ‡πÅ‡∏•‡∏∞ decimals == 0 ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    """
    name = str(config.get("name", "") or "")
    typ  = str(config.get("type", "") or "")
    kw   = str(config.get("keyword", "") or "")
    blob = f"{name} {typ} {kw}".lower()

    # ‡πÄ‡∏ô‡πâ‡∏ô‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏°‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ flowrate/pressure
    hit = any(k in blob for k in [
        "‡πÄ‡∏•‡∏Ç‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå", "meter", "total", "totalizer", "tot", "kwh", "m3", "m¬≥"
    ])
    dec = int(config.get("decimals", 0) or 0)

    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° (pressure/flowrate) ‡∏°‡∏±‡∏Å‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏∞‡∏™‡∏°
    if dec > 0:
        return False

    return hit

def get_last_good_value(point_id: str, upto_date):
    """
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Manual_Value ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà FLAGGED) ‡∏ó‡∏µ‡πà timestamp <= upto_date 23:59:59
    """
    df = load_dailyreadings_tail(limit=4000)
    if df.empty:
        return None

    pid = _norm_pid(point_id)

    # normalize columns
    if "point_id" not in df.columns or "timestamp" not in df.columns:
        return None

    df["point_id"] = df["point_id"].astype(str).map(_norm_pid)
    df["Status"] = df.get("Status", "").astype(str).str.strip().str.upper()

    # filter pid
    df = df[df["point_id"] == pid]
    if df.empty:
        return None

    # drop flagged
    df = df[~df["Status"].str.contains("FLAGGED", na=False)]
    if df.empty:
        return None

    # parse timestamp
    df["timestamp_dt"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df = df.dropna(subset=["timestamp_dt"])
    if df.empty:
        return None

    cutoff = pd.to_datetime(str(upto_date) + " 23:59:59")
    df = df[df["timestamp_dt"] <= cutoff]
    if df.empty:
        return None

    df = df.sort_values("timestamp_dt")
    last = pd.to_numeric(df.iloc[-1].get("Manual_Value", None), errors="coerce")
    if pd.isna(last):
        return None
    return float(last)

def estimate_max_delta(point_id: str, upto_date, fallback=20000, max_cap=500000):
    """
    ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏û‡∏î‡∏≤‡∏ô 3 ‡∏ß‡∏¥‡∏ò‡∏µ:
    1) Q95 (95th percentile): ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ outlier ‡∏ó‡∏≥‡∏•‡∏≤‡∏¢
    2) Median * 6: ‡∏¢‡∏≠‡∏°‡∏Ñ‡πà‡∏≤ spike ‡∏ö‡πâ‡∏≤‡∏á
    3) Max daily increase * 1.2: ‡∏¢‡∏≠‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤ spike ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    
    ‡πÄ‡∏≠‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏™‡∏≤‡∏°‡∏ß‡∏¥‡∏ò‡∏µ ‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô
    """
    df = load_dailyreadings_tail(limit=8000)
    if df.empty:
        return fallback

    pid = _norm_pid(point_id)
    if "point_id" not in df.columns or "timestamp" not in df.columns:
        return fallback

    df["point_id"] = df["point_id"].astype(str).map(_norm_pid)
    df["Status"] = df.get("Status", "").astype(str).str.strip().str.upper()
    df = df[df["point_id"] == pid]
    df = df[~df["Status"].str.contains("FLAGGED", na=False)]
    if df.empty:
        return fallback

    df["timestamp_dt"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df = df.dropna(subset=["timestamp_dt"])
    cutoff = pd.to_datetime(str(upto_date) + " 23:59:59")
    df = df[df["timestamp_dt"] <= cutoff]
    if df.empty:
        return fallback

    df = df.sort_values("timestamp_dt").tail(60)  # ‡πÄ‡∏≠‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 60 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
    vals = pd.to_numeric(df.get("Manual_Value", None), errors="coerce").dropna().astype(float).tolist()
    if len(vals) < 4:
        return fallback

    diffs = []
    for a, b in zip(vals[:-1], vals[1:]):
        d = b - a
        if d >= 0:  # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏°)
            diffs.append(d)

    if len(diffs) < 3:
        return fallback

    diffs = np.array(diffs, dtype=float)
    
    # ‚úÖ ‡πÉ‡∏´‡∏°‡πà: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏û‡∏î‡∏≤‡∏ô 3 ‡∏ß‡∏¥‡∏ò‡∏µ
    q95 = float(np.quantile(diffs, 0.95))
    med = float(np.median(diffs))
    max_diff = float(np.max(diffs))
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ 1: Q95 * 3 (‡∏õ‡∏Å‡∏ï‡∏¥)
    est1 = q95 * 3.0
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ 2: Median * 6 (‡∏¢‡∏≠‡∏°‡∏Ñ‡πà‡∏≤ spike)
    est2 = med * 6.0
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ 3: Max * 1.2 (‡∏¢‡∏≠‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î)
    est3 = max_diff * 1.2
    
    # ‡πÄ‡∏≠‡∏≤‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô)
    est = max(est1, est2, est3)
    
    # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢: ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ fallback * 0.25, ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å max_cap
    est = min(est, float(max_cap))
    est = max(est, float(fallback * 0.25))
    
    return int(est)

def pick_by_history(best_val: float, candidates: list, prev_val: float, max_delta: int):
    """
    ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å candidate ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤:
      1) Original AI score (candidate["score"])
      2) History compatibility score (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô)
      3) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö growth rate ‡πÉ‡∏´‡πâ‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (picked_value, message, changed_from_ai)
    """
    if prev_val is None or not candidates:
        return best_val, "", False

    lo = float(prev_val)
    hi = float(prev_val) + float(max_delta)

    # ‚úÖ ‡πÉ‡∏´‡∏°‡πà: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì history compatibility score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å candidate
    scored_cands = []
    for c in candidates:
        try:
            v = float(c.get("val"))
        except Exception:
            continue

        ai_score = float(c.get("score", 0))
        
        # History compatibility: ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ v ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô
        # - ‡∏ñ‡πâ‡∏≤ v ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô [prev_val, prev_val + max_delta] -> compatible
        # - ‡∏ñ‡πâ‡∏≤ v ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ prev_val -> ‡∏≠‡∏≤‡∏à‡∏ú‡∏¥‡∏î (‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏°‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á)
        # - ‡∏ñ‡πâ‡∏≤ v ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ prev_val + max_delta -> ‡∏≠‡∏≤‡∏à outlier ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î
        
        if v < lo:
            # ‚ùå ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô: ‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡πÜ (‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏°‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏•‡∏á)
            hist_score = -1000.0
        elif v <= hi:
            # ‚úÖ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á [prev_val, prev_val + max_delta]: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°
            # - ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ prev_val ‡∏°‡∏≤‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥)
            # - ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ hi ‡∏°‡∏≤‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡∏Å‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥)
            delta_from_prev = v - lo
            ratio_in_range = delta_from_prev / float(max_delta)  # 0.0 = ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î, 1.0 = ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            
            # ‡∏´‡∏≤‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏î‡∏¥‡∏° (ratio < 0.3) -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á (‡∏õ‡∏Å‡∏ï‡∏¥)
            # ‡∏´‡∏≤‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á (0.3 <= ratio < 0.7) -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            # ‡∏´‡∏≤‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (ratio >= 0.7) -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡∏Å‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥)
            if ratio_in_range < 0.3:
                hist_score = 300.0 + (0.3 - ratio_in_range) * 500.0  # 300-450
            elif ratio_in_range < 0.7:
                hist_score = 200.0 - (ratio_in_range - 0.3) * 200.0  # 200-80
            else:
                hist_score = 80.0 - (ratio_in_range - 0.7) * 400.0   # 80-0
        else:
            # ‚ö†Ô∏è ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏î‡∏≤‡∏ô: ‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ö‡πâ‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á)
            excess = v - hi
            hist_score = max(0.0, 150.0 - excess * 2.0)
        
        # ‡∏£‡∏ß‡∏° AI score + history score (AI score 70% + history score 30%)
        combined_score = (ai_score * 0.7) + (hist_score * 0.3)
        scored_cands.append({
            **c,
            "val": v,
            "ai_score": ai_score,
            "hist_score": hist_score,
            "combined_score": combined_score,
        })

    if not scored_cands:
        return best_val, "", False

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å candidate ‡∏ó‡∏µ‡πà‡∏°‡∏µ combined_score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    scored_cands.sort(key=lambda x: x["combined_score"], reverse=True)
    picked_c = scored_cands[0]
    picked_val = float(picked_c["val"])
    hist_sc = float(picked_c["hist_score"])
    comb_sc = float(picked_c["combined_score"])

    changed = (best_val is not None) and (abs(picked_val - float(best_val)) > 1e-9)
    
    # ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° feedback ‡∏≠‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    if picked_val >= lo and picked_val <= hi:
        delta_pct = ((picked_val - lo) / float(max_delta) * 100) if max_delta > 0 else 0
        msg = f"‚úÖ History Guard: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç {picked_val:.0f} (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô {delta_pct:.0f}% | combined_score={comb_sc:.0f})"
    elif picked_val < lo:
        msg = f"‚ö†Ô∏è History Guard: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å {picked_val:.0f} (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô {lo:.0f}) - ‡∏≠‡∏≤‡∏à‡∏ú‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö!"
        return best_val, msg, False  # ‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò candidate ‡∏ô‡∏µ‡πâ
    else:
        msg = f"‚ö†Ô∏è History Guard: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å {picked_val:.0f} (‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏î‡∏≤‡∏ô {hi:.0f}) - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡∏Å‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥"
    
    return picked_val, msg, changed

def apply_history_guard(point_id: str, best_val: float, candidates: list, config: dict, selected_date):
    """
    ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÉ‡∏ä‡πâ History Guard ‡∏Å‡∏±‡∏ö‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏°
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: (final_value, message)
    - message: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• feedback ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏õ (‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à ‡∏Ø‡∏•‡∏Ø)
    """
    if not is_cumulative_meter(config):
        return best_val, ""

    prev = get_last_good_value(point_id, selected_date - timedelta(days=1))
    if prev is None:
        return best_val, "‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ AI ‡∏ï‡∏£‡∏á ‡πÜ)"

    max_delta = estimate_max_delta(point_id, selected_date - timedelta(days=1), fallback=20000)
    picked, msg, _changed = pick_by_history(best_val, candidates, prev_val=prev, max_delta=max_delta)
    
    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° feedback ‡∏ß‡πà‡∏≤ "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô"
    if msg.startswith("‚úÖ"):
        msg += f" (‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô={prev:.0f})"
    
    return picked, msg

# =========================================================
# --- UI LOGIC ---
# =========================================================
def reset_emp_meter_state():
    st.session_state.emp_ai_value = None
    st.session_state.emp_img_hash = ""
    st.session_state.emp_ai_msg = ""
    
    # ‚úÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏û‡∏¥‡πà‡∏° nonce ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Streamlit ‡∏™‡∏£‡πâ‡∏≤‡∏á widget ‡πÉ‡∏´‡∏°‡πà
    st.session_state.emp_nonce = int(st.session_state.get("emp_nonce", 0)) + 1

mode = st.sidebar.radio(
    "üîß ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
    ["üìù ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏à‡∏î‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå",
     "üì∏ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô (‡∏°‡∏µ point_id ‡πÉ‡∏ô‡∏£‡∏π‡∏õ)",
     "üì• ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Excel (SCADA Export)",
     "üñ•Ô∏è Dashboard Screenshot (OCR)",
     "ÔøΩÔ∏è SQL Server (CUTEST SCADA - Test)",
     "ÔøΩüëÆ‚Äç‚ôÇÔ∏è Admin Approval"]
)
if mode == "üìù ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏à‡∏î‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå":
    st.title("Smart Meter System")
    st.markdown("### Water treatment Plant - Borthongindustrial")
    st.caption("Version 6.2 (QR-first for Mobile + Skip Confirm)")

    # --- session state ---
    if 'confirm_mode' not in st.session_state: st.session_state.confirm_mode = False
    if 'warning_msg' not in st.session_state: st.session_state.warning_msg = ""
    if 'last_manual_val' not in st.session_state: st.session_state.last_manual_val = 0.0

    if "emp_step" not in st.session_state: st.session_state.emp_step = "SCAN_QR"
    if "emp_point_id" not in st.session_state: st.session_state.emp_point_id = ""

    # ‚úÖ Step A: ‡πÄ‡∏û‡∏¥‡πà‡∏° nonce state (‡∏ß‡∏≤‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)
    if "emp_nonce" not in st.session_state:
        st.session_state.emp_nonce = 0

    all_meters = load_points_master()
    if not all_meters:
        st.error("‚ùå ‡πÇ‡∏´‡∏•‡∏î PointsMaster ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")
        st.stop()

    # --- ‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ö‡∏ô‡∏™‡∏∏‡∏î (‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô) ---
    c_insp, c_date = st.columns(2)
    with c_insp:
        inspector = st.text_input("‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à", "Admin", key="emp_inspector")
    with c_date:
        selected_date = st.date_input(
            "üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏•‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ)",
            value=get_thai_time().date(),
            key="emp_date"
        )

    # =========================================================
    # ‚úÖ (2) Progress + Missing Alert (Sidebar)
    # =========================================================
    prog = get_waterreport_progress_snapshot(selected_date)
    done_set = set(prog.get("done_set") or [])
    done_val_map = dict(prog.get("value_map") or {})
    total = int(prog.get("total", 0) or 0)
    filled = int(prog.get("filled", 0) or 0)
    ratio = (filled / total) if total else 0.0
    st.sidebar.caption(
        f"‡∏ï‡∏±‡πâ‡∏á report_col ‡πÅ‡∏•‡πâ‡∏ß: {int(prog.get('total_report',0) or 0)} | "
        f"‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á: {int(prog.get('config_missing',0) or 0)}"
    )

    st.sidebar.markdown("## ‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏Ñ‡πà‡∏≤ (‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ)")
    st.sidebar.progress(ratio)
    st.sidebar.write(f"‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß **{filled}/{total} ‡∏à‡∏∏‡∏î** ({ratio*100:.1f}%)")

    if prog.get("ok"):
        st.sidebar.caption(f"Sheet: {prog.get('sheet_title')} | Row: {prog.get('row')} | ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {prog.get('asof')}")
    else:
        st.sidebar.error("‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        st.sidebar.caption(str(prog.get("error", ""))[:300])

    missing_list = prog.get("missing") or []
    if missing_list:
        with st.sidebar.expander(f"üö® ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡∏á ({len(missing_list)}) ‡∏à‡∏∏‡∏î", expanded=False):
            show_n = 40
            for m in missing_list[:show_n]:
                nm = m.get("name") or ""
                st.write(f"- {m['point_id']}" + (f" ‚Äî {nm}" if nm else ""))
            if len(missing_list) > show_n:
                st.caption(f"...‡∏≠‡∏µ‡∏Å {len(missing_list)-show_n} ‡∏à‡∏∏‡∏î")

        # Quick jump ‡πÑ‡∏õ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡∏á (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)
        miss_ids = [m["point_id"] for m in missing_list if m.get("point_id")]
        jump_pid = st.sidebar.selectbox("‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡∏á", options=["(‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)"] + miss_ids, key="emp_jump_missing")
        if st.sidebar.button("‚û°Ô∏è ‡πÑ‡∏õ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ", use_container_width=True, key="emp_jump_btn"):
            if jump_pid != "(‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)":
                reset_emp_meter_state()
                st.session_state.emp_point_id = str(jump_pid).strip().upper()
                st.session_state.emp_step = "INPUT"
                st.session_state.confirm_mode = False
                st.rerun()
    else:
        st.sidebar.success("‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß üéâ")
 
    # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏´‡∏°‡∏î mismatch confirm ‡πÉ‡∏´‡πâ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏î‡∏¥‡∏°
    if st.session_state.get("confirm_mode", False):
        st.session_state.emp_point_id = st.session_state.get("last_point_id", st.session_state.emp_point_id)
        st.session_state.emp_step = "INPUT"

    # =========================================================
    # STEP 1: SCAN QR
    # =========================================================
    if st.session_state.emp_step == "SCAN_QR":
        st.subheader("‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏™‡πÅ‡∏Å‡∏ô QR ‡∏ó‡∏µ‡πà‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå")
        st.write("üìå ‡∏ñ‡πà‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÉ‡∏Å‡∏•‡πâ ‡πÜ ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15‚Äì25 ‡∏ã‡∏°.)")

        tabs = st.tabs(["üì∑ ‡∏ñ‡πà‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏•‡πâ‡∏≠‡∏á", "üñºÔ∏è ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ QR (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ö‡∏ô‡∏Ñ‡∏≠‡∏°)"])
        with tabs[0]:
            qr_pic = st.camera_input("‡∏ñ‡πà‡∏≤‡∏¢ QR ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î", key=f"emp_qr_cam_{st.session_state.emp_nonce}")
        with tabs[1]:
            qr_upload = st.file_uploader(
                "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ QR (jpg/png)",
                type=["jpg", "jpeg", "png"],
                key=f"emp_qr_upload_{st.session_state.emp_nonce}",
                help="‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ö‡∏ô‡∏Ñ‡∏≠‡∏°/‡πÑ‡∏°‡πà‡∏°‡∏µ camera_input"
            )

        qr_bytes = None
        if qr_pic is not None:
            qr_bytes = qr_pic.getvalue()
        elif qr_upload is not None:
            qr_bytes = qr_upload.getvalue()

        if qr_bytes:
            pid = decode_qr(qr_bytes)
            if pid:
                reset_emp_meter_state()
                st.session_state.emp_point_id = pid
                st.session_state.emp_step = "INPUT"
                st.rerun()
            else:
                st.warning("‡∏¢‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô QR ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: ‡∏•‡∏≠‡∏á‡∏ñ‡πà‡∏≤‡∏¢/‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏õ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ QR")

        # --- ‡∏ó‡∏≤‡∏á‡∏´‡∏ô‡∏µ‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô (‡∏ã‡πà‡∏≠‡∏ô) ---
        with st.expander("‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ? ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏≠‡∏á"):
            manual_pid = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå point_id", key="emp_manual_pid")
            if st.button("‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏£‡∏´‡∏±‡∏™", use_container_width=True, key="emp_manual_ok"):
                if manual_pid.strip():
                    reset_emp_meter_state()
                    st.session_state.emp_point_id = manual_pid.strip().upper()
                    st.session_state.emp_step = "INPUT"
                    st.rerun()
                else:
                    st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏£‡∏´‡∏±‡∏™‡∏Å‡πà‡∏≠‡∏ô")

        st.stop()

    # =========================================================
    # STEP 2: CONFIRM POINT (show name + ref image)
    # =========================================================
    if st.session_state.emp_step == "CONFIRM_POINT":
        pid = st.session_state.emp_point_id
        config = get_meter_config(pid)
        if not config:
            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö point_id: {pid} ‡πÉ‡∏ô PointsMaster")
            if st.button("‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏´‡∏°‡πà", use_container_width=True):
                st.session_state.emp_step = "SCAN_QR"
                st.session_state.emp_point_id = ""
                st.rerun()
            st.stop()

        meter_type = infer_meter_type(config)

        st.subheader("‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡∏£‡∏ß‡∏à")
        st.write(f"**Point:** {pid}")
        if config.get("name"):
            st.write(f"**‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏∏‡∏î:** {config.get('name')}")
        st.write(f"**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:** {'üíß Water' if meter_type=='Water' else '‚ö° Electric'}")
        # ‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å GCS ‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå service account ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á public)
        ref_bytes, ref_path = load_ref_image_bytes_any(pid)
        if ref_bytes:
            st.image(ref_bytes, caption=f"‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (Reference): {ref_path}", use_container_width=True)
        else:
            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô bucket ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ")
            st.caption("‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ point_id ‡πÄ‡∏ä‡πà‡∏ô CH_S11D_106_....jpg ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ref_images/CH_S11D_106.jpg")


        b1, b2 = st.columns(2)
        if b1.button("‚úÖ ‡πÉ‡∏ä‡πà‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ", type="primary", use_container_width=True):
            st.session_state.emp_step = "INPUT"
            st.rerun()
        if b2.button("‚ùå ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà / ‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏´‡∏°‡πà", use_container_width=True):
            st.session_state.emp_step = "SCAN_QR"
            st.session_state.emp_point_id = ""
            st.rerun()

        st.stop()

    # =========================================================
    # STEP 3: INPUT + PHOTO + SAVE
    # =========================================================
    # ‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ = emp_step == "INPUT"
    point_id = st.session_state.emp_point_id
    config = get_meter_config(point_id)
    if not config:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö config ‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ")
        st.session_state.emp_step = "SCAN_QR"
        st.session_state.emp_point_id = ""
        st.stop()

    report_col = str(config.get('report_col', '-') or '-').strip()
    meter_type = infer_meter_type(config)

    # =========================================================
    # ‚úÖ (3) Duplicate Guard: ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô WaterReport ‡πÅ‡∏•‡πâ‡∏ß
    # =========================================================
    pid_u = str(point_id).strip().upper()
    if pid_u in done_set:
        existing_val = done_val_map.get(pid_u, "")
        st.warning(
            f"‚ö†Ô∏è ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏•‡∏á‡πÉ‡∏ô WaterReport ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {selected_date.strftime('%Y-%m-%d')} ‡πÅ‡∏•‡πâ‡∏ß"
            + (f" (‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ: {existing_val})" if str(existing_val).strip() else "")
            + "\n\n‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏•‡∏á‡∏ã‡πâ‡∏≥ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏õ‡∏ó‡∏≥‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡∏á‡∏à‡∏≤‡∏Å sidebar)"
        )
        
    st.write("---")
    c1, c2 = st.columns([2, 1])

    with c1:
        st.markdown(f"üìç ‡∏à‡∏∏‡∏î‡∏ï‡∏£‡∏ß‡∏à: **{point_id}**")
        if config.get("name"):
            st.caption(config.get("name"))
        st.markdown(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: <span class='report-badge'>{report_col}</span>", unsafe_allow_html=True)
        if st.button("üîÅ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏∏‡∏î (‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏´‡∏°‡πà)", use_container_width=True, key="emp_change_point"):
            reset_emp_meter_state()
            st.session_state.emp_step = "SCAN_QR"
            st.session_state.emp_point_id = ""
            st.session_state.confirm_mode = False
            st.rerun()

    with c2:
        decimals = int(config.get("decimals", 0) or 0)
        step = 1.0 if decimals == 0 else (10 ** (-decimals))
        # ‚úÖ Dynamic format: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö decimals ‡∏ó‡∏∏‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô (0, 1, 2, 3, ...)
        # ‚úÖ Use % style for Streamlit st.number_input
        fmt = f"%.{decimals}f"
        st.caption("‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏•‡πâ‡∏ß AI ‡∏à‡∏∞‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡πà‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á")

    # --- (Optional) ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πà‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ---
    with st.expander("üñºÔ∏è ‡∏î‡∏π‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ)"):
        ref_bytes, ref_path = load_ref_image_bytes_any(point_id)
        if ref_bytes:
            st.image(ref_bytes, caption="Reference: " + str(ref_path), use_container_width=True)
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (Reference) ‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ô bucket")

    tab_cam, tab_up = st.tabs(["üì∑ ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ", "üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î"])

    with tab_cam:
        img_cam = st.camera_input("‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå", key=f"emp_meter_cam_{st.session_state.emp_nonce}")

    with tab_up:
        img_up = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û",
            type=['jpg', 'png', 'jpeg'],
            key=f"emp_meter_upload_{st.session_state.emp_nonce}"
        )
        if img_up is not None:
            st.image(img_up, caption=f"‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: {getattr(img_up, 'name', 'upload')}", use_container_width=True)

    img_file = img_cam if img_cam is not None else img_up

    st.write("---")
    st.subheader("‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û/‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‚Üí AI ‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡πà‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

    # --- ‡∏Å‡∏±‡∏ô OCR ‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏ô‡πâ‡∏≤ rerun ---
    if "emp_ai_value" not in st.session_state:
        st.session_state.emp_ai_value = None
    if "emp_img_hash" not in st.session_state:
        st.session_state.emp_img_hash = ""
    if "emp_ai_msg" not in st.session_state:
         st.session_state.emp_ai_msg = ""

    if img_file is None:
        st.info("üì∑ ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ (‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ) ‡πÅ‡∏•‡πâ‡∏ß AI ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
        st.stop()

    img_bytes = img_file.getvalue()
    img_hash = hashlib.md5(img_bytes).hexdigest()

    # ‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡∏õ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà
    if img_hash != st.session_state.emp_img_hash:
        st.session_state.emp_img_hash = img_hash
        with st.spinner("ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤..."):
            best, cand = ocr_process(img_bytes, config, debug=False, return_candidates=True)
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ History Guard ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏∞‡∏™‡∏° (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏∏‡∏î‡∏≠‡∏∑‡πà‡∏ô)
            best2, msg = best, ""
            try:
                best2, msg = apply_history_guard(point_id, best, cand, config, selected_date)
            except Exception:
                best2, msg = best, ""
                
            st.session_state.emp_ai_value = float(best2)
            st.session_state.emp_ai_msg = msg

    # --- FIX: ‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤ AI ‡∏ï‡∏¥‡∏î‡∏•‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ st.number_input ‡∏•‡πâ‡∏° ---
    ai_val = float(st.session_state.emp_ai_value or 0.0)

    min_allowed = 0.0
    prefill_val = ai_val if ai_val >= min_allowed else min_allowed
    if ai_val < min_allowed:
        st.warning("‚ö†Ô∏è AI ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏•‡∏ö (‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏¥‡∏î) ‚Äî ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")

    st.write(f"ü§ñ **AI ‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡πà‡∏≤:** {fmt % ai_val}")
    if st.session_state.get("emp_ai_msg"):
        st.info(st.session_state.emp_ai_msg)

    choice = st.radio(
        "‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡πÑ‡∏´‡∏ô?",
        ["‚úÖ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ AI", "‚úçÔ∏è ‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏á"],
        horizontal=True,
        key="emp_choice"
    )

    if choice == "‚úçÔ∏è ‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏á":
        final_val = st.number_input(
            "‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á",
            value=float(prefill_val),
            min_value=min_allowed,
            step=step,
            format=fmt,
            key="emp_override_val"
        )
        status = "CONFIRMED_MANUAL"
    else:
        if ai_val < min_allowed:
            st.error("‚ùå AI ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏•‡∏ö ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ö‡∏ö '‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ AI' ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å '‚úçÔ∏è ‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏á'")
            st.stop()
        final_val = float(ai_val)
        status = "CONFIRMED_AI"

    st.info(f"‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {fmt % float(final_val)}")

    col_save, col_retry = st.columns(2)

    if col_save.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤", type="primary", use_container_width=True):
        try:
            filename = f"{point_id}_{selected_date.strftime('%Y%m%d')}_{get_thai_time().strftime('%H%M%S')}.jpg"
            image_url = upload_image_to_storage(img_bytes, filename)

            ok = save_to_db(point_id, inspector, meter_type, float(final_val), float(ai_val), status, selected_date, image_url)
            if ok:
                ok_r, msg_r = export_to_real_report(point_id, float(final_val), inspector, report_col, selected_date, debug=True)
                if not ok_r:
                    st.warning('‚ö†Ô∏è ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ TEST waterreport ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ' + msg_r)
                st.success("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

                # ‡πÑ‡∏õ‡∏à‡∏∏‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                reset_emp_meter_state()
                st.session_state.emp_step = "SCAN_QR"
                st.session_state.emp_point_id = ""
                st.rerun()
            else:
                st.error("‚ùå Save Failed")
        except Exception as e:
            st.error(f"‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

    if col_retry.button("üîÅ ‡∏ñ‡πà‡∏≤‡∏¢/‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà", use_container_width=True):
        reset_emp_meter_state()
        st.rerun()

elif mode == "üì∏ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô (‡∏°‡∏µ point_id ‡πÉ‡∏ô‡∏£‡∏π‡∏õ)":
    st.title("üì∏ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô ‚Üí ‡∏≠‡πà‡∏≤‡∏ô point_id ‚Üí ‡∏•‡∏á WaterReport")
    st.caption("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå/zip ‡∏ó‡∏µ‡πà‡∏°‡∏µ point_id ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß")

    c_insp, c_date = st.columns(2)
    with c_insp:
        inspector = st.text_input("‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "Admin", key="bulk_inspector")
    with c_date:
        # ‚úÖ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ key ‡πÉ‡∏´‡πâ date picker ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÑ‡∏°‡πà cache)
        report_date = st.date_input("üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", value=get_thai_time().date())

    norm_map = build_pid_norm_map()
    pm = load_points_master() or []
    all_pids = sorted({str(r.get("point_id","")).strip().upper() for r in pm if r.get("point_id")})

    up_files = st.file_uploader(
        "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ (‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå) ‡∏´‡∏£‡∏∑‡∏≠ zip",
        type=["jpg","jpeg","png","zip"],
        accept_multiple_files=True,
        key="bulk_upload"
    )
    if not up_files:
        st.stop()

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‚Üí ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô reset bulk_rows
    current_file_ids = tuple(sorted([f.name for f in up_files]))
    if "bulk_last_files" not in st.session_state:
        st.session_state["bulk_last_files"] = None
    
    if st.session_state["bulk_last_files"] != current_file_ids:
        # ‚úÖ ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‚Üí reset all state
        st.session_state["bulk_rows"] = None
        st.session_state["bulk_expanded"] = {}
        st.session_state["bulk_last_files"] = current_file_ids

    # ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö zip + ‡∏£‡∏π‡∏õ‡∏ï‡∏£‡∏á ‡πÜ
    images = []  # [{name, bytes}]
    for f in up_files:
        name = getattr(f, "name", "upload")
        b = f.getvalue()
        if name.lower().endswith(".zip"):
            z = zipfile.ZipFile(io.BytesIO(b))
            for zi in z.infolist():
                if zi.filename.lower().endswith((".jpg",".jpeg",".png")):
                    images.append({"name": os.path.basename(zi.filename), "bytes": z.read(zi)})
        else:
            images.append({"name": name, "bytes": b})

    st.write(f"‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: **{len(images)}** ‡πÑ‡∏ü‡∏•‡πå")
    st.session_state["bulk_image_map"] = {it["name"]: it["bytes"] for it in images}

    if "bulk_rows" not in st.session_state:
        st.session_state["bulk_rows"] = None

    # ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ process ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏û‡πÄ‡∏™‡∏£‡πá‡∏à (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°)
    if st.session_state["bulk_rows"] is None:
        st.info("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ...")
        rows = []
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á progress bar ‡∏î‡πâ‡∏≤‡∏ô‡∏ô‡∏≠‡∏Å loop ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô realtime
        progress_container = st.empty()
        status_container = st.empty()
        
        for i, it in enumerate(images, start=1):
            img_name = it["name"]
            img_bytes = it["bytes"]

            # Update progress text
            status_container.text(f"üìç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {i}/{len(images)} - {img_name[:40]}")
            progress_container.progress(i / len(images))

            pid, _pid_text = extract_point_id_from_image(img_bytes, norm_map)
            pid_u = str(pid).strip().upper() if pid else ""

            cfg = get_meter_config(pid_u) if pid_u else None
            ai_val = None
            msg = ""
            stt = "NO_PID"
            candidates_list = []

            if pid_u and cfg:
                try:
                    best, cand = ocr_process(img_bytes, cfg, return_candidates=True)
                    # ‚úÖ ‡πÉ‡∏ä‡πâ History Guard + candidates
                    best2, hmsg = apply_history_guard(pid_u, best, cand, cfg, report_date)
                    ai_val = float(best2)
                    msg = hmsg or ""
                    stt = "OK"
                    # ‡πÄ‡∏Å‡πá‡∏ö candidates ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                    candidates_list = cand if isinstance(cand, list) else []
                except Exception as e:
                    stt = "OCR_FAIL"
                    msg = str(e)[:200]
            elif pid_u and not cfg:
                stt = "NO_CONFIG"
                msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö config ‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ô PointsMaster"

            rows.append({
                "file": img_name,
                "point_id": pid_u or "",
                "ai_value": ai_val,
                "final_value": ai_val,
                "status": stt,
                "note": msg,
                "candidates": candidates_list,
                "image_bytes": img_bytes,  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡πÑ‡∏ß‡πâ
            })

        progress_container.empty()
        status_container.empty()
        
        st.session_state["bulk_rows"] = rows
        st.session_state["bulk_candidates_storage"] = {rows[i]["file"]: rows[i].get("candidates", []) for i in range(len(rows))}
        st.success(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à {len(rows)} ‡∏£‡∏π‡∏õ")
        st.rerun()

    rows = st.session_state.get("bulk_rows")
    if not rows:
        st.stop()

    st.subheader("üìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç")
    
    # ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô
    ok_count = sum(1 for r in rows if r.get("status") == "OK")
    no_pid = sum(1 for r in rows if r.get("status") == "NO_PID")
    no_cfg = sum(1 for r in rows if r.get("status") == "NO_CONFIG")
    fail = sum(1 for r in rows if r.get("status") == "OCR_FAIL")
    
    col_ok, col_pid, col_cfg, col_fail = st.columns(4)
    col_ok.metric("‚úÖ OK", ok_count)
    col_pid.metric("üö´ ‡πÑ‡∏°‡πà‡∏°‡∏µ ID", no_pid)
    col_cfg.metric("‚öôÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ config", no_cfg)
    col_fail.metric("‚ùå Fail", fail)
    
    st.divider()
    
    # ‚úÖ ‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏≥ expand state
    if "bulk_expanded" not in st.session_state:
        st.session_state["bulk_expanded"] = {}
    
    # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á compact ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡∏∏‡πà‡∏° expand ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
    st.markdown("### üñºÔ∏è ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏π‡∏õ (‡∏Ñ‡∏•‡∏¥‡∏Å '‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)")
    
    for idx, r in enumerate(rows):
        file_name = r.get("file", "")
        pid = r.get("point_id", "")
        val = r.get("final_value", 0)
        status = r.get("status", "")
        note = r.get("note", "")
        img_bytes = r.get("image_bytes")
        candidates = r.get("candidates", [])
        
        # Status emoji
        status_emoji = {
            "OK": "‚úÖ",
            "SAVED": "üíæ",
            "NO_PID": "üö´",
            "NO_CONFIG": "‚öôÔ∏è",
            "OCR_FAIL": "‚ùå",
        }.get(status, "‚ùì")
        
        # Compact display
        col1, col2, col3, col4, col5 = st.columns([2, 2, 1.5, 1.5, 1], gap="small")
        
        with col1:
            st.caption(f"üìÑ {file_name[:30]}")
        with col2:
            st.caption(f"**{pid}**" if pid else "‚Äî")
        with col3:
            # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ 0 ‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô) + format ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
            if val is not None and str(val).strip() != "":
                cfg = get_meter_config(pid)
                decimals = int(cfg.get('decimals', 0) or 0) if cfg else 0
                fmt = f"{{:.{decimals}f}}"
                st.caption(f"‡∏Ñ‡πà‡∏≤: **{fmt.format(val)}**")
            else:
                st.caption("‚Äî")
        with col4:
            st.caption(f"{status_emoji} {status}")
        with col5:
            expand_key = f"expand_{idx}"
            if st.button("üìã", key=f"btn_{idx}", help="‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"):
                st.session_state["bulk_expanded"][expand_key] = not st.session_state["bulk_expanded"].get(expand_key, False)
                st.rerun()
        
        # ‚úÖ Expandable detail view
        if st.session_state["bulk_expanded"].get(expand_key, False):
            with st.container(border=True):
                det_col1, det_col2 = st.columns([2, 1], vertical_alignment="top")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ
                with det_col1:
                    st.markdown("#### üñºÔ∏è ‡∏£‡∏π‡∏õ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö")
                    if img_bytes:
                        st.image(img_bytes, use_container_width=True)
                    else:
                        st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ")
                
                # ‡πÅ‡∏™‡∏î‡∏á candidates + ‡πÅ‡∏Å‡πâ‡∏Ñ‡πà‡∏≤ + ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
                with det_col2:
                    # ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô NO_PID ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå point_id ‡πÄ‡∏≠‡∏á
                    if status == "NO_PID":
                        st.markdown("#### üìç ‡∏û‡∏¥‡∏°‡∏û‡πå point_id")
                        manual_pid_input = st.selectbox(
                            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å point_id ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠",
                            options=["(‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏≠‡∏á)"] + all_pids,
                            key=f"no_pid_select_{idx}"
                        )
                        
                        if manual_pid_input == "(‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏≠‡∏á)":
                            manual_pid_text = st.text_input(
                                "‡∏û‡∏¥‡∏°‡∏û‡πå point_id",
                                value="",
                                placeholder="‡πÄ‡∏ä‡πà‡∏ô GU_BP_3_2",
                                key=f"no_pid_input_{idx}"
                            )
                            selected_manual_pid = manual_pid_text.strip().upper() if manual_pid_text else ""
                        else:
                            selected_manual_pid = manual_pid_input
                        
                        if st.button("‚úÖ ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô point_id", key=f"confirm_pid_{idx}", use_container_width=True, type="primary"):
                            if selected_manual_pid:
                                # ‡πÄ‡∏ä‡πá‡∏Ñ config
                                cfg = get_meter_config(selected_manual_pid)
                                if cfg:
                                    rows[idx]["point_id"] = selected_manual_pid
                                    rows[idx]["status"] = "OK"
                                    rows[idx]["note"] = f"‚úÖ Manual input: {selected_manual_pid}"
                                    st.session_state["bulk_rows"] = rows
                                    st.success(f"‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {selected_manual_pid}")
                                    st.rerun()
                                else:
                                    st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {selected_manual_pid}")
                            else:
                                st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå point_id")
                        
                        st.divider()
                    
                    st.markdown("#### üìã ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤")
                    
                    # Show candidates top 3
                    if candidates:
                        st.caption("Top candidates:")
                        try:
                            for c_idx, c in enumerate(candidates[:3]):
                                c_val = float(c.get("val", 0))
                                c_score = float(c.get("score", 0))
                                # ‚úÖ Format ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
                                cfg = get_meter_config(rows[idx].get("point_id", ""))
                                decimals = int(cfg.get('decimals', 0) or 0) if cfg else 0
                                fmt = f"{{:.{decimals}f}}"
                                val_str = fmt.format(c_val)
                                if st.button(f"‡πÉ‡∏ä‡πâ {val_str} (score {c_score:.0f})", key=f"use_cand_{idx}_{c_idx}", use_container_width=True):
                                    rows[idx]["final_value"] = c_val
                                    st.session_state["bulk_rows"] = rows
                                    st.success(f"‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {val_str}")
                                    st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
                    else:
                        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ candidates")
                    
                    st.divider()
                    
                    # Manual edit
                    st.caption("üìù ‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏≠‡∏á:")
                    
                    # ‚úÖ Get point_id first (selectbox comes first)
                    new_pid = st.selectbox(
                        "point_id",
                        options=[""] + all_pids,
                        index=([""] + all_pids).index(str(rows[idx].get("point_id", "")).strip().upper()) 
                               if str(rows[idx].get("point_id", "")).strip().upper() in ([""] + all_pids) else 0,
                        key=f"manual_pid_{idx}"
                    )
                    
                    # ‚úÖ Get decimals to format correctly (now new_pid is defined)
                    try:
                        cfg_manual = get_meter_config(new_pid or rows[idx].get("point_id", ""))
                        decimals_manual = int(cfg_manual.get('decimals', 0) or 0) if cfg_manual else 0
                        step_manual = 1.0 if decimals_manual == 0 else (10 ** (-decimals_manual))
                        # ‚úÖ Use % style format for Streamlit st.number_input
                        fmt_manual = f"%.{decimals_manual}f"
                        
                        # ‚úÖ Safe get final_value with default 0
                        try:
                            final_value_current = float(rows[idx].get("final_value") or 0)
                        except (ValueError, TypeError):
                            final_value_current = 0.0
                        
                        new_val = st.number_input(
                            "‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà",
                            value=final_value_current,
                            step=step_manual,
                            format=fmt_manual,
                            key=f"manual_val_{idx}"
                        )
                    except Exception as e:
                        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤: {e}")
                        st.stop()
                    
                    if st.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", key=f"save_{idx}", use_container_width=True, type="primary"):
                        rows[idx]["final_value"] = new_val
                        rows[idx]["point_id"] = new_pid
                        st.session_state["bulk_rows"] = rows
                        st.success("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß")
                        st.session_state["bulk_expanded"][expand_key] = False
                        st.rerun()
                    
                    st.divider()
                    
                    # ‚úÖ ‡∏õ‡∏∏‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheet ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    st.markdown("#### üìä ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheet")
                    
                    final_pid = new_pid or rows[idx].get("point_id", "")
                    final_val = new_val if new_val is not None else rows[idx].get("final_value", None)
                    
                    # ‚úÖ ‡πÅ‡∏Å‡πâ validation: ‡πÄ‡∏ä‡πá‡∏Ñ None/empty string ‡πÅ‡∏ó‡∏ô 0
                    if not final_pid:
                        st.warning("‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ point_id ‡∏Å‡πà‡∏≠‡∏ô")
                    elif final_val is None or str(final_val).strip() == "":
                        st.warning("‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡πà‡∏≠‡∏ô")
                    else:
                        col_save, col_skip = st.columns(2)
                        
                        with col_save:
                            if st.button("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ", key=f"save_sheet_{idx}", type="primary", use_container_width=True):
                                if not final_pid or final_val is None or str(final_val).strip() == "":
                                    st.error("‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ + point_id ‡∏Å‡πà‡∏≠‡∏ô")
                                else:
                                    cfg = get_meter_config(final_pid)
                                    if not cfg:
                                        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö config")
                                    else:
                                        report_col = str(cfg.get("report_col", "") or "").strip()
                                        if not report_col or report_col in ("-", "‚Äî", "‚Äì"):
                                            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ report_col")
                                        else:
                                            # ‚úÖ Parse value to float
                                            try:
                                                write_val = float(str(final_val).replace(",", "").strip())
                                            except Exception:
                                                write_val = str(final_val).strip()
                                            
                                            # ‚úÖ Use inspector from parent scope
                                            inspector_name = str(inspector).strip() or "Admin"
                                            
                                            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheet
                                            ok_r, msg_r = export_to_real_report(
                                                final_pid, 
                                                write_val, 
                                                inspector_name, 
                                                report_col, 
                                                report_date, 
                                                debug=True
                                            )
                                        
                                        if ok_r:
                                            st.success(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {msg_r}")
                                            rows[idx]["status"] = "SAVED"  # Mark as saved
                                            st.session_state["bulk_rows"] = rows
                                            st.session_state["bulk_expanded"][expand_key] = False
                                            st.rerun()
                                        else:
                                            st.error(f"‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {msg_r}")
                        
                        with col_skip:
                            if st.button("‚è≠Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°", key=f"skip_{idx}", use_container_width=True):
                                st.session_state["bulk_expanded"][expand_key] = False
                                st.rerun()
                
                st.caption(f"üìå Status: {status}")
                if note:
                    st.caption(f"üí¨ {note}")
        
        st.divider()

    write_mode_ui = st.radio(
        "‡πÄ‡∏ß‡∏•‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?",
        ["‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)"],
        index=0,
        horizontal=True,
        key="bulk_write_mode",
    )

    # ‚úÖ ‡∏õ‡∏∏‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)
    rows_final = st.session_state.get("bulk_rows", rows)
    unsaved_count = sum(1 for r in rows_final if r.get("status") != "SAVED")
    
    if unsaved_count > 0:
        if st.button(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ({unsaved_count} ‡∏à‡∏∏‡∏î)", type="primary", use_container_width=True):
            report_items = []
            db_rows = []
            fail_list = []

            folder = f"daily_bulk/{report_date.strftime('%Y%m%d')}"
            inspector_name = inspector or "Admin"

            for r in rows_final:
                if r.get("status") == "SAVED":
                    continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß
                
                pid_u = str(r.get("point_id","")).strip().upper()
                val = r.get("final_value", None)

                if not pid_u or val is None or str(val).strip() == "":
                    continue

                cfg = get_meter_config(pid_u)
                if not cfg:
                    fail_list.append((pid_u, "NO_CONFIG_IN_PointMaster"))
                    continue

                report_col = str(cfg.get("report_col","")).strip()
                if not report_col or report_col in ("-","‚Äî","‚Äì"):
                    fail_list.append((pid_u, "NO_REPORT_COL"))
                    continue

                img_bytes = r.get("image_bytes")

                image_url = "-"
                if img_bytes:
                    pid_slug = pid_u.replace(" ", "_")
                    filename = f"{folder}/{pid_slug}_{get_thai_time().strftime('%H%M%S')}.jpg"
                    image_url = upload_image_to_storage(img_bytes, filename)

                try:
                    write_val = float(str(val).replace(",", "").strip())
                except Exception:
                    write_val = str(val).strip()

                report_items.append({"point_id": pid_u, "value": write_val, "report_col": report_col})

                try:
                    meter_type = infer_meter_type(cfg)
                except Exception:
                    meter_type = "Electric"

                record_ts = datetime.combine(report_date, get_thai_time().time()).strftime("%Y-%m-%d %H:%M:%S")
                db_rows.append([record_ts, meter_type, pid_u, inspector_name, write_val, write_val, "AUTO_BULK_IMAGE_OCR", image_url])

            if not report_items:
                st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            else:
                ok_db, db_msg = append_rows_dailyreadings_batch(db_rows)
                if not ok_db:
                    st.warning(f"‚ö†Ô∏è Log DailyReadings ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {db_msg}")

                wm = "overwrite" if write_mode_ui.startswith("‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö") else "empty_only"
                ok_pids, fail_report = export_many_to_real_report_batch(report_items, report_date, debug=True, write_mode=wm)

                # ‚úÖ 1.3: Update logging with results
                if HAS_LOGGER:
                    update_log_success(ok_pids)
                    if fail_report:
                        update_log_failed(fail_report)
                    # Show daily report
                    st.info(print_daily_report())

                # ‚úÖ Show results
                if ok_pids:
                    st.success(f"‚úÖ ‡∏•‡∏á WaterReport ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(ok_pids)} ‡∏à‡∏∏‡∏î")
                
                if fail_report:
                    st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(fail_report)} ‡∏à‡∏∏‡∏î")
                    with st.expander("üìã ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:"):
                        for pid, reason in fail_report:
                            st.caption(f"  ‚Ä¢ {pid}: {reason}")
                
                # ‚úÖ Update rows status carefully
                for r in rows_final:
                    pid_u = str(r.get("point_id","")).strip().upper()
                    if pid_u in ok_pids:
                        r["status"] = "SAVED"
                    elif any(pid_u == str(f[0]).strip().upper() for f in fail_report):
                        r["status"] = "ERROR"
                
                st.session_state["bulk_rows"] = rows_final
                st.rerun()
    else:
        st.success("‚úÖ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß!")

elif mode == "üñ•Ô∏è Dashboard Screenshot (OCR)":
    st.title("üñ•Ô∏è Dashboard Screenshot ‚Üí WaterReport")
    st.caption("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ Dashboard ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ Pressure/Flowrate/Flow_Total ‡∏Ç‡∏≠‡∏á FLOW 1-3")

    c_insp, c_date = st.columns(2)
    with c_insp:
        inspector = st.text_input("‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "Admin", key="dash_inspector")
    with c_date:
        report_date = st.date_input("üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏Å‡∏£‡∏≠‡∏Å‡πÉ‡∏ô WaterReport)", value=get_thai_time().date(), key="dash_date")

    up = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ Dashboard (JPG/PNG)", type=["jpg", "jpeg", "png"], key="dash_img")
    if not up:
        st.info("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤")
        st.stop()

    img_bytes = up.getvalue()
    st.image(img_bytes, caption=f"‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î: {getattr(up, 'name', 'dashboard')}", use_container_width=True)

    # ‡∏Å‡∏±‡∏ô OCR ‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏ï‡∏≠‡∏ô rerun
    if "dash_img_hash" not in st.session_state:
        st.session_state.dash_img_hash = ""
    if "dash_rows" not in st.session_state:
        st.session_state.dash_rows = None
    if "dash_dbg" not in st.session_state:
        st.session_state.dash_dbg = None

    img_hash = hashlib.md5(img_bytes).hexdigest()
    if img_hash != st.session_state.dash_img_hash:
        st.session_state.dash_img_hash = img_hash
        st.session_state.dash_rows = None
        st.session_state.dash_dbg = None

    if st.button("üîé ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ (OCR)"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ..."):
            rows, dbg = extract_dashboard_flow_values(img_bytes, debug=True)
        st.session_state.dash_rows = rows
        st.session_state.dash_dbg = dbg

    rows = st.session_state.dash_rows
    if not rows:
        st.stop()

    st.subheader("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ")
    st.dataframe(pd.DataFrame(rows), use_container_width=True)

    pm = load_points_master()
    all_pids = sorted({str(r.get("point_id", "")).strip().upper() for r in pm if r.get("point_id")})

    st.subheader("‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‚Üí point_id")
    picked = []

    for r in rows:
        flow_label = r.get("flow", "")
        try:
            n = int(str(flow_label).strip().split()[-1])
        except Exception:
            n = None

        st.markdown(f"#### {flow_label}")
        cols = st.columns(3)

        metrics = [
            ("pressure_bar", "Pressure (bar)"),
            ("flowrate_m3h", "Flowrate (m3/h)"),
            ("flow_total_m3", "Flow_Total (m3)"),
        ]

        for i, (k, label) in enumerate(metrics):
            v = r.get(k)
            with cols[i]:
                st.caption(label)
                st.write(v)

                default_pid = (_DASH_DEFAULT_POINT_MAP.get((n, k), "") if n else "")
                default_pid = str(default_pid).strip().upper()

                options = ["(‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)"] + all_pids
                default_idx = options.index(default_pid) if default_pid in options else 0

                sel = st.selectbox(
                    "point_id",
                    options=options,
                    index=default_idx,
                    key=f"dash_pid_{flow_label}_{k}"
                )

                if sel != "(‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)" and v is not None:
                    picked.append({"point_id": sel, "value": v})

    with st.expander("Debug OCR"):
        st.json(st.session_state.dash_dbg or {})

    st.subheader("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport")
    if st.button("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport (‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)"):
        inspector_name = inspector or "Admin"

        report_items = []
        db_rows = []
        fail_list = []

        for it in picked:
            pid_u = str(it.get("point_id", "")).strip().upper()
            val = it.get("value", None)
            if not pid_u or val is None or str(val).strip() == "":
                continue

            cfg = get_meter_config(pid_u)
            if not cfg:
                fail_list.append((pid_u, "NO_CONFIG_IN_POINTSMaster"))
                continue

            report_col = str(cfg.get("report_col", "") or "").strip()
            if (not report_col) or (report_col in ("-", "‚Äî", "‚Äì")):
                fail_list.append((pid_u, "NO_REPORT_COL_IN_POINTSMaster"))
                continue

            try:
                write_val = float(str(val).replace(",", "").strip())
            except Exception:
                write_val = str(val).strip()

            report_items.append({"point_id": pid_u, "value": write_val, "report_col": report_col})

            try:
                meter_type = infer_meter_type(cfg)
            except Exception:
                meter_type = "Electric"

            current_time = get_thai_time().time()
            record_ts = datetime.combine(report_date, current_time).strftime("%Y-%m-%d %H:%M:%S")
            db_rows.append([record_ts, meter_type, pid_u, inspector_name, write_val, write_val, "AUTO_DASHBOARD_OCR", "-"])

        if not report_items:
            st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            st.stop()

        ok_db, db_msg = append_rows_dailyreadings_batch(db_rows)
        if not ok_db:
            st.warning(f"‚ö†Ô∏è Log ‡∏•‡∏á DailyReadings ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {db_msg}")

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport..."):
            ok_pids, fail_report = export_many_to_real_report_batch(report_items, report_date, debug=True)

        # ‚úÖ 1.3: Update logging with results
        if HAS_LOGGER:
            update_log_success(ok_pids)
            if fail_report:
                update_log_failed(fail_report)
            # Show daily report
            st.info(print_daily_report())

        st.success(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(ok_pids)} ‡∏à‡∏∏‡∏î")
        if fail_report:
            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(fail_report)} ‡∏à‡∏∏‡∏î")
            st.write([[pid, reason] for pid, reason in fail_report])

elif mode == "ÔøΩÔ∏è SQL Server (CUTEST SCADA - Test)":
    st.title("üóÑÔ∏è SQL Server Integration (Test Mode)")
    st.markdown("### ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CUTEST SCADA 2018 SQL Server ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á")
    
    st.warning("‚ö†Ô∏è ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Test Mode) - ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Google Sheet")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ driver ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
    has_driver = HAS_PYMSSQL or HAS_PYODBC or HAS_SQLALCHEMY
    if not has_driver:
        st.error("""
        ‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SQL Driver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á:
        
        **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö macOS/Linux:**
        ```
        pip install pymssql
        ```
        
        **‡∏´‡∏£‡∏∑‡∏≠ (‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ):**
        ```
        pip install sqlalchemy
        ```
        
        **‡∏´‡∏£‡∏∑‡∏≠ (Windows):**
        ```
        pip install pyodbc
        ```
        """)
        st.stop()
    
    # ‡πÅ‡∏™‡∏î‡∏á driver ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà
    driver_list = []
    if HAS_PYMSSQL: driver_list.append("‚úÖ pymssql")
    if HAS_SQLALCHEMY: driver_list.append("‚úÖ sqlalchemy")
    if HAS_PYODBC: driver_list.append("‚úÖ pyodbc")
    st.info(f"üîå Driver ‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(driver_list)}")
    
    st.markdown("---")
    st.subheader("1Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ SQL Server")
    
    col1, col2 = st.columns(2)
    with col1:
        sql_server = st.text_input("Server Address", placeholder="192.168.1.100 ‡∏´‡∏£‡∏∑‡∏≠ localhost", value=st.session_state.get("sql_server", ""))
        sql_username = st.text_input("Username", placeholder="sa", value=st.session_state.get("sql_username", ""))
    
    with col2:
        sql_database = st.text_input("Database Name", placeholder="CUTEST_DB", value=st.session_state.get("sql_database", ""))
        sql_password = st.text_input("Password", type="password", placeholder="password", value=st.session_state.get("sql_password", ""))
    
    if st.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"):
        st.session_state["sql_server"] = sql_server
        st.session_state["sql_database"] = sql_database
        st.session_state["sql_username"] = sql_username
        st.success("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß")
    
    if st.button("üîå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠", type="primary"):
        if not sql_server or not sql_database or not sql_username:
            st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö")
        else:
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠..."):
                success, message = test_sql_connection(sql_server, sql_database, sql_username, sql_password)
                if success:
                    st.success(message)
                else:
                    st.error(message)
    
    st.markdown("---")
    st.subheader("2Ô∏è‚É£ ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å SCADA")
    
    all_meters = load_points_master()
    meter_choices = {m['point_id']: f"{m['point_id']} - {m.get('device_name', '')}" for m in all_meters if m.get('point_id')}
    
    if not meter_choices:
        st.warning("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Point ID ‡πÉ‡∏ô PointsMaster")
    else:
        selected_pid = st.selectbox(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Point ID",
            options=list(meter_choices.keys()),
            format_func=lambda x: meter_choices[x]
        )
        
        col_date, col_time = st.columns(2)
        with col_date:
            query_date = st.date_input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", value=get_thai_time().date())
        with col_time:
            query_time = st.text_input("‡πÄ‡∏ß‡∏•‡∏≤ (HH:MM) - ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö", placeholder="14:30")
        
        if st.button("üîç ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", type="primary"):
            if not sql_server or not sql_database or not sql_username:
                st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL Server ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö")
            else:
                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
                    result = query_scada_values(
                        sql_server,
                        sql_database,
                        sql_username,
                        sql_password,
                        selected_pid,
                        query_date,
                        query_time
                    )
                
                if result["success"]:
                    st.success("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", f"{result.get('value', 'N/A')}")
                    with col2:
                        st.metric("‡πÄ‡∏ß‡∏•‡∏≤", result.get('timestamp', 'N/A')[:19])
                    with col3:
                        st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß", result.get('record_count', 0))
                    
                    st.info(f"üìä ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á: {result.get('table', 'Unknown')}")
                    
                    if result.get('all_records'):
                        st.subheader("üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
                        df_records = pd.DataFrame(result['all_records'])
                        st.dataframe(df_records, use_container_width=True)
                        
                        csv_data = df_records.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            "üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV",
                            csv_data,
                            f"scada_{selected_pid}_{query_date}.csv",
                            "text/csv"
                        )
                else:
                    st.error(result.get('message', "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"))
    
    st.markdown("---")
    st.subheader("üìù ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏")
    st.info("""
    **CUTEST SCADA 2018 ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á SQL:**
    - `History_Data` - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å
    - `Readings` - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô
    - `TagHistory` - ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ Tag
    
    **‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ:**
    - `TagName` ‡∏´‡∏£‡∏∑‡∏≠ `PointID` - ‡∏£‡∏´‡∏±‡∏™‡∏à‡∏∏‡∏î‡∏ß‡∏±‡∏î
    - `Value` - ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î
    - `Timestamp` - ‡πÄ‡∏ß‡∏•‡∏≤
    """)

elif mode == "ÔøΩüëÆ‚Äç‚ôÇÔ∏è Admin Approval":
    st.title("üëÆ‚Äç‚ôÇÔ∏è Admin Dashboard")
    st.caption("‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡πâ‡∏≥/‡πÑ‡∏ü")
    if st.button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä"): st.rerun()

    sh = gc.open(DB_SHEET_NAME)
    ws = sh.worksheet("DailyReadings")
    data = ws.get_all_records()
    pending = [d for d in data if str(d.get('Status', '')).strip().upper() == 'FLAGGED']

    if not pending: st.success("‚úÖ All Clear")
    else:
        for i, item in enumerate(pending):
            with st.container():
                st.markdown("---")
                c_info, c_val, c_act = st.columns([1.5, 1.5, 1])
                with c_info:
                    st.subheader(f"üö© {item.get('point_id')}")
                    st.caption(f"Inspector: {item.get('inspector')}")
                    img_url = item.get('image_url')
                    if img_url and img_url != '-' and str(img_url).startswith('http'):
                        st.image(img_url, width=220)
                    else:
                        st.warning("No Image")

                with c_val:
                    m_val = safe_float(item.get('Manual_Value'), 0.0)
                    a_val = safe_float(item.get('AI_Value'), 0.0)
                    options_map = {
                        f"üë§ ‡∏Ñ‡∏ô‡∏à‡∏î: {m_val}": m_val,
                        f"ü§ñ AI: {a_val}": a_val
                    }
                    selected_label = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:", list(options_map.keys()), key=f"rad_{i}")
                    choice = options_map[selected_label]

                with c_act:
                    st.write("")
                    if st.button("‚úÖ ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥", key=f"btn_{i}", type="primary"):
                        try:
                            timestamp = str(item.get('timestamp', '')).strip()
                            point_id = str(item.get('point_id', '')).strip()
                            cells = ws.findall(timestamp)
                            updated = False
                            for cell in cells:
                                if str(ws.cell(cell.row, 3).value).strip() == point_id:
                                    ws.update_cell(cell.row, 7, "APPROVED")
                                    ws.update_cell(cell.row, 5, choice)
                                    config = get_meter_config(point_id)
                                    report_col = (config.get('report_col', '') if config else '')
                                    
                                    # ‚úÖ Parse timestamp ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Sheet ‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡∏ï‡∏≠‡∏ô Approve
                                    try:
                                        dt_obj = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
                                        approve_date = dt_obj.date()
                                    except:
                                        approve_date = get_thai_time().date() # fallback
                                        
                                    ok_r, msg_r = export_to_real_report(point_id, choice, str(item.get('inspector', '')), report_col, approve_date, debug=True)
                                    if not ok_r:
                                        st.warning('‚ö†Ô∏è ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ TEST waterreport ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ' + msg_r)
                                    updated = True; break
                            if updated: st.success("Approved!"); st.rerun()
                            else: st.warning("‡∏´‡∏≤ row ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠")
                        except Exception as e: st.error(f"Error approve: {e}")

elif mode == "üì• ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Excel (SCADA Export)":
    st.title("üì• ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Excel (SCADA Export)")
    st.caption("‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ SCADA: ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà SCADA export ‡∏°‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ + ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")

    st.info(
        "‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ (‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡∏õ.6)\n"
        "1) ‡∏Å‡∏î 'Browse files' ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏°‡∏≤ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå)\n"
        "2) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô' ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô WaterReport\n"
        "3) ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° '‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Excel' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤\n"
        "4) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Excel -> ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ô‡∏±‡πâ‡∏ô\n"
        "5) ‡∏Å‡∏î '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport' ‡∏à‡∏ö ‚úÖ"
    )

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    report_date = st.date_input("üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏Å‡∏£‡∏≠‡∏Å‡πÉ‡∏ô WaterReport)", value=get_thai_time().date())
    report_date_str = report_date.strftime("%Y/%m/%d")

    # ‡πÇ‡∏´‡∏•‡∏î mapping
    st.subheader("1) ‡πÑ‡∏ü‡∏•‡πå Mapping (DB_Water_Scada.xlsx)")
    mapping_rows = []
    if os.path.exists("DB_Water_Scada.xlsx"):
        st.success("‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå DB_Water_Scada.xlsx ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå ‚úÖ (‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)")
        mapping_rows = load_scada_excel_mapping(local_path="DB_Water_Scada.xlsx")
    else:
        st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå DB_Water_Scada.xlsx ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô (‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å ‡πÜ)")
        uploaded_map = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î DB_Water_Scada.xlsx", type=["xlsx"])
        if uploaded_map is not None:
            mapping_rows = load_scada_excel_mapping(uploaded_bytes=uploaded_map.getvalue())

    if not mapping_rows:
        st.stop()

    # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Excel export (‡∏à‡∏≥‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ / ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ)
    st.subheader("2) ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà SCADA export (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ)")

    import hashlib, time

    # --- ‡πÇ‡∏´‡∏•‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ) ---
    if "scada_files" not in st.session_state:
        # filename -> {bytes, sha1, size, added_at, processed_sha1}
        st.session_state["scada_files"] = {}
    if "excel_updated_pids_last_run" not in st.session_state:
        st.session_state["excel_updated_pids_last_run"] = []

    # 2.1 ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà (‡∏à‡∏∞‡∏ñ‡∏π‡∏Å '‡πÄ‡∏û‡∏¥‡πà‡∏°' ‡πÄ‡∏Ç‡πâ‡∏≤ list ‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö)
    exports_new = st.file_uploader(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå) ‡πÄ‡∏ä‡πà‡∏ô ...Daily_Report.xlsx, ...UF_System.xlsx, ...SMMT_Daily_Report.xlsx",
        type=["xlsx"],
        accept_multiple_files=True,
        key="scada_exports_uploader",
    )

    added_count = 0
    if exports_new:
        for f in exports_new:
            b = f.getvalue()
            h = hashlib.sha1(b).hexdigest()
            old = st.session_state["scada_files"].get(f.name)
            if (old is None) or (old.get("sha1") != h):
                st.session_state["scada_files"][f.name] = {
                    "bytes": b,
                    "sha1": h,
                    "size": len(b),
                    "added_at": time.time(),
                    "processed_sha1": (old or {}).get("processed_sha1"),
                }
                added_count += 1

    if added_count:
        st.success(f"‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà {added_count} ‡πÑ‡∏ü‡∏•‡πå ‚úÖ (‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà)")

    files_dict = st.session_state.get("scada_files", {})
    if not files_dict:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Excel ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡πÑ‡∏ü‡∏•‡πå")
        st.stop()

    # 2.2 ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    def _is_new_file(meta: dict) -> bool:
        return (meta or {}).get("processed_sha1") != (meta or {}).get("sha1")

    file_rows = []
    for name, meta in files_dict.items():
        file_rows.append({
            "‡πÑ‡∏ü‡∏•‡πå": name,
            "‡∏Ç‡∏ô‡∏≤‡∏î(MB)": round((meta.get("size", 0) or 0) / 1_000_000, 2),
            "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞": "NEW" if _is_new_file(meta) else "‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß",
        })

    st.dataframe(pd.DataFrame(file_rows), use_container_width=True)

    c1, c2, c3 = st.columns([2, 1, 1])
    with c1:
        remove_sel = st.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)", options=list(files_dict.keys()), default=[])
    with c2:
        if st.button("üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"):
            for fn in remove_sel:
                files_dict.pop(fn, None)
            st.session_state["scada_files"] = files_dict
            st.rerun()
    with c3:
        if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
            st.session_state["scada_files"] = {}
            st.session_state.pop("excel_results", None)
            st.session_state.pop("excel_missing", None)
            st.session_state["excel_updated_pids_last_run"] = []
            st.rerun()

    # 2.3 ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô
    st.markdown("### 2.3 ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå")
    process_mode = st.radio(
        "‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?",
        ["üìö ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ", "‚ûï ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà (NEW)", "üéØ ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"],
        index=0,
        horizontal=True,
        key="scada_process_mode",
    )

    # ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ max_scan_rows
    with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô (‡∏Å‡∏£‡∏ì‡∏µ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö):"):
        scan_option = st.radio(
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå?",
            ["üöÄ ‡∏™‡πÅ‡∏Å‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß (5,000 ‡πÅ‡∏ñ‡∏ß)", "‚öñÔ∏è ‡∏™‡πÅ‡∏Å‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏•‡∏≤‡∏á (50,000 ‡πÅ‡∏ñ‡∏ß)", "üîç ‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà (100,000+ ‡πÅ‡∏ñ‡∏ß)", "‚ö° ‡∏™‡πÅ‡∏Å‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡∏ä‡πâ‡∏≤)"],
            index=1,  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: ‡∏ï‡∏±‡∏ß‡∏Å‡∏•‡∏≤‡∏á
            horizontal=False,
        )
        
        max_scan_rows_custom = 0  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô = ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        if "üöÄ" in scan_option:
            max_scan_rows_custom = 5000
        elif "‚öñÔ∏è" in scan_option:
            max_scan_rows_custom = 50000
        elif "üîç" in scan_option:
            max_scan_rows_custom = 500000  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 100000 ‡πÄ‡∏õ‡πá‡∏ô 500000
        elif "‚ö°" in scan_option:
            max_scan_rows_custom = 999999999  # ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
            st.warning("‚ö†Ô∏è ‡πÇ‡∏´‡∏°‡∏î '‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î' ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ä‡πâ‡∏≤ ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô > 500,000 ‡πÅ‡∏ñ‡∏ß)")
        
        st.caption(f"‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {max_scan_rows_custom:,} ‡πÅ‡∏ñ‡∏ß" if max_scan_rows_custom else "‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")

    all_files = list(files_dict.keys())
    new_files = [fn for fn in all_files if _is_new_file(files_dict.get(fn, {}))]

    proc_files = []
    if process_mode.startswith("üìö"):
        proc_files = all_files
    elif process_mode.startswith("‚ûï"):
        proc_files = new_files
        if not proc_files:
            st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå NEW ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏≠‡∏∑‡πà‡∏ô)")
    else:
        proc_files = st.multiselect(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡πà‡∏≤‡∏ô",
            options=all_files,
            default=all_files,
            key="scada_selected_files",
        )

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á' ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤)
    uploaded_exports_proc = {fn: files_dict[fn]["bytes"] for fn in proc_files if fn in files_dict}

    # ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤
    if st.button("üîé ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Excel"):
        if not uploaded_exports_proc:
            st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
            st.stop()

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Excel..."):
            # === (Optional) ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏£‡∏ì‡∏µ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ ===
            # ‡∏õ‡∏Å‡∏ï‡∏¥‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô NO_FILE ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
            file_key_map = {}
            key_norms = sorted({_strip_date_prefix(r.get("file_key", "")) for r in mapping_rows if r.get("file_key")})

            with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡∏∂‡πâ‡∏ô NO_FILE / ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)"):
                if not key_norms:
                    st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö file_key ‡πÉ‡∏ô mapping")
                else:
                    # **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
                    options = ["(Auto)"] + list(uploaded_exports_proc.keys())

                    for kn in key_norms:
                        if not kn:
                            continue

                        default_choice = "(Auto)"
                        kn_strip = (kn or "").strip().lower()
                        kn_norm = _norm_filekey(kn_strip)

                        # 1) ‡∏ï‡∏£‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß
                        exact_cands = []
                        for fname in uploaded_exports_proc.keys():
                            f_strip = _strip_date_prefix(fname)
                            if f_strip == kn_strip:
                                exact_cands.append(fname)

                        # 2) ‡∏ï‡∏£‡∏á‡πÅ‡∏ö‡∏ö normalize
                        if not exact_cands and kn_norm:
                            for fname in uploaded_exports_proc.keys():
                                f_strip = _strip_date_prefix(fname)
                                if _norm_filekey(f_strip) == kn_norm:
                                    exact_cands.append(fname)

                        if exact_cands:
                            if "smmt" not in kn_norm:
                                non_smmt = [f for f in exact_cands if "smmt" not in _norm_filekey(_strip_date_prefix(f))]
                                default_choice = non_smmt[0] if non_smmt else exact_cands[0]
                            else:
                                default_choice = exact_cands[0]
                        else:
                            # 3) fallback ‡πÅ‡∏ö‡∏ö scoring
                            best = None
                            best_score = -10**9
                            for fname in uploaded_exports_proc.keys():
                                f_strip = _strip_date_prefix(fname)
                                f_norm = _norm_filekey(f_strip)
                                score = 0
                                if f_strip == kn_strip:
                                    score += 1000
                                if f_norm == kn_norm and kn_norm:
                                    score += 900
                                if kn_strip and kn_strip in f_strip:
                                    score += 80
                                if kn_norm and kn_norm in f_norm:
                                    score += 60
                                if ("smmt" in f_norm) != ("smmt" in kn_norm):
                                    score -= 500
                                if kn_norm and f_norm.startswith(kn_norm):
                                    score += 40
                                if score > best_score:
                                    best_score = score
                                    best = fname
                            if best is not None and best_score >= 200:
                                default_choice = best

                        # UF_System: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå UF ‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏î‡∏≤ AF_Report/Report_Gen (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ)
                        if default_choice == "(Auto)":
                            kn2 = _norm_filekey(kn)
                            if "uf" in kn2 or "uf_system" in kn2 or "ufsystem" in kn2:
                                for fname in uploaded_exports_proc.keys():
                                    fn2 = _norm_filekey(fname)
                                    if fn2.startswith("af_report") or "report_gen" in fn2:
                                        default_choice = fname
                                        break

                        sel = st.selectbox(
                            f"‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{kn}'",
                            options=options,
                            index=options.index(default_choice) if default_choice in options else 0,
                            key=f"filemap_{kn}",
                        )
                        if sel != "(Auto)":
                            file_key_map[kn] = sel

                    st.caption("‡∏ó‡∏¥‡∏õ: ‡∏ñ‡πâ‡∏≤ UF/System ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå AF_Report_Gen.. ‡∏°‡∏≤‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå UF_System")

            # --- ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---
            allow_single = True if process_mode.startswith("üìö") else False
            results_new, missing_new = extract_scada_values_from_exports(
                mapping_rows,
                uploaded_exports_proc,
                file_key_map=file_key_map,
                target_date=report_date,
                allow_single_file_fallback=allow_single,
                custom_max_scan_rows=max_scan_rows_custom,
            )

            # --- ‡∏£‡∏ß‡∏°‡∏ú‡∏•: ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà/‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡πÉ‡∏´‡πâ '‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°' ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏° ---
            prev = st.session_state.get("excel_results")
            merged = []
            updated_pids = set()

            if prev and (not process_mode.startswith("üìö")):
                prev_by_pid = {str(r.get("point_id")): r for r in prev}
                for r in results_new:
                    pid = str(r.get("point_id"))
                    ok_new = (r.get("status") == "OK") and (r.get("value") is not None)
                    if ok_new:
                        rr = dict(r)
                        rr["_updated"] = True
                        merged.append(rr)
                        updated_pids.add(pid)
                    else:
                        old = prev_by_pid.get(pid)
                        ok_old = old and (old.get("status") == "OK") and (old.get("value") is not None)
                        if ok_old:
                            oo = dict(old)
                            oo["_updated"] = False
                            merged.append(oo)
                        else:
                            rr = dict(r)
                            rr["_updated"] = False
                            merged.append(rr)
            else:
                for r in results_new:
                    ok_new = (r.get("status") == "OK") and (r.get("value") is not None)
                    rr = dict(r)
                    rr["_updated"] = bool(ok_new)
                    merged.append(rr)
                    if ok_new:
                        updated_pids.add(str(r.get("point_id")))

            # ‡∏ó‡∏≥ missing ‡∏à‡∏≤‡∏Å merged (‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏≠‡∏á)
            missing_point_ids = [r.get("point_id") for r in merged if not (r.get("status") == "OK" and r.get("value") is not None)]
            missing_merged = [{"point_id": pid} for pid in missing_point_ids if pid]

            # mark processed ‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô '‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß')
            for fn in proc_files:
                if fn in files_dict:
                    files_dict[fn]["processed_sha1"] = files_dict[fn].get("sha1")
            st.session_state["scada_files"] = files_dict

        # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô session
        st.session_state["excel_results"] = merged
        st.session_state["excel_missing"] = missing_merged
        st.session_state["excel_updated_pids_last_run"] = sorted(list(updated_pids))

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏™‡∏î‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏Å‡πâ/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    if "excel_results" in st.session_state:
        results = st.session_state["excel_results"]
        missing = st.session_state.get("excel_missing", [])

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ + ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        ok_count = sum(1 for r in results if r.get("status") == "OK" and r.get("value") is not None)
        st.success(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß {ok_count}/{len(results)} ‡∏à‡∏∏‡∏î")

        # ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà -> ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå _updated ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
        show_only_updated = st.checkbox("üÜï ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", value=False)

        show_only_missing = st.checkbox("üö´ ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Excel", value=False)
        df_show = pd.DataFrame(results)
        if show_only_missing and (not df_show.empty) and ("status" in df_show.columns):
            df_show = df_show[df_show["status"] != "OK"]
        if show_only_updated and (not df_show.empty) and ("_updated" in df_show.columns):
            df_show = df_show[df_show["_updated"] == True]
        st.dataframe(df_show, use_container_width=True)


        # ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢
        missing_point_ids = [m["point_id"] for m in missing]
        if missing_point_ids:
            st.warning("‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à/‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Excel: " + ", ".join(missing_point_ids))

        # ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢
        manual_inputs = {}
        with st.expander("‚úçÔ∏è ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Excel (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)"):
            for pid in missing_point_ids:
                manual_inputs[pid] = st.text_input(f"{pid} (‡∏Å‡∏£‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)", value="")

        # ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤ final
        final_values = {}
        for r in results:
            pid = r["point_id"]
            val = r["value"]
            if pid in manual_inputs and manual_inputs[pid].strip() != "":
                val = manual_inputs[pid].strip()
            final_values[pid] = val

        # ‡∏õ‡∏∏‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport
        st.subheader("3) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport")
        st.caption("‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á) ‡∏•‡∏á WaterReport ‡∏ï‡∏≤‡∏° report_col ‡πÉ‡∏ô PointsMaster")

        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏Å‡∏±‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö / ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
        save_scope = st.radio(
            "‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÑ‡∏´‡∏ô?",
            ["‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"],
            index=0,
            horizontal=True,
            key="scada_save_scope",
        )
        write_mode_ui = st.radio(
            "‡πÄ‡∏ß‡∏•‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?",
            ["‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)"],
            index=0,
            horizontal=True,
            key="scada_write_mode",
        )

        if st.button("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport (‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)"):
            inspector_name = "Admin"

            # 1) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (validate + ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤)
            report_items = []   # ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ WaterReport
            db_rows = []        # log ‡∏•‡∏á DailyReadings
            fail_list = []      # [(pid, reason), ...]

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î' ‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏≠‡∏Å
            last_updated = set(st.session_state.get("excel_updated_pids_last_run", []) or [])
            manual_updated = {pid for pid, vv in manual_inputs.items() if str(vv).strip() != ""}
            allowed_pids = None
            if save_scope.startswith("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞"):
                allowed_pids = last_updated.union(manual_updated)
                if not allowed_pids:
                    st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
                    st.stop()

            for pid, val in final_values.items():
                pid_u = str(pid).strip().upper()
                if allowed_pids is not None and pid_u not in allowed_pids:
                    continue

                if val is None or str(val).strip() == "":
                    continue

                cfg = get_meter_config(pid_u)
                if not cfg:
                    fail_list.append((pid_u, "NO_CONFIG_IN_POINTSMaster"))
                    continue

                report_col = str(cfg.get("report_col", "") or "").strip()
                if (not report_col) or (report_col in ("-", "‚Äî", "‚Äì")):
                    fail_list.append((pid_u, "NO_REPORT_COL_IN_POINTSMaster"))
                    continue

                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
                write_val = val
                try:
                    write_val = float(str(val).replace(",", "").strip())
                except Exception:
                    write_val = str(val).strip()

                report_items.append({
                    "point_id": pid_u,
                    "value": write_val,
                    "report_col": report_col
                })

                # ‡∏ó‡∏≥ log ‡∏•‡∏á DB (DailyReadings) ‚Äî timestamp = ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô + ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÑ‡∏ó‡∏¢)
                try:
                    meter_type = infer_meter_type(cfg)
                except Exception:
                    meter_type = "Electric"

                try:
                    current_time = get_thai_time().time()
                    record_ts = datetime.combine(report_date, current_time).strftime("%Y-%m-%d %H:%M:%S")
                except Exception:
                    record_ts = get_thai_time().strftime("%Y-%m-%d %H:%M:%S")

                db_rows.append([
                    record_ts,
                    meter_type,
                    pid_u,
                    inspector_name,
                    write_val,   # Manual_Value
                    write_val,   # AI_Value
                    "AUTO_EXCEL_SCADA",
                    "-"          # image_url
                ])

            if not report_items:
                st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ß‡πà‡∏≤‡∏á)")
                st.stop()

            # 2) log ‡∏•‡∏á DB ‡πÅ‡∏ö‡∏ö batch (‡∏•‡∏î requests)
            ok_db, db_msg = append_rows_dailyreadings_batch(db_rows)
            db_ok_count = len(db_rows) if ok_db else 0
            if not ok_db:
                # ‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡∏£‡∏∞‡∏ö‡∏ö ‡πÅ‡∏Ñ‡πà‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏•‡πá‡∏≠‡∏Å DB ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                st.warning(f"‚ö†Ô∏è Log ‡∏•‡∏á DailyReadings ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {db_msg}")

            # 3) export ‡∏•‡∏á WaterReport ‡πÅ‡∏ö‡∏ö batch (‡∏•‡∏î Read requests)
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport..."):
                wm = "overwrite" if write_mode_ui.startswith("‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö") else "empty_only"
                ok_pids, fail_report = export_many_to_real_report_batch(report_items, report_date, debug=True, write_mode=wm)

            # ‚úÖ 1.3: Update logging with results
            if HAS_LOGGER:
                update_log_success(ok_pids)
                if fail_report:
                    update_log_failed(fail_report)
                # Show daily report
                st.info(print_daily_report())

            report_ok = len(ok_pids)
            report_fail = list(fail_report)

            # ‡πÅ‡∏¢‡∏Å '‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å error ‡∏à‡∏£‡∏¥‡∏á
            skipped = [(pid, reason) for pid, reason in report_fail if str(reason) == 'SKIP_NON_EMPTY']
            report_fail_real = [(pid, reason) for pid, reason in report_fail if str(reason) != 'SKIP_NON_EMPTY']

            st.success(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á WaterReport ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {report_ok} ‡∏à‡∏∏‡∏î")
            st.info(f"üóÉÔ∏è Log ‡∏•‡∏á DailyReadings ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {db_ok_count} ‡∏à‡∏∏‡∏î")

            if skipped:
                st.info(f"‚è≠Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {len(skipped)} ‡∏à‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î '‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î' ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏ö)")

            if report_fail_real:
                st.error(f"‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(report_fail_real)} ‡∏à‡∏∏‡∏î")
                st.write([[pid, reason] for pid, reason in report_fail_real])
        st.divider()
        st.info("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ '‡∏°‡∏µ‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ü 1 ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ export ‡∏°‡∏≤‡πÉ‡∏ô Excel' -> ‡πÉ‡∏ä‡πâ‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏≠‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡πâ‡∏≥)")
