#!/usr/bin/env python3
"""
ü§ñ Auto Processor - ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
============================================================

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏û‡∏µ‡πà‡∏û‡∏à‡∏ô‡πå (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
1. ‡∏ä‡πà‡∏≤‡∏á‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå 3 ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å SCADA Server ‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå WATCH_FOLDER
2. Script ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á Google Sheets
4. ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Processed ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp

‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Scheduled (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥):
  python auto_processor.py --mode scheduled

‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Watch Folder (real-time):
  python auto_processor.py --mode watch

‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Manual (‡∏ó‡∏±‡∏ô‡∏ó‡∏µ):
  python auto_processor.py --mode manual
"""

import os
import sys
import glob
import shutil
import hashlib
import logging
import time
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd

# ‡πÉ‡∏ä‡πâ functions ‡∏à‡∏≤‡∏Å app.py
sys.path.insert(0, os.path.dirname(__file__))

# Import from standalone wrapper instead of app.py directly
sys.path.insert(0, os.path.dirname(__file__))

try:
    from app_standalone import (
        load_scada_excel_mapping,
        extract_scada_values_from_exports,
        gc,
        DB_SHEET_NAME,
        get_thai_time
    )
except ImportError as e:
    print(f"‚ùå Error importing from app_standalone.py: {e}")
    print("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå app_standalone.py ‡πÅ‡∏•‡∏∞ app.py ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô")
    sys.exit(1)

# ==================== Configuration ====================

CONFIG = {
    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏≤‡∏á‡∏à‡∏∞‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 2 ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
    # 1. Folder ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: "D:\WaterMeter\Uploads"
    # 2. Folder ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô: "D:\WaterMeter\Uploads\{date}" (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
    "WATCH_FOLDER": r"D:\WaterMeter\Uploads",
    
    # ‡πÉ‡∏ä‡πâ folder ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô True)
    "USE_DATE_FOLDERS": True,  # True = ‡∏´‡∏≤ folder ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö "5_2_69", "6_2_69"
    
    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
    "PROCESSED_FOLDER": r"D:\WaterMeter\Processed",
    
    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö log
    "LOG_FOLDER": r"D:\WaterMeter\Logs",
    
    # Pattern ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    "FILE_PATTERNS": [
        "*Daily_Report*.xlsx",       # ‡πÄ‡∏ä‡πà‡∏ô 2026_02_4_Daily_Report.xlsx
        "*UF_System*.xlsx",
        "*SMMT_Daily*.xlsx",         # ‡πÄ‡∏ä‡πà‡∏ô 2026_02_4_SMMT_Daily_Report.xlsx
        "AF_Report_Gen.xlsx",        # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà)
        "*AF_Report*.xlsx"
    ],
    
    # ‡πÑ‡∏ü‡∏•‡πå mapping (DB_Water_Scada.xlsx)
    "MAPPING_FILE": "DB_Water_Scada.xlsx",
    
    # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (50,000 = ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á)
    "MAX_SCAN_ROWS": 50000,
    
    # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scheduled mode)
    "SCHEDULED_TIMES": ["08:00", "16:00"],  # 08:00 ‡∏ô. ‡πÅ‡∏•‡∏∞ 16:00 ‡∏ô.
    
    # Check interval ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö watch mode (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    "WATCH_INTERVAL": 300,  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ
    
    # ‡∏™‡πà‡∏á notification ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    "ENABLE_NOTIFICATION": False,
    
    # Email settings (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î notification)
    "NOTIFICATION_EMAIL": "admin@example.com",
}

# ==================== Setup Logging ====================

def setup_logging():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö logging"""
    log_folder = Path(CONFIG["LOG_FOLDER"])
    log_folder.mkdir(parents=True, exist_ok=True)
    
    log_file = log_folder / f"auto_processor_{datetime.now().strftime('%Y%m')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

logger = setup_logging()

# ==================== Helper Functions ====================

def parse_date_from_folder_name(folder_name):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠ folder ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
    - 5_2_69 ‚Üí 2026-02-05
    - 15_2_69 ‚Üí 2026-02-15
    - 05_02_69 ‚Üí 2026-02-05
    - 5_2_2569 ‚Üí 2026-02-05
    
    Returns:
        datetime.date or None
    """
    import re
    
    # Pattern: d_m_yy or dd_mm_yy or d_m_yyyy
    patterns = [
        r'^(\d{1,2})_(\d{1,2})_(\d{2})$',      # 5_2_69
        r'^(\d{1,2})_(\d{1,2})_(\d{4})$',      # 5_2_2569
    ]
    
    for pattern in patterns:
        match = re.match(pattern, folder_name)
        if match:
            day, month, year = match.groups()
            day = int(day)
            month = int(month)
            year = int(year)
            
            # ‡πÅ‡∏õ‡∏•‡∏á year ‡πÅ‡∏ö‡∏ö Buddhist Era (2569) ‚Üí Christian Era (2026)
            if year > 2500:
                year = year - 543
            # ‡πÅ‡∏õ‡∏•‡∏á short year (69) ‚Üí full year (2026)
            elif year < 100:
                year = 2000 + year
            
            try:
                return datetime(year, month, day).date()
            except ValueError:
                logger.warning(f"Invalid date from folder: {folder_name}")
                return None
    
    return None

def create_folders():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    folders = [
        CONFIG["WATCH_FOLDER"],
        CONFIG["PROCESSED_FOLDER"],
        CONFIG["LOG_FOLDER"]
    ]
    
    for folder in folders:
        Path(folder).mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ Folder ready: {folder}")

def get_file_hash(filepath):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì hash ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥)"""
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def find_new_files():
    """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå watch"""
    watch_folder = Path(CONFIG["WATCH_FOLDER"])
    found_files = []
    
    if CONFIG["USE_DATE_FOLDERS"]:
        # ‡πÇ‡∏´‡∏°‡∏î: ‡∏´‡∏≤ folder ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 5_2_69, 6_2_69)
        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: d_m_yy, dd_m_yy, d_mm_yy, dd_mm_yy
        date_folders = []
        
        for item in watch_folder.iterdir():
            if item.is_dir():
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠ folder ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                # ‡πÄ‡∏ä‡πà‡∏ô 5_2_69, 05_02_69, 5_2_2569
                if '_' in item.name and item.name.replace('_', '').isdigit():
                    date_folders.append(item)
        
        if date_folders:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° modified time (‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
            date_folders.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            logger.info(f"üìÅ Found {len(date_folders)} date folder(s)")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• folder ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
            for folder in date_folders[:3]:  # ‡πÄ‡∏≠‡∏≤ 3 folder ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                logger.info(f"   Scanning: {folder.name}")
                for pattern in CONFIG["FILE_PATTERNS"]:
                    files = list(folder.glob(pattern))
                    found_files.extend(files)
        else:
            logger.warning("‚ö†Ô∏è No date folders found! Looking in main folder...")
            # Fallback: ‡∏´‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å
            for pattern in CONFIG["FILE_PATTERNS"]:
                files = list(watch_folder.glob(pattern))
                found_files.extend(files)
    else:
        # ‡πÇ‡∏´‡∏°‡∏î: ‡∏´‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        for pattern in CONFIG["FILE_PATTERNS"]:
            files = list(watch_folder.glob(pattern))
            found_files.extend(files)
    
    # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ã‡πà‡∏≠‡∏ô (‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ~)
    found_files = [f for f in found_files if not f.name.startswith('~')]
    
    logger.info(f"üîç Found {len(found_files)} file(s) total")
    return found_files

def load_processed_history():
    """‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß"""
    history_file = Path(CONFIG["LOG_FOLDER"]) / "processed_history.json"
    
    if history_file.exists():
        with open(history_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_processed_history(history):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"""
    history_file = Path(CONFIG["LOG_FOLDER"]) / "processed_history.json"
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)

def is_file_processed(filepath, history):
    """‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"""
    file_hash = get_file_hash(filepath)
    filename = os.path.basename(filepath)
    
    return history.get(filename, {}).get('hash') == file_hash

# ==================== Core Processing ====================

def process_files_batch(files, target_date=None):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheets
    
    Returns:
        dict: ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    """
    if not files:
        logger.warning("‚ö†Ô∏è No files to process")
        return {"success": 0, "failed": 0, "total": 0}
    
    if target_date is None:
        target_date = get_thai_time().date()
    
    logger.info(f"üìÖ Target date: {target_date}")
    logger.info(f"üìÇ Processing {len(files)} file(s)...")
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î mapping
    try:
        mapping_file = Path(__file__).parent / CONFIG["MAPPING_FILE"]
        if not mapping_file.exists():
            logger.error(f"‚ùå Mapping file not found: {mapping_file}")
            return {"success": 0, "failed": 0, "total": 0, "error": "Mapping file not found"}
        
        mapping = load_scada_excel_mapping(str(mapping_file))
        logger.info(f"‚úÖ Loaded {len(mapping)} mapping entries")
    except Exception as e:
        logger.error(f"‚ùå Error loading mapping: {e}")
        return {"success": 0, "failed": 0, "total": 0, "error": str(e)}
    
    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel
    uploaded_exports = {}
    for file_path in files:
        try:
            filename = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                uploaded_exports[filename] = f.read()
            logger.info(f"‚úÖ Loaded: {filename} ({os.path.getsize(file_path) / 1024 / 1024:.1f} MB)")
        except Exception as e:
            logger.error(f"‚ùå Error reading {file_path}: {e}")
    
    if not uploaded_exports:
        logger.error("‚ùå No files could be loaded")
        return {"success": 0, "failed": 0, "total": 0, "error": "No files loaded"}
    
    # 3. Extract values
    try:
        logger.info("üîÑ Extracting values from Excel files...")
        results, missing = extract_scada_values_from_exports(
            uploaded_exports=uploaded_exports,
            mapping_rows=mapping,
            target_date=target_date,
            custom_max_scan_rows=CONFIG["MAX_SCAN_ROWS"]
        )
        logger.info(f"‚úÖ Extracted {len(results)} point values")
    except Exception as e:
        logger.error(f"‚ùå Error extracting values: {e}")
        return {"success": 0, "failed": 0, "total": 0, "error": str(e)}
    
    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheets
    success_count = 0
    failed_count = 0
    
    try:
        sh = gc.open(DB_SHEET_NAME)
        ws = sh.worksheet("DailyReadings")
        
        current_time = get_thai_time()
        timestamp_str = current_time.strftime("%Y-%m-%d %H:%M:%S")
        
        # results is a list of dicts, not a dict itself
        for result in results:
            point_id = result.get("point_id")
            val = result.get("value")
            status = result.get("status")
            
            if val is not None and status == "OK":
                try:
                    row = [
                        timestamp_str,
                        "SCADA",
                        point_id,
                        "AUTO_SYSTEM",
                        val,
                        "-",
                        "AUTO",
                        "-"
                    ]
                    ws.append_row(row)
                    success_count += 1
                    logger.debug(f"  ‚úì {point_id}: {val}")
                except Exception as e:
                    failed_count += 1
                    logger.error(f"  ‚úó {point_id}: {e}")
            else:
                failed_count += 1
                logger.warning(f"  ‚ö† {point_id}: {status or 'No value'}")
        
        logger.info(f"‚úÖ Saved {success_count}/{len(results)} records to Google Sheets")
        
    except Exception as e:
        logger.error(f"‚ùå Error saving to Google Sheets: {e}")
        return {
            "success": success_count,
            "failed": len(results) - success_count,
            "total": len(results),
            "error": str(e)
        }
    
    return {
        "success": success_count,
        "failed": failed_count,
        "total": len(results),
        "missing": len(missing),
        "files_processed": len(files)
    }

def move_to_processed(files):
    """‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Processed"""
    processed_folder = Path(CONFIG["PROCESSED_FOLDER"])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ‡∏•‡∏ö duplicates ‡∏≠‡∏≠‡∏Å (‡πÉ‡∏ä‡πâ set ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list)
    unique_files = list(set([str(f) for f in files]))
    
    for file_path in unique_files:
        try:
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß)
            if not os.path.exists(file_path):
                logger.debug(f"‚è≠ Skipped (already moved): {os.path.basename(file_path)}")
                continue
                
            filename = os.path.basename(file_path)
            dest = processed_folder / f"{timestamp}_{filename}"
            shutil.move(str(file_path), str(dest))
            logger.info(f"üì¶ Moved: {filename} ‚Üí {dest.name}")
        except Exception as e:
            logger.error(f"‚ùå Error moving {file_path}: {e}")

# ==================== Processing Modes ====================

def process_manual():
    """‡πÇ‡∏´‡∏°‡∏î Manual: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ"""
    logger.info("=" * 60)
    logger.info("üöÄ Manual Processing Mode")
    logger.info("=" * 60)
    
    create_folders()
    files = find_new_files()
    
    if not files:
        logger.info("‚ÑπÔ∏è No files to process. Exiting.")
        return
    
    stats = process_files_batch([str(f) for f in files])
    
    if stats.get("success", 0) > 0:
        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó history ‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö hash)
        history = load_processed_history()
        unique_files = list(set([str(f) for f in files]))
        
        for file_path in unique_files:
            if os.path.exists(file_path):  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà
                filename = os.path.basename(file_path)
                history[filename] = {
                    "hash": get_file_hash(str(file_path)),
                    "processed_at": datetime.now().isoformat(),
                    "records": stats.get("success", 0)
                }
        save_processed_history(history)
        
        # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å save history ‡πÅ‡∏•‡πâ‡∏ß
        move_to_processed([str(f) for f in files])
    
    logger.info("=" * 60)
    logger.info(f"‚úÖ Processing complete!")
    logger.info(f"   Success: {stats.get('success', 0)}")
    logger.info(f"   Failed:  {stats.get('failed', 0)}")
    logger.info(f"   Total:   {stats.get('total', 0)}")
    logger.info("=" * 60)

def process_scheduled():
    """‡πÇ‡∏´‡∏°‡∏î Scheduled: ‡∏£‡∏±‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    logger.info("=" * 60)
    logger.info("‚è∞ Scheduled Processing Mode")
    logger.info(f"   Scheduled times: {CONFIG['SCHEDULED_TIMES']}")
    logger.info("=" * 60)
    
    create_folders()
    
    while True:
        current_time = datetime.now().strftime("%H:%M")
        
        if current_time in CONFIG["SCHEDULED_TIMES"]:
            logger.info(f"üîî Scheduled time reached: {current_time}")
            process_manual()
            
            # ‡∏£‡∏≠ 1 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ã‡πâ‡∏≥
            time.sleep(60)
        
        time.sleep(30)  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

def process_watch():
    """‡πÇ‡∏´‡∏°‡∏î Watch: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö real-time"""
    logger.info("=" * 60)
    logger.info("üëÄ Watch Folder Mode")
    logger.info(f"   Watch folder: {CONFIG['WATCH_FOLDER']}")
    logger.info(f"   Check interval: {CONFIG['WATCH_INTERVAL']} seconds")
    logger.info("=" * 60)
    
    create_folders()
    history = load_processed_history()
    
    while True:
        try:
            files = find_new_files()
            new_files = [f for f in files if not is_file_processed(str(f), history)]
            
            if new_files:
                logger.info(f"üÜï Found {len(new_files)} new file(s)")
                stats = process_files_batch([str(f) for f in new_files])
                
                if stats.get("success", 0) > 0:
                    move_to_processed([str(f) for f in new_files])
                    
                    # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó history
                    for file_path in new_files:
                        filename = os.path.basename(file_path)
                        history[filename] = {
                            "hash": get_file_hash(str(file_path)),
                            "processed_at": datetime.now().isoformat(),
                            "records": stats.get("success", 0)
                        }
                    save_processed_history(history)
            
            time.sleep(CONFIG["WATCH_INTERVAL"])
            
        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è Watch mode stopped by user")
            break
        except Exception as e:
            logger.error(f"‚ùå Error in watch loop: {e}")
            time.sleep(60)

# ==================== Main ====================

def main():
    parser = argparse.ArgumentParser(description="ü§ñ Auto Processor - ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
    parser.add_argument(
        '--mode',
        choices=['manual', 'scheduled', 'watch'],
        default='manual',
        help='‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: manual (‡∏ó‡∏±‡∏ô‡∏ó‡∏µ), scheduled (‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤), watch (‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)'
    )
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'manual':
            process_manual()
        elif args.mode == 'scheduled':
            process_scheduled()
        elif args.mode == 'watch':
            process_watch()
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Process interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
